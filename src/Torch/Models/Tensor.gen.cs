// Code generated by CodeMinion: https://github.com/SciSharp/CodeMinion

using System;
using System.Collections;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Runtime.InteropServices;
using System.Text;
using Python.Runtime;
using Numpy;
using Numpy.Models;

namespace Torch
{
    public partial class Tensor
    {
        
        /// <summary>
        /// Returns a new Tensor with data as the tensor data.
        /// By default, the returned Tensor has the same torch.dtype and
        /// torch.device as this tensor.
        /// 
        /// Warning
        /// new_tensor() always copies data. If you have a Tensor
        /// data and want to avoid a copy, use torch.Tensor.requires_grad_()
        /// or torch.Tensor.detach().
        /// If you have a numpy array and want to avoid a copy, use
        /// torch.from_numpy().
        /// 
        /// Warning
        /// When data is a tensor x, new_tensor() reads out ‘the data’ from whatever it is passed,
        /// and constructs a leaf variable. Therefore tensor.new_tensor(x) is equivalent to x.clone().detach()
        /// and tensor.new_tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).
        /// The equivalents using clone() and detach() are recommended.
        /// </summary>
        /// <param name="data">
        /// The returned Tensor copies data.
        /// </param>
        /// <param name="dtype">
        /// the desired type of returned tensor.
        /// Default: if None, same torch.dtype as this tensor.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, same torch.device as this tensor.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor new_tensor(NDarray data, Dtype dtype = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                data,
            });
            var kwargs=new PyDict();
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("new_tensor", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a Tensor of size size filled with fill_value.
        /// By default, the returned Tensor has the same torch.dtype and
        /// torch.device as this tensor.
        /// </summary>
        /// <param name="fill_value">
        /// the number to fill the output tensor with.
        /// </param>
        /// <param name="dtype">
        /// the desired type of returned tensor.
        /// Default: if None, same torch.dtype as this tensor.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, same torch.device as this tensor.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor new_full<T>(Shape size, T fill_value, Dtype dtype = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                size,
                fill_value,
            });
            var kwargs=new PyDict();
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("new_full", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a Tensor of size size filled with uninitialized data.
        /// By default, the returned Tensor has the same torch.dtype and
        /// torch.device as this tensor.
        /// </summary>
        /// <param name="dtype">
        /// the desired type of returned tensor.
        /// Default: if None, same torch.dtype as this tensor.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, same torch.device as this tensor.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor new_empty(Shape size, Dtype dtype = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                size,
            });
            var kwargs=new PyDict();
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("new_empty", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a Tensor of size size filled with 1.
        /// By default, the returned Tensor has the same torch.dtype and
        /// torch.device as this tensor.
        /// </summary>
        /// <param name="size">
        /// a list, tuple, or torch.Size of integers defining the
        /// shape of the output tensor.
        /// </param>
        /// <param name="dtype">
        /// the desired type of returned tensor.
        /// Default: if None, same torch.dtype as this tensor.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, same torch.device as this tensor.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor new_ones(Shape size, Dtype dtype = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                size,
            });
            var kwargs=new PyDict();
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("new_ones", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a Tensor of size size filled with 0.
        /// By default, the returned Tensor has the same torch.dtype and
        /// torch.device as this tensor.
        /// </summary>
        /// <param name="size">
        /// a list, tuple, or torch.Size of integers defining the
        /// shape of the output tensor.
        /// </param>
        /// <param name="dtype">
        /// the desired type of returned tensor.
        /// Default: if None, same torch.dtype as this tensor.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, same torch.device as this tensor.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor new_zeros(Shape size, Dtype dtype = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                size,
            });
            var kwargs=new PyDict();
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("new_zeros", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Computes the gradient of current tensor w.r.t. graph leaves.
        /// 
        /// The graph is differentiated using the chain rule. If the tensor is
        /// non-scalar (i.e. its data has more than one element) and requires
        /// gradient, the function additionally requires specifying gradient.
        /// It should be a tensor of matching type and location, that contains
        /// the gradient of the differentiated function w.r.t. self.
        /// 
        /// This function accumulates gradients in the leaves - you might need to
        /// zero them before calling it.
        /// </summary>
        /// <param name="gradient">
        /// Gradient w.r.t. the
        /// tensor. If it is a tensor, it will be automatically converted
        /// to a Tensor that does not require grad unless create_graph is True.
        /// None values can be specified for scalar Tensors or ones that
        /// don’t require grad. If a None value would be acceptable then
        /// this argument is optional.
        /// </param>
        /// <param name="retain_graph">
        /// If False, the graph used to compute
        /// the grads will be freed. Note that in nearly all cases setting
        /// this option to True is not needed and often can be worked around
        /// in a much more efficient way. Defaults to the value of
        /// create_graph.
        /// </param>
        /// <param name="create_graph">
        /// If True, graph of the derivative will
        /// be constructed, allowing to compute higher order derivative
        /// products. Defaults to False.
        /// </param>
        public void backward(Tensor gradient = null, bool? retain_graph = null, bool? create_graph = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
            });
            var kwargs=new PyDict();
            if (gradient!=null) kwargs["gradient"]=ToPython(gradient);
            if (retain_graph!=null) kwargs["retain_graph"]=ToPython(retain_graph);
            if (create_graph!=null) kwargs["create_graph"]=ToPython(create_graph);
            dynamic py = __self__.InvokeMethod("backward", pyargs, kwargs);
        }
        
        /// <summary>
        /// self.byte() is equivalent to self.to(torch.uint8). See to().
        /// </summary>
        public Tensor @byte()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("byte");
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Fills the tensor with numbers drawn from the Cauchy distribution:
        /// 
        /// \[f(x) = \dfrac{1}{\pi} \dfrac{\sigma}{(x - \text{median})^2 + \sigma^2}\]
        /// </summary>
        public Tensor cauchy_(double median = 0, double sigma = 1, object generator = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
            });
            var kwargs=new PyDict();
            if (median!=0) kwargs["median"]=ToPython(median);
            if (sigma!=1) kwargs["sigma"]=ToPython(sigma);
            if (generator!=null) kwargs["generator"]=ToPython(generator);
            dynamic py = __self__.InvokeMethod("cauchy_", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// self.char() is equivalent to self.to(torch.int8). See to().
        /// </summary>
        public Tensor @char()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("char");
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a copy of the self tensor. The copy has the same size and data
        /// type as self.
        /// 
        /// Note
        /// Unlike copy_(), this function is recorded in the computation graph. Gradients
        /// propagating to the cloned tensor will propagate to the original tensor.
        /// </summary>
        public Tensor clone()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("clone");
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a contiguous tensor containing the same data as self tensor. If
        /// self tensor is contiguous, this function returns the self
        /// tensor.
        /// </summary>
        public Tensor contiguous()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("contiguous");
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Copies the elements from src into self tensor and returns
        /// self.
        /// 
        /// The src tensor must be broadcastable
        /// with the self tensor. It may be of a different data type or reside on a
        /// different device.
        /// </summary>
        /// <param name="src">
        /// the source tensor to copy from
        /// </param>
        /// <param name="non_blocking">
        /// if True and this copy is between CPU and GPU,
        /// the copy may occur asynchronously with respect to the host. For other
        /// cases, this argument has no effect.
        /// </param>
        public Tensor copy_(Tensor src, bool non_blocking = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                src,
            });
            var kwargs=new PyDict();
            if (non_blocking!=false) kwargs["non_blocking"]=ToPython(non_blocking);
            dynamic py = __self__.InvokeMethod("copy_", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a copy of this object in CPU memory.
        /// 
        /// If this object is already in CPU memory and on the correct device,
        /// then no copy is performed and the original object is returned.
        /// </summary>
        public Tensor cpu()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("cpu");
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a copy of this object in CUDA memory.
        /// 
        /// If this object is already in CUDA memory and on the correct device,
        /// then no copy is performed and the original object is returned.
        /// </summary>
        /// <param name="device">
        /// The destination GPU device.
        /// Defaults to the current CUDA device.
        /// </param>
        /// <param name="non_blocking">
        /// If True and the source is in pinned memory,
        /// the copy will be asynchronous with respect to the host.
        /// Otherwise, the argument has no effect. Default: False.
        /// </param>
        public Tensor cuda(Device device = null, bool non_blocking = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
            });
            var kwargs=new PyDict();
            if (device!=null) kwargs["device"]=ToPython(device);
            if (non_blocking!=false) kwargs["non_blocking"]=ToPython(non_blocking);
            dynamic py = __self__.InvokeMethod("cuda", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns the address of the first element of self tensor.
        /// </summary>
        public int data_ptr()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("data_ptr");
            return ToCsharp<int>(py);
        }
        
        /// <summary>
        /// Given a quantized Tensor, dequantize it and return the dequantized float Tensor.
        /// </summary>
        public Tensor dequantize()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("dequantize");
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// If self is a sparse COO tensor (i.e., with torch.sparse_coo layout),
        /// this returns a the number of dense dimensions. Otherwise, this throws an
        /// error.
        /// 
        /// See also Tensor.sparse_dim().
        /// </summary>
        public int dense_dim()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("dense_dim");
            return ToCsharp<int>(py);
        }
        
        /// <summary>
        /// Returns a new Tensor, detached from the current graph.
        /// 
        /// The result will never require gradient.
        /// 
        /// Note
        /// Returned Tensor shares the same storage with the original one.
        /// In-place modifications on either of them will be seen, and may trigger
        /// errors in correctness checks.
        /// IMPORTANT NOTE: Previously, in-place size / stride / storage changes
        /// (such as resize_ / resize_as_ / set_ / transpose_) to the returned tensor
        /// also update the original tensor. Now, these in-place changes will not update the
        /// original tensor anymore, and will instead trigger an error.
        /// For sparse tensors:
        /// In-place indices / values changes (such as zero_ / copy_ / add_) to the
        /// returned tensor will not update the original tensor anymore, and will instead
        /// trigger an error.
        /// </summary>
        public void detach()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("detach");
        }
        
        /// <summary>
        /// Detaches the Tensor from the graph that created it, making it a leaf.
        /// Views cannot be detached in-place.
        /// </summary>
        public void detach_()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("detach_");
        }
        
        /// <summary>
        /// Returns the number of dimensions of self tensor.
        /// </summary>
        public int dim()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("dim");
            return ToCsharp<int>(py);
        }
        
        /// <summary>
        /// self.double() is equivalent to self.to(torch.float64). See to().
        /// </summary>
        public Tensor @double()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("double");
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns the size in bytes of an individual element.
        /// </summary>
        public int element_size()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("element_size");
            return ToCsharp<int>(py);
        }
        
        /// <summary>
        /// Returns a new view of the self tensor with singleton dimensions expanded
        /// to a larger size.
        /// 
        /// Passing -1 as the size for a dimension means not changing the size of
        /// that dimension.
        /// 
        /// Tensor can be also expanded to a larger number of dimensions, and the
        /// new ones will be appended at the front. For the new dimensions, the
        /// size cannot be set to -1.
        /// 
        /// Expanding a tensor does not allocate new memory, but only creates a
        /// new view on the existing tensor where a dimension of size one is
        /// expanded to a larger size by setting the stride to 0. Any dimension
        /// of size 1 can be expanded to an arbitrary value without allocating new
        /// memory.
        /// </summary>
        public Tensor expand(params int[] sizes)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                sizes,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("expand", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Expand this tensor to the same size as other.
        /// self.expand_as(other) is equivalent to self.expand(other.size()).
        /// 
        /// Please see expand() for more information about expand.
        /// </summary>
        public Tensor expand_as(Tensor other)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                other,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("expand_as", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Fills self tensor with elements drawn from the exponential distribution:
        /// 
        /// \[f(x) = \lambda e^{-\lambda x}\]
        /// </summary>
        public Tensor exponential_(double lambd = 1, object generator = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
            });
            var kwargs=new PyDict();
            if (lambd!=1) kwargs["lambd"]=ToPython(lambd);
            if (generator!=null) kwargs["generator"]=ToPython(generator);
            dynamic py = __self__.InvokeMethod("exponential_", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Fills self tensor with the specified value.
        /// </summary>
        public Tensor fill_<T>(T @value)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                @value,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("fill_", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// self.float() is equivalent to self.to(torch.float32). See to().
        /// </summary>
        public Tensor @float()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("float");
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Fills self tensor with elements drawn from the geometric distribution:
        /// 
        /// \[f(X=k) = (1 - p)^{k - 1} p\]
        /// </summary>
        public Tensor geometric_(double p, object generator = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                p,
            });
            var kwargs=new PyDict();
            if (generator!=null) kwargs["generator"]=ToPython(generator);
            dynamic py = __self__.InvokeMethod("geometric_", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.
        /// For CPU tensors, an error is thrown.
        /// </summary>
        public int get_device_nr()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("get_device_nr");
            return ToCsharp<int>(py);
        }
        
        /// <summary>
        /// self.half() is equivalent to self.to(torch.float16). See to().
        /// </summary>
        public Tensor half()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("half");
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Accumulate the elements of tensor into the self tensor by adding
        /// to the indices in the order given in index. For example, if dim == 0
        /// and index[i] == j, then the ith row of tensor is added to the
        /// jth row of self.
        /// 
        /// The dimth dimension of tensor must have the same size as the
        /// length of index (which must be a vector), and all other dimensions must
        /// match self, or an error will be raised.
        /// 
        /// Note
        /// When using the CUDA backend, this operation may induce nondeterministic
        /// behaviour that is not easily switched off.
        /// Please see the notes on Reproducibility for background.
        /// </summary>
        /// <param name="dim">
        /// dimension along which to index
        /// </param>
        /// <param name="index">
        /// indices of tensor to select from
        /// </param>
        /// <param name="tensor">
        /// the tensor containing values to add
        /// </param>
        public Tensor index_add_(int dim, Tensor<long> index, Tensor tensor)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                dim,
                index,
                tensor,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("index_add_", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Out-of-place version of torch.Tensor.index_add_()
        /// </summary>
        public Tensor index_add(int dim, Tensor<long> index, Tensor tensor)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                dim,
                index,
                tensor,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("index_add", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Copies the elements of tensor into the self tensor by selecting
        /// the indices in the order given in index. For example, if dim == 0
        /// and index[i] == j, then the ith row of tensor is copied to the
        /// jth row of self.
        /// 
        /// The dimth dimension of tensor must have the same size as the
        /// length of index (which must be a vector), and all other dimensions must
        /// match self, or an error will be raised.
        /// </summary>
        /// <param name="dim">
        /// dimension along which to index
        /// </param>
        /// <param name="index">
        /// indices of tensor to select from
        /// </param>
        /// <param name="tensor">
        /// the tensor containing values to copy
        /// </param>
        public Tensor index_copy_(int dim, Tensor<long> index, Tensor tensor)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                dim,
                index,
                tensor,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("index_copy_", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Out-of-place version of torch.Tensor.index_copy_()
        /// </summary>
        public Tensor index_copy(int dim, Tensor<long> index, Tensor tensor)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                dim,
                index,
                tensor,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("index_copy", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Fills the elements of the self tensor with value val by
        /// selecting the indices in the order given in index.
        /// </summary>
        /// <param name="dim">
        /// dimension along which to index
        /// </param>
        /// <param name="index">
        /// indices of self tensor to fill in
        /// </param>
        /// <param name="val">
        /// the value to fill with
        /// </param>
        public Tensor index_fill_(int dim, Tensor<long> index, float val)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                dim,
                index,
                val,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("index_fill_", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Out-of-place version of torch.Tensor.index_fill_()
        /// </summary>
        public Tensor index_fill(int dim, Tensor<long> index, float @value)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                dim,
                index,
                @value,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("index_fill", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Puts values from the tensor value into the tensor self using
        /// the indices specified in indices (which is a tuple of Tensors). The
        /// expression tensor.index_put_(indices, value) is equivalent to
        /// tensor[indices] = value. Returns self.
        /// 
        /// If accumulate is True, the elements in tensor are added to
        /// self. If accumulate is False, the behavior is undefined if indices
        /// contain duplicate elements.
        /// </summary>
        /// <param name="indices">
        /// tensors used to index into self.
        /// </param>
        /// <param name="@value">
        /// tensor of same dtype as self.
        /// </param>
        /// <param name="accumulate">
        /// whether to accumulate into self
        /// </param>
        public Tensor index_put_(Tensor<long>[] indices, Tensor @value, bool accumulate = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                indices,
                @value,
            });
            var kwargs=new PyDict();
            if (accumulate!=false) kwargs["accumulate"]=ToPython(accumulate);
            dynamic py = __self__.InvokeMethod("index_put_", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Out-place version of index_put_()
        /// </summary>
        public Tensor index_put(Tensor<long>[] indices, Tensor @value, bool accumulate = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                indices,
                @value,
            });
            var kwargs=new PyDict();
            if (accumulate!=false) kwargs["accumulate"]=ToPython(accumulate);
            dynamic py = __self__.InvokeMethod("index_put", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// If self is a sparse COO tensor (i.e., with torch.sparse_coo layout),
        /// this returns a view of the contained indices tensor. Otherwise, this throws an
        /// error.
        /// 
        /// See also Tensor.values().
        /// 
        /// Note
        /// This method can only be called on a coalesced sparse tensor. See
        /// Tensor.coalesce() for details.
        /// </summary>
        public Tensor indices()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("indices");
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// self.int() is equivalent to self.to(torch.int32). See to().
        /// </summary>
        public Tensor @int()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("int");
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Given a quantized Tensor,
        /// self.int_repr() returns a CPU Tensor with uint8_t as data type that stores the
        /// underlying uint8_t values of the given Tensor.
        /// </summary>
        public Tensor int_repr()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("int_repr");
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns True if self tensor is contiguous in memory in C order.
        /// </summary>
        public bool is_contiguous()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("is_contiguous");
            return ToCsharp<bool>(py);
        }
        
        /// <summary>
        /// Returns True if the data type of self is a floating point data type.
        /// </summary>
        public (bool, bool) is_floating_point()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("is_floating_point");
            var t = py as PyTuple;
            return (ToCsharp<bool>(t[0]), ToCsharp<bool>(t[1]));
        }
        
        /// <summary>
        /// All Tensors that have requires_grad which is False will be leaf Tensors by convention.
        /// 
        /// For Tensors that have requires_grad which is True, they will be leaf Tensors if they were
        /// created by the user. This means that they are not the result of an operation and so
        /// grad_fn is None.
        /// 
        /// Only leaf Tensors will have their grad populated during a call to backward().
        /// To get grad populated for non-leaf Tensors, you can use retain_grad().
        /// </summary>
        public void is_leaf()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("is_leaf");
        }
        
        /// <summary>
        /// Returns true if this tensor resides in pinned memory
        /// </summary>
        public void is_pinned()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("is_pinned");
        }
        
        /// <summary>
        /// Returns True if this object refers to the same THTensor object from the
        /// Torch C API as the given tensor.
        /// </summary>
        public bool is_set_to(Tensor tensor)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                tensor,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("is_set_to", pyargs, kwargs);
            return ToCsharp<bool>(py);
        }
        
        /// <summary>
        /// Checks if tensor is in shared memory.
        /// 
        /// This is always True for CUDA tensors.
        /// </summary>
        public void is_shared()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("is_shared");
        }
        
        /// <summary>
        /// Returns True if the data type of self is a signed data type.
        /// </summary>
        public bool is_signed()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("is_signed");
            return ToCsharp<bool>(py);
        }
        
        public void is_sparse()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("is_sparse");
        }
        
        /// <summary>
        /// Fills self tensor with numbers samples from the log-normal distribution
        /// parameterized by the given mean \(\mu\) and standard deviation
        /// \(\sigma\). Note that mean and std are the mean and
        /// standard deviation of the underlying normal distribution, and not of the
        /// returned distribution:
        /// 
        /// \[f(x) = \dfrac{1}{x \sigma \sqrt{2\pi}}\ e^{-\frac{(\ln x - \mu)^2}{2\sigma^2}}\]
        /// </summary>
        public void log_normal_(double mean = 1, double std = 2, object generator = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
            });
            var kwargs=new PyDict();
            if (mean!=1) kwargs["mean"]=ToPython(mean);
            if (std!=2) kwargs["std"]=ToPython(std);
            if (generator!=null) kwargs["generator"]=ToPython(generator);
            dynamic py = __self__.InvokeMethod("log_normal_", pyargs, kwargs);
        }
        
        /// <summary>
        /// self.long() is equivalent to self.to(torch.int64). See to().
        /// </summary>
        public Tensor @long()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("long");
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Applies callable for each element in self tensor and the given
        /// tensor and stores the results in self tensor. self tensor and
        /// the given tensor must be broadcastable.
        /// 
        /// The callable should have the signature:
        /// 
        /// def callable(a, b) -&gt; number
        /// </summary>
        public void map_(Tensor tensor, Delegate callable)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                tensor,
                callable,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("map_", pyargs, kwargs);
        }
        
        /// <summary>
        /// Copies elements from source into self tensor at positions where
        /// the mask is one.
        /// The shape of mask must be broadcastable
        /// with the shape of the underlying tensor. The source should have at least
        /// as many elements as the number of ones in mask
        /// </summary>
        /// <param name="mask">
        /// the binary mask
        /// </param>
        /// <param name="source">
        /// the tensor to copy from
        /// </param>
        public void masked_scatter_(Tensor<byte> mask, Tensor source)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                mask,
                source,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("masked_scatter_", pyargs, kwargs);
        }
        
    }
}
