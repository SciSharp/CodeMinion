// Code generated by CodeMinion: https://github.com/SciSharp/CodeMinion

using System;
using System.Collections;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Runtime.InteropServices;
using System.Text;
using Python.Runtime;
using Numpy;
using Numpy.Models;

namespace Torch
{
    public static partial class torch {
        public static partial class nn {
            public static partial class parallel {
                /// <summary>
                ///	Implements distributed data parallelism that is based on
                ///	torch.distributed package at the module level.<br></br>
                ///	
                ///	This container parallelizes the application of the given module by
                ///	splitting the input across the specified devices by chunking in the batch
                ///	dimension.<br></br>
                ///	 The module is replicated on each machine and each device, and
                ///	each such replica handles a portion of the input.<br></br>
                ///	 During the backwards
                ///	pass, gradients from each node are averaged.<br></br>
                ///	
                ///	The batch size should be larger than the number of GPUs used locally.<br></br>
                ///	
                ///	See also: Basics and Use nn.DataParallel instead of multiprocessing.<br></br>
                ///	
                ///	The same constraints on input as in torch.nn.DataParallel apply.<br></br>
                ///	
                ///	Creation of this class requires that torch.distributed to be already
                ///	initialized, by calling torch.distributed.init_process_group().<br></br>
                ///	
                ///	DistributedDataParallel can be used in the following two ways:
                ///	
                ///	Single-Process Multi-GPU
                ///	
                ///	In this case, a single process will be
                ///	spawned on each host/node and each process will operate on all the GPUs
                ///	of the node where it’s running.<br></br>
                ///	 To use DistributedDataParallel in
                ///	this way, you can simply construct the model as the following:
                ///	
                ///	&gt;&gt;&gt; torch.distributed.init_process_group(backend=&quot;nccl&quot;)
                ///	&gt;&gt;&gt; model = DistributedDataParallel(model) # device_ids will include all GPU devices by default
                ///	
                ///	Multi-Process Single-GPU
                ///	
                ///	This is the highly recommended way to use DistributedDataParallel, with
                ///	multiple processes, each of which operates on a single GPU.<br></br>
                ///	 This is
                ///	currently the fastest approach to do data parallel training using PyTorch
                ///	and applies to both single-node(multi-GPU) and multi-node data
                ///	parallel training.<br></br>
                ///	 It is proven to be significantly faster than
                ///	torch.nn.DataParallel for single-node multi-GPU data
                ///	parallel training.<br></br>
                ///	
                ///	Here is how to use it: on each host with N GPUs, you should spawn up N
                ///	processes, while ensuring that each process individually works on a single GPU
                ///	from 0 to N-1. Therefore, it is your job to ensure that your training script
                ///	operates on a single given GPU by calling:
                ///	
                ///	&gt;&gt;&gt; torch.cuda.set_device(i)
                ///	
                ///	where i is from 0 to N-1. In each process, you should refer the following
                ///	to construct this module:
                ///	
                ///	&gt;&gt;&gt; torch.distributed.init_process_group(backend=&#39;nccl&#39;, world_size=4, init_method=&#39;...&#39;)
                ///	&gt;&gt;&gt; model = DistributedDataParallel(model, device_ids=[i], output_device=i)
                ///	
                ///	In order to spawn up multiple processes per node, you can use either
                ///	torch.distributed.launch or torch.multiprocessing.spawn
                ///	
                ///	Note
                ///	nccl backend is currently the fastest and
                ///	highly recommended backend to be used with Multi-Process Single-GPU
                ///	distributed training and this applies to both single-node and multi-node
                ///	distributed training
                ///	
                ///	Note
                ///	This module also supports mixed-precision distributed training.<br></br>
                ///	
                ///	This means that your model can have different types of parameters such
                ///	as mixed types of fp16 and fp32, the gradient reduction on these
                ///	mixed types of parameters will just work fine.<br></br>
                ///	
                ///	Also note that nccl backend is currently the fastest and highly
                ///	recommended backend for fp16/fp32 mixed-precision training.<br></br>
                ///	
                ///	Note
                ///	If you use torch.save on one process to checkpoint the module,
                ///	and torch.load on some other processes to recover it, make sure that
                ///	map_location is configured properly for every process.<br></br>
                ///	 Without
                ///	map_location, torch.load would recover the module to devices
                ///	where the module was saved from.<br></br>
                ///	
                ///	Warning
                ///	This module works only with the gloo and nccl backends.<br></br>
                ///	
                ///	Warning
                ///	Constructor, forward method, and differentiation of the output (or a
                ///	function of the output of this module) is a distributed synchronization
                ///	point.<br></br>
                ///	 Take that into account in case different processes might be
                ///	executing different code.<br></br>
                ///	
                ///	Warning
                ///	This module assumes all parameters are registered in the model by the
                ///	time it is created.<br></br>
                ///	 No parameters should be added nor removed later.<br></br>
                ///	
                ///	Same applies to buffers.<br></br>
                ///	
                ///	Warning
                ///	This module assumes all parameters are registered in the model of each
                ///	distributed processes are in the same order.<br></br>
                ///	 The module itself will
                ///	conduct gradient all-reduction following the reverse order of the
                ///	registered parameters of the model.<br></br>
                ///	 In other words, it is users’
                ///	responsibility to ensure that each distributed process has the exact
                ///	same model and thus the exact same parameter registration order.<br></br>
                ///	
                ///	Warning
                ///	This module assumes all buffers and gradients are dense.<br></br>
                ///	
                ///	Warning
                ///	This module doesn’t work with torch.autograd.grad() (i.e.<br></br>
                ///	 it will
                ///	only work if gradients are to be accumulated in .grad attributes of
                ///	parameters).<br></br>
                ///	
                ///	Warning
                ///	If you plan on using this module with a nccl backend or a gloo
                ///	backend (that uses Infiniband), together with a DataLoader that uses
                ///	multiple workers, please change the multiprocessing start method to
                ///	forkserver (Python 3 only) or spawn.<br></br>
                ///	 Unfortunately
                ///	Gloo (that uses Infiniband) and NCCL2 are not fork safe, and you will
                ///	likely experience deadlocks if you don’t change this setting.<br></br>
                ///	
                ///	Warning
                ///	Forward and backward hooks defined on module and its submodules
                ///	won’t be invoked anymore, unless the hooks are initialized in the
                ///	forward() method.<br></br>
                ///	
                ///	Warning
                ///	You should never try to change your model’s parameters after wrapping
                ///	up your model with DistributedDataParallel.<br></br>
                ///	 In other words, when
                ///	wrapping up your model with DistributedDataParallel, the constructor of
                ///	DistributedDataParallel will register the additional gradient
                ///	reduction functions on all the parameters of the model itself at the
                ///	time of construction.<br></br>
                ///	 If you change the model’s parameters after
                ///	the DistributedDataParallel construction, this is not supported and
                ///	unexpected behaviors can happen, since some parameters’ gradient
                ///	reduction functions might not get called.<br></br>
                ///	
                ///	Note
                ///	Parameters are never broadcast between processes.<br></br>
                ///	 The module performs
                ///	an all-reduce step on gradients and assumes that they will be modified
                ///	by the optimizer in all processes in the same way.<br></br>
                ///	 Buffers
                ///	(e.g.<br></br>
                ///	 BatchNorm stats) are broadcast from the module in process of rank
                ///	0, to all other replicas in the system in every iteration.
                /// </summary>
                public partial class DistributedDataParallel : Module
                {
                    // auto-generated class
                    
                    public DistributedDataParallel(PyObject pyobj) : base(pyobj) { }
                    
                    public DistributedDataParallel(Module other) : base(other.PyObject as PyObject) { }
                    
                    public DistributedDataParallel(Module module, int[] device_ids, int? output_device = null, bool broadcast_buffers = true, PyObject process_group = null, int bucket_cap_mb = 25, bool find_unused_parameters = false, bool check_reduction = false)
                    {
                        //auto-generated code, do not change
                        var nn = self.GetAttr("nn");
                        var parallel = nn.GetAttr("parallel");
                        var __self__=parallel;
                        var pyargs=ToTuple(new object[]
                        {
                            module,
                            device_ids,
                        });
                        var kwargs=new PyDict();
                        if (output_device!=null) kwargs["output_device"]=ToPython(output_device);
                        if (broadcast_buffers!=true) kwargs["broadcast_buffers"]=ToPython(broadcast_buffers);
                        if (process_group!=null) kwargs["process_group"]=ToPython(process_group);
                        if (bucket_cap_mb!=25) kwargs["bucket_cap_mb"]=ToPython(bucket_cap_mb);
                        if (find_unused_parameters!=false) kwargs["find_unused_parameters"]=ToPython(find_unused_parameters);
                        if (check_reduction!=false) kwargs["check_reduction"]=ToPython(check_reduction);
                        dynamic py = __self__.InvokeMethod("DistributedDataParallel", pyargs, kwargs);
                        self=py as PyObject;
                    }
                    public DistributedDataParallel(Module module, Device[] device_ids = null, Device output_device = null, bool broadcast_buffers = true, PyObject process_group = null, int bucket_cap_mb = 25, bool find_unused_parameters = false, bool check_reduction = false)
                    {
                        //auto-generated code, do not change
                        var nn = self.GetAttr("nn");
                        var parallel = nn.GetAttr("parallel");
                        var __self__=parallel;
                        var pyargs=ToTuple(new object[]
                        {
                            module,
                        });
                        var kwargs=new PyDict();
                        if (device_ids!=null) kwargs["device_ids"]=ToPython(device_ids);
                        if (output_device!=null) kwargs["output_device"]=ToPython(output_device);
                        if (broadcast_buffers!=true) kwargs["broadcast_buffers"]=ToPython(broadcast_buffers);
                        if (process_group!=null) kwargs["process_group"]=ToPython(process_group);
                        if (bucket_cap_mb!=25) kwargs["bucket_cap_mb"]=ToPython(bucket_cap_mb);
                        if (find_unused_parameters!=false) kwargs["find_unused_parameters"]=ToPython(find_unused_parameters);
                        if (check_reduction!=false) kwargs["check_reduction"]=ToPython(check_reduction);
                        dynamic py = __self__.InvokeMethod("DistributedDataParallel", pyargs, kwargs);
                        self=py as PyObject;
                    }
                    
                }
            }
        }
    }
    
}
