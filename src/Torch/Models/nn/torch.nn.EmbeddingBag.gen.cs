// Code generated by CodeMinion: https://github.com/SciSharp/CodeMinion

using System;
using System.Collections;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Runtime.InteropServices;
using System.Text;
using Python.Runtime;
using Numpy;
using Numpy.Models;

namespace Torch
{
    public static partial class torch {
        public static partial class nn {
            /// <summary>
            ///	Computes sums or means of ‘bags’ of embeddings, without instantiating the
            ///	intermediate embeddings.<br></br>
            ///	
            ///	For bags of constant length and no per_sample_weights, this class
            ///	
            ///	with mode=&quot;sum&quot; is equivalent to Embedding followed by torch.sum(dim=0),
            ///	with mode=&quot;mean&quot; is equivalent to Embedding followed by torch.mean(dim=0),
            ///	with mode=&quot;max&quot; is equivalent to Embedding followed by torch.max(dim=0).<br></br>
            ///	
            ///	However, EmbeddingBag is much more time and memory efficient than using a chain of these
            ///	operations.<br></br>
            ///	
            ///	EmbeddingBag also supports per-sample weights as an argument to the forward
            ///	pass.<br></br>
            ///	 This scales the output of the Embedding before performing a weighted
            ///	reduction as specified by mode.<br></br>
            ///	 If per_sample_weights` is passed, the
            ///	only supported mode is &quot;sum&quot;, which computes a weighted sum according to
            ///	per_sample_weights.
            /// </summary>
            public partial class EmbeddingBag : Module
            {
                // auto-generated class
                
                public EmbeddingBag(PyObject pyobj) : base(pyobj) { }
                
                public EmbeddingBag(Module other) : base(other.PyObject as PyObject) { }
                
                public EmbeddingBag(int num_embeddings, int embedding_dim, float? max_norm = null, float? norm_type = 2.0f, bool? scale_grad_by_freq = false, string mode = "mean", bool? sparse = false)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var __self__=nn;
                    var pyargs=ToTuple(new object[]
                    {
                        num_embeddings,
                        embedding_dim,
                    });
                    var kwargs=new PyDict();
                    if (max_norm!=null) kwargs["max_norm"]=ToPython(max_norm);
                    if (norm_type!=2.0f) kwargs["norm_type"]=ToPython(norm_type);
                    if (scale_grad_by_freq!=false) kwargs["scale_grad_by_freq"]=ToPython(scale_grad_by_freq);
                    if (mode!="mean") kwargs["mode"]=ToPython(mode);
                    if (sparse!=false) kwargs["sparse"]=ToPython(sparse);
                    dynamic py = __self__.InvokeMethod("EmbeddingBag", pyargs, kwargs);
                    self=py as PyObject;
                }
                
            }
        }
    }
    
}
