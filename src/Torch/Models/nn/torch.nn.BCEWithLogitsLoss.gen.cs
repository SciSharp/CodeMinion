// Code generated by CodeMinion: https://github.com/SciSharp/CodeMinion

using System;
using System.Collections;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Runtime.InteropServices;
using System.Text;
using Python.Runtime;
using Numpy;
using Numpy.Models;

namespace Torch
{
    public static partial class torch {
        public static partial class nn {
            /// <summary>
            ///	This loss combines a Sigmoid layer and the BCELoss in one single
            ///	class.<br></br>
            ///	 This version is more numerically stable than using a plain Sigmoid
            ///	followed by a BCELoss as, by combining the operations into one layer,
            ///	we take advantage of the log-sum-exp trick for numerical stability.<br></br>
            ///	
            ///	The unreduced (i.e.<br></br>
            ///	 with reduction set to 'none') loss can be described as:
            ///	
            ///	\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
            ///	l_n = - w_n \left[ y_n \cdot \log \sigma(x_n)
            ///	+ (1 - y_n) \cdot \log (1 - \sigma(x_n)) \right],
            ///	
            ///	\]
            ///	
            ///	where \(N\) is the batch size.<br></br>
            ///	 If reduction is not 'none'
            ///	(default 'mean'), then
            ///	
            ///	\[\ell(x, y) = \begin{cases}
            ///	    \operatorname{mean}(L), & \text{if reduction} = \text{'mean';}\\
            ///	    \operatorname{sum}(L),  & \text{if reduction} = \text{'sum'.}
            ///	\end{cases}
            ///	
            ///	\]
            ///	
            ///	This is used for measuring the error of a reconstruction in for example
            ///	an auto-encoder.<br></br>
            ///	 Note that the targets t[i] should be numbers
            ///	between 0 and 1.<br></br>
            ///	
            ///	Itâ€™s possible to trade off recall and precision by adding weights to positive examples.<br></br>
            ///	
            ///	In the case of multi-label classification the loss can be described as:
            ///	
            ///	\[\ell_c(x, y) = L_c = \{l_{1,c},\dots,l_{N,c}\}^\top, \quad
            ///	l_{n,c} = - w_{n,c} \left[ p_c y_{n,c} \cdot \log \sigma(x_{n,c})
            ///	+ (1 - y_{n,c}) \cdot \log (1 - \sigma(x_{n,c})) \right],
            ///	
            ///	\]
            ///	
            ///	where \(c\) is the class number (\(c &gt; 1\) for multi-label binary classification,
            ///	\(c = 1\) for single-label binary classification),
            ///	\(n\) is the number of the sample in the batch and
            ///	\(p_c\) is the weight of the positive answer for the class \(c\).<br></br>
            ///	
            ///	\(p_c &gt; 1\) increases the recall, \(p_c &lt; 1\) increases the precision.<br></br>
            ///	
            ///	For example, if a dataset contains 100 positive and 300 negative examples of a single class,
            ///	then pos_weight for the class should be equal to \(\frac{300}{100}=3\).<br></br>
            ///	
            ///	The loss would act as if the dataset contains \(3\times 100=300\) positive examples.<br></br>
            ///	
            ///	Examples:
            ///	
            ///	&gt;&gt;&gt; target = torch.ones([10, 64], dtype=torch.float32)  # 64 classes, batch size = 10
            ///	&gt;&gt;&gt; output = torch.full([10, 64], 0.999)  # A prediction (logit)
            ///	&gt;&gt;&gt; pos_weight = torch.ones([64])  # All weights are equal to 1
            ///	&gt;&gt;&gt; criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)
            ///	&gt;&gt;&gt; criterion(output, target)  # -log(sigmoid(0.999))
            ///	tensor(0.3135)
            /// </summary>
            public partial class BCEWithLogitsLoss : Module
            {
                // auto-generated class
                
                public BCEWithLogitsLoss(PyObject pyobj) : base(pyobj) { }
                
                public BCEWithLogitsLoss(Module other) : base(other.PyObject as PyObject) { }
                
                public BCEWithLogitsLoss(Tensor weight = null, bool? size_average = null, bool? reduce = null, string reduction = "mean", Tensor pos_weight = null)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var __self__=nn;
                    var pyargs=ToTuple(new object[]
                    {
                    });
                    var kwargs=new PyDict();
                    if (weight!=null) kwargs["weight"]=ToPython(weight);
                    if (size_average!=null) kwargs["size_average"]=ToPython(size_average);
                    if (reduce!=null) kwargs["reduce"]=ToPython(reduce);
                    if (reduction!="mean") kwargs["reduction"]=ToPython(reduction);
                    if (pos_weight!=null) kwargs["pos_weight"]=ToPython(pos_weight);
                    dynamic py = __self__.InvokeMethod("BCEWithLogitsLoss", pyargs, kwargs);
                    self=py as PyObject;
                }
                
            }
        }
    }
    
}
