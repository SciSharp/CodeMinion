// Code generated by CodeMinion: https://github.com/SciSharp/CodeMinion

using System;
using System.Collections;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Runtime.InteropServices;
using System.Text;
using Python.Runtime;
using Numpy;
using Numpy.Models;

namespace Torch
{
    public static partial class torch {
        public static partial class nn {
            public static partial class parallel {
                /// <summary>
                ///	Implements distributed data parallelism for CPU at the module level.<br></br>
                ///	
                ///	This module supports the mpi and gloo backends.<br></br>
                ///	
                ///	This container parallelizes the application of the given module by splitting
                ///	the input across the specified devices by chunking in the batch
                ///	dimension.<br></br>
                ///	 The module is replicated on each machine, and each such replica
                ///	handles a portion of the input.<br></br>
                ///	 During the backwards pass, gradients from
                ///	each node are averaged.<br></br>
                ///	
                ///	This module could be used in conjunction with the DistributedSampler,
                ///	(see DistributedSampler)
                ///	which will load a subset of the original dataset for each node with the same
                ///	batch size.<br></br>
                ///	 So strong scaling should be configured like this:
                ///	
                ///	n = 1, batch size = 12
                ///	
                ///	n = 2, batch size = 64
                ///	
                ///	n = 4, batch size = 32
                ///	
                ///	n = 8, batch size = 16
                ///	
                ///	Creation of this class requires the distributed package to be already
                ///	initialized in the process group mode
                ///	(see torch.distributed.init_process_group()).<br></br>
                ///	
                ///	Warning
                ///	Constructor, forward method, and differentiation of the output (or a
                ///	function of the output of this module) is a distributed synchronization
                ///	point.<br></br>
                ///	 Take that into account in case different node might be
                ///	executing different code.<br></br>
                ///	
                ///	Warning
                ///	This module assumes all parameters are registered in the model by the
                ///	time it is created.<br></br>
                ///	 No parameters should be added nor removed later.<br></br>
                ///	
                ///	Warning
                ///	This module assumes all gradients are dense.<br></br>
                ///	
                ///	Warning
                ///	This module doesn’t work with torch.autograd.grad() (i.e.<br></br>
                ///	 it will
                ///	only work if gradients are to be accumulated in .grad attributes of
                ///	parameters).<br></br>
                ///	
                ///	Warning
                ///	Forward and backward hooks defined on module and its submodules
                ///	won’t be invoked anymore, unless the hooks are initialized in the
                ///	forward() method.<br></br>
                ///	
                ///	Note
                ///	Parameters are broadcast between nodes in the __init__() function.<br></br>
                ///	 The
                ///	module performs an all-reduce step on gradients and assumes that they
                ///	will be modified by the optimizer in all nodes in the same way.
                /// </summary>
                public partial class DistributedDataParallelCPU : Module
                {
                   
                    public DistributedDataParallelCPU(PyObject pyobj) : base(pyobj) { }
                                     
                    public DistributedDataParallelCPU(Module module)
                    {
                        var nn = self.GetAttr("nn");
                        var parallel = nn.GetAttr("parallel");
                        var __self__ = parallel;
                        // NOTE: this is the special case if you "cast" a DistributedDataParallelCPU that you get as Module
                        if ((module.PyObject as PyObject).IsInstance(parallel.GetAttr("DistributedDataParallelCPU")))
                        {
                            self = module.PyObject;
                            return;
                        }
                        // this is the constructor for a new DistributedDataParallelCPU
                        var pyargs =ToTuple(new object[]
                        {
                            module,
                        });
                        var kwargs=new PyDict();
                        dynamic py = __self__.InvokeMethod("DistributedDataParallelCPU", pyargs, kwargs);
                        self=py as PyObject;
                    }
                    
                }
            }
        }
    }
    
}
