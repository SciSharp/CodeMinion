// Code generated by CodeMinion: https://github.com/SciSharp/CodeMinion

using System;
using System.Collections;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Runtime.InteropServices;
using System.Text;
using Python.Runtime;
using Numpy;
using Numpy.Models;

namespace Torch
{
    public static partial class torch {
        public static partial class nn {
            /// <summary>
            ///	Efficient softmax approximation as described in
            ///	Efficient softmax approximation for GPUs by Edouard Grave, Armand Joulin,
            ///	Moustapha Cissé, David Grangier, and Hervé Jégou.<br></br>
            ///	
            ///	Adaptive softmax is an approximate strategy for training models with large
            ///	output spaces.<br></br>
            ///	 It is most effective when the label distribution is highly
            ///	imbalanced, for example in natural language modelling, where the word
            ///	frequency distribution approximately follows the Zipf’s law.<br></br>
            ///	
            ///	Adaptive softmax partitions the labels into several clusters, according to
            ///	their frequency.<br></br>
            ///	 These clusters may contain different number of targets
            ///	each.<br></br>
            ///	
            ///	Additionally, clusters containing less frequent labels assign lower
            ///	dimensional embeddings to those labels, which speeds up the computation.<br></br>
            ///	
            ///	For each minibatch, only clusters for which at least one target is
            ///	present are evaluated.<br></br>
            ///	
            ///	The idea is that the clusters which are accessed frequently
            ///	(like the first one, containing most frequent labels), should also be cheap
            ///	to compute – that is, contain a small number of assigned labels.<br></br>
            ///	
            ///	We highly recommend taking a look at the original paper for more details.<br></br>
            ///	
            ///	cutoffs should be an ordered Sequence of integers sorted
            ///	in the increasing order.<br></br>
            ///	
            ///	It controls number of clusters and the partitioning of targets into
            ///	clusters.<br></br>
            ///	 For example setting cutoffs = [10, 100, 1000]
            ///	means that first 10 targets will be assigned
            ///	to the ‘head’ of the adaptive softmax, targets 11, 12, …, 100 will be
            ///	assigned to the first cluster, and targets 101, 102, …, 1000 will be
            ///	assigned to the second cluster, while targets
            ///	1001, 1002, …, n_classes - 1 will be assigned
            ///	to the last, third cluster.<br></br>
            ///	
            ///	div_value is used to compute the size of each additional cluster,
            ///	which is given as
            ///	\(\left\lfloor\frac{in\_features}{div\_value^{idx}}\right\rfloor\),
            ///	where \(idx\) is the cluster index (with clusters
            ///	for less frequent words having larger indices,
            ///	and indices starting from \(1\)).<br></br>
            ///	
            ///	head_bias if set to True, adds a bias term to the ‘head’ of the
            ///	adaptive softmax.<br></br>
            ///	 See paper for details.<br></br>
            ///	 Set to False in the official
            ///	implementation.<br></br>
            ///	
            ///	Warning
            ///	Labels passed as inputs to this module should be sorted accoridng to
            ///	their frequency.<br></br>
            ///	 This means that the most frequent label should be
            ///	represented by the index 0, and the least frequent
            ///	label should be represented by the index n_classes - 1.<br></br>
            ///	
            ///	Note
            ///	This module returns a NamedTuple with output
            ///	and loss fields.<br></br>
            ///	 See further documentation for details.<br></br>
            ///	
            ///	Note
            ///	To compute log-probabilities for all classes, the log_prob
            ///	method can be used.
            /// </summary>
            public partial class AdaptiveLogSoftmaxWithLoss : Module
            {
                // auto-generated class
                
                public AdaptiveLogSoftmaxWithLoss(PyObject pyobj) : base(pyobj) { }
                
                public AdaptiveLogSoftmaxWithLoss(Module other) : base(other.PyObject as PyObject) { }
                
                public AdaptiveLogSoftmaxWithLoss(int in_features, int n_classes, int[] cutoffs, float? div_value = 4.0f, bool? head_bias = false)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var __self__=nn;
                    var pyargs=ToTuple(new object[]
                    {
                        in_features,
                        n_classes,
                        cutoffs,
                    });
                    var kwargs=new PyDict();
                    if (div_value!=4.0f) kwargs["div_value"]=ToPython(div_value);
                    if (head_bias!=false) kwargs["head_bias"]=ToPython(head_bias);
                    dynamic py = __self__.InvokeMethod("AdaptiveLogSoftmaxWithLoss", pyargs, kwargs);
                    self=py as PyObject;
                }
                
                /// <summary>
                ///	Computes log probabilities for all \(n\_classes\)
                /// </summary>
                public void log_prob(Tensor input)
                {
                    //auto-generated code, do not change
                    var __self__=self;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    dynamic py = __self__.InvokeMethod("log_prob", pyargs, kwargs);
                }
                
                /// <summary>
                ///	This is equivalent to self.log_pob(input).argmax(dim=1),
                ///	but is more efficient in some cases.
                /// </summary>
                public Tensor predict(Tensor input)
                {
                    //auto-generated code, do not change
                    var __self__=self;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    dynamic py = __self__.InvokeMethod("predict", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
                
            }
        }
    }
    
}
