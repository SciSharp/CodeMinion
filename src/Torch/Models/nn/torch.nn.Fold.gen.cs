// Code generated by CodeMinion: https://github.com/SciSharp/CodeMinion

using System;
using System.Collections;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Runtime.InteropServices;
using System.Text;
using Python.Runtime;
using Numpy;
using Numpy.Models;

namespace Torch
{
    public static partial class torch {
        public static partial class nn {
            /// <summary>
            ///	Combines an array of sliding local blocks into a large containing
            ///	tensor.<br></br>
            ///	
            ///	Consider a batched input tensor containing sliding local blocks,
            ///	e.g., patches of images, of shape \((N, C \times  \prod(\text{kernel\_size}), L)\),
            ///	where \(N\) is batch dimension, \(C \times \prod(\text{kernel\_size})\)
            ///	is the number of values within a block (a block has \(\prod(\text{kernel\_size})\)
            ///	spatial locations each containing a \(C\)-channeled vector), and
            ///	\(L\) is the total number of blocks.<br></br>
            ///	 (This is exactly the
            ///	same specification as the output shape of Unfold.) This
            ///	operation combines these local blocks into the large output tensor
            ///	of shape \((N, C, \text{output\_size}[0], \text{output\_size}[1], \dots)\)
            ///	by summing the overlapping values.<br></br>
            ///	 Similar to Unfold, the
            ///	arguments must satisfy
            ///	
            ///	\[L = \prod_d \left\lfloor\frac{\text{output\_size}[d] + 2 \times \text{padding}[d] %
            ///	    - \text{dilation}[d] \times (\text{kernel\_size}[d] - 1) - 1}{\text{stride}[d]} + 1\right\rfloor,
            ///	
            ///	\]
            ///	
            ///	where \(d\) is over all spatial dimensions.<br></br>
            ///	
            ///	output_size describes the spatial shape of the large containing
            ///	tensor of the sliding local blocks.<br></br>
            ///	 It is useful to resolve the ambiguity
            ///	when multiple input shapes map to same number of sliding blocks, e.g.,
            ///	with stride &gt; 0.<br></br>
            ///	
            ///	The padding, stride and dilation arguments specify
            ///	how the sliding blocks are retrieved.<br></br>
            ///	
            ///	stride controls the stride for the sliding blocks.<br></br>
            ///	
            ///	padding controls the amount of implicit zero-paddings on both
            ///	sides for padding number of points for each dimension before
            ///	reshaping.<br></br>
            ///	
            ///	dilation controls the spacing between the kernel points; also known as the Ã  trous algorithm.<br></br>
            ///	
            ///	It is harder to describe, but this link has a nice visualization of what dilation does.
            /// </summary>
            public partial class Fold : Module
            {
                // auto-generated class
                
                public Fold(PyObject pyobj) : base(pyobj) { }
                
                public Fold(Module other) : base(other.PyObject as PyObject) { }
                
                public Fold(int output_size, int[] kernel_size, int[] stride, int? padding = 0, int? dilation = 1)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var __self__=nn;
                    var pyargs=ToTuple(new object[]
                    {
                        output_size,
                        kernel_size,
                        stride,
                    });
                    var kwargs=new PyDict();
                    if (padding!=0) kwargs["padding"]=ToPython(padding);
                    if (dilation!=1) kwargs["dilation"]=ToPython(dilation);
                    dynamic py = __self__.InvokeMethod("Fold", pyargs, kwargs);
                    self=py as PyObject;
                }
                
            }
        }
    }
    
}
