// Code generated by CodeMinion: https://github.com/SciSharp/CodeMinion

using System;
using System.Collections;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Runtime.InteropServices;
using System.Text;
using Python.Runtime;
using Numpy;
using Numpy.Models;

namespace Torch
{
    public static partial class torch {
        public static partial class nn {
            /// <summary>
            ///	Applies a 1D convolution over an input signal composed of several input
            ///	planes.<br></br>
            ///	
            ///	In the simplest case, the output value of the layer with input size
            ///	\((N, C_{\text{in}}, L)\) and output \((N, C_{\text{out}}, L_{\text{out}})\) can be
            ///	precisely described as:
            ///	
            ///	\[\text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +
            ///	\sum_{k = 0}^{C_{in} - 1} \text{weight}(C_{\text{out}_j}, k)
            ///	\star \text{input}(N_i, k)
            ///	
            ///	\]
            ///	
            ///	where \(\star\) is the valid cross-correlation operator,
            ///	\(N\) is a batch size, \(C\) denotes a number of channels,
            ///	\(L\) is a length of signal sequence.<br></br>
            ///	
            ///	stride controls the stride for the cross-correlation, a single
            ///	number or a one-element tuple.<br></br>
            ///	
            ///	padding controls the amount of implicit zero-paddings on both sides
            ///	for padding number of points.<br></br>
            ///	
            ///	dilation controls the spacing between the kernel points; also
            ///	known as the Ã  trous algorithm.<br></br>
            ///	 It is harder to describe, but this link
            ///	has a nice visualization of what dilation does.<br></br>
            ///	
            ///	groups controls the connections between inputs and outputs.<br></br>
            ///	
            ///	in_channels and out_channels must both be divisible by
            ///	groups.<br></br>
            ///	 For example,
            ///	
            ///	At groups=1, all inputs are convolved to all outputs.<br></br>
            ///	
            ///	At groups=2, the operation becomes equivalent to having two conv
            ///	layers side by side, each seeing half the input channels,
            ///	and producing half the output channels, and both subsequently
            ///	concatenated.<br></br>
            ///	
            ///	At groups= in_channels, each input channel is convolved with
            ///	its own set of filters,
            ///	of size
            ///	\(\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor\).<br></br>
            ///	
            ///	Note
            ///	Depending of the size of your kernel, several (of the last)
            ///	columns of the input might be lost, because it is a valid
            ///	cross-correlation, and not a full cross-correlation.<br></br>
            ///	
            ///	It is up to the user to add proper padding.<br></br>
            ///	
            ///	Note
            ///	When groups == in_channels and out_channels == K * in_channels,
            ///	where K is a positive integer, this operation is also termed in
            ///	literature as depthwise convolution.<br></br>
            ///	
            ///	In other words, for an input of size \((N, C_{in}, L_{in})\),
            ///	a depthwise convolution with a depthwise multiplier K, can be constructed by arguments
            ///	\((C_\text{in}=C_{in}, C_\text{out}=C_{in} \times K, ..., \text{groups}=C_{in})\).<br></br>
            ///	
            ///	Note
            ///	In some circumstances when using the CUDA backend with CuDNN, this operator
            ///	may select a nondeterministic algorithm to increase performance.<br></br>
            ///	 If this is
            ///	undesirable, you can try to make the operation deterministic (potentially at
            ///	a performance cost) by setting torch.backends.cudnn.deterministic =
            ///	True.<br></br>
            ///	
            ///	Please see the notes on Reproducibility for background.
            /// </summary>
            public partial class Conv1d : Module
            {
                // auto-generated class
                
                public Conv1d(PyObject pyobj) : base(pyobj) { }
                
                public Conv1d(Module other) : base(other.PyObject as PyObject) { }
                
                public Conv1d(int in_channels, int out_channels, int kernel_size, int? stride = 1, int? padding = 0, string padding_mode = "zeros", int? dilation = 1, int? groups = 1, bool? bias = true)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var __self__=nn;
                    var pyargs=ToTuple(new object[]
                    {
                        in_channels,
                        out_channels,
                        kernel_size,
                    });
                    var kwargs=new PyDict();
                    if (stride!=1) kwargs["stride"]=ToPython(stride);
                    if (padding!=0) kwargs["padding"]=ToPython(padding);
                    if (padding_mode!="zeros") kwargs["padding_mode"]=ToPython(padding_mode);
                    if (dilation!=1) kwargs["dilation"]=ToPython(dilation);
                    if (groups!=1) kwargs["groups"]=ToPython(groups);
                    if (bias!=true) kwargs["bias"]=ToPython(bias);
                    dynamic py = __self__.InvokeMethod("Conv1d", pyargs, kwargs);
                    self=py as PyObject;
                }
                
            }
        }
    }
    
}
