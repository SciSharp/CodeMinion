// Code generated by CodeMinion: https://github.com/SciSharp/CodeMinion

using System;
using System.Collections;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Runtime.InteropServices;
using System.Text;
using Python.Runtime;
using Numpy;
using Numpy.Models;

namespace Torch
{
    public static partial class torch
    {
        
        public static partial class nn {
            public static partial class utils {
                /// <summary>
                ///	Clips gradient norm of an iterable of parameters.<br></br>
                ///	
                ///	The norm is computed over all gradients together, as if they were
                ///	concatenated into a single vector.<br></br>
                ///	 Gradients are modified in-place.
                /// </summary>
                /// <param name="parameters">
                ///	an iterable of Tensors or a
                ///	single Tensor that will have gradients normalized
                /// </param>
                /// <param name="max_norm">
                ///	max norm of the gradients
                /// </param>
                /// <param name="norm_type">
                ///	type of the used p-norm.<br></br>
                ///	Can be 'inf' for
                ///	infinity norm.
                /// </param>
                public static void clip_grad_norm_(IEnumerable<Tensor> parameters, float max_norm, float norm_type = 2f)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var utils = nn.GetAttr("utils");
                    var __self__=utils;
                    var pyargs=ToTuple(new object[]
                    {
                        parameters,
                        max_norm,
                    });
                    var kwargs=new PyDict();
                    if (norm_type!=2f) kwargs["norm_type"]=ToPython(norm_type);
                    dynamic py = __self__.InvokeMethod("clip_grad_norm_", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class utils {
                /// <summary>
                ///	Clips gradient of an iterable of parameters at specified value.<br></br>
                ///	
                ///	Gradients are modified in-place.
                /// </summary>
                /// <param name="parameters">
                ///	an iterable of Tensors or a
                ///	single Tensor that will have gradients normalized
                /// </param>
                /// <param name="clip_value">
                ///	maximum allowed value of the gradients.<br></br>
                ///	
                ///	The gradients are clipped in the range
                ///	\(\left[\text{-clip\_value}, \text{clip\_value}\right]\)
                /// </param>
                public static void clip_grad_value_(IEnumerable<Tensor> parameters, float clip_value)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var utils = nn.GetAttr("utils");
                    var __self__=utils;
                    var pyargs=ToTuple(new object[]
                    {
                        parameters,
                        clip_value,
                    });
                    var kwargs=new PyDict();
                    dynamic py = __self__.InvokeMethod("clip_grad_value_", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class utils {
                /// <summary>
                ///	Convert parameters to one vector
                /// </summary>
                public static void parameters_to_vector(IEnumerable<Tensor> parameters)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var utils = nn.GetAttr("utils");
                    var __self__=utils;
                    var pyargs=ToTuple(new object[]
                    {
                        parameters,
                    });
                    var kwargs=new PyDict();
                    dynamic py = __self__.InvokeMethod("parameters_to_vector", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class utils {
                /// <summary>
                ///	Convert one vector to the parameters
                /// </summary>
                /// <param name="vec">
                ///	a single vector represents the parameters of a model.
                /// </param>
                /// <param name="parameters">
                ///	an iterator of Tensors that are the
                ///	parameters of a model.
                /// </param>
                public static void vector_to_parameters(Tensor vec, IEnumerable<Tensor> parameters)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var utils = nn.GetAttr("utils");
                    var __self__=utils;
                    var pyargs=ToTuple(new object[]
                    {
                        vec,
                        parameters,
                    });
                    var kwargs=new PyDict();
                    dynamic py = __self__.InvokeMethod("vector_to_parameters", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class utils {
                /// <summary>
                ///	Applies weight normalization to a parameter in the given module.<br></br>
                ///	
                ///	\[\mathbf{w} = g \dfrac{\mathbf{v}}{\|\mathbf{v}\|}
                ///	
                ///	\]
                ///	
                ///	Weight normalization is a reparameterization that decouples the magnitude
                ///	of a weight tensor from its direction.<br></br>
                ///	 This replaces the parameter specified
                ///	by name (e.g.<br></br>
                ///	 'weight') with two parameters: one specifying the magnitude
                ///	(e.g.<br></br>
                ///	 'weight_g') and one specifying the direction (e.g.<br></br>
                ///	 'weight_v').<br></br>
                ///	
                ///	Weight normalization is implemented via a hook that recomputes the weight
                ///	tensor from the magnitude and direction before every forward()
                ///	call.<br></br>
                ///	
                ///	By default, with dim=0, the norm is computed independently per output
                ///	channel/plane.<br></br>
                ///	 To compute a norm over the entire weight tensor, use
                ///	dim=None.<br></br>
                ///	
                ///	See https://arxiv.org/abs/1602.07868
                /// </summary>
                /// <param name="module">
                ///	containing module
                /// </param>
                /// <param name="name">
                ///	name of weight parameter
                /// </param>
                /// <param name="dim">
                ///	dimension over which to compute the norm
                /// </param>
                public static void weight_norm(Module module, string name = "weight", int? dim = 0)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var utils = nn.GetAttr("utils");
                    var __self__=utils;
                    var pyargs=ToTuple(new object[]
                    {
                        module,
                    });
                    var kwargs=new PyDict();
                    if (name!="weight") kwargs["name"]=ToPython(name);
                    if (dim!=0) kwargs["dim"]=ToPython(dim);
                    dynamic py = __self__.InvokeMethod("weight_norm", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class utils {
                /// <summary>
                ///	Removes the weight normalization reparameterization from a module.
                /// </summary>
                /// <param name="module">
                ///	containing module
                /// </param>
                /// <param name="name">
                ///	name of weight parameter
                /// </param>
                public static void remove_weight_norm(Module module, string name = "weight")
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var utils = nn.GetAttr("utils");
                    var __self__=utils;
                    var pyargs=ToTuple(new object[]
                    {
                        module,
                    });
                    var kwargs=new PyDict();
                    if (name!="weight") kwargs["name"]=ToPython(name);
                    dynamic py = __self__.InvokeMethod("remove_weight_norm", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class utils {
                /// <summary>
                ///	Applies spectral normalization to a parameter in the given module.<br></br>
                ///	
                ///	\[\mathbf{W}_{SN} = \dfrac{\mathbf{W}}{\sigma(\mathbf{W})},
                ///	\sigma(\mathbf{W}) = \max_{\mathbf{h}: \mathbf{h} \ne 0} \dfrac{\|\mathbf{W} \mathbf{h}\|_2}{\|\mathbf{h}\|_2}
                ///	
                ///	\]
                ///	
                ///	Spectral normalization stabilizes the training of discriminators (critics)
                ///	in Generative Adversarial Networks (GANs) by rescaling the weight tensor
                ///	with spectral norm \(\sigma\) of the weight matrix calculated using
                ///	power iteration method.<br></br>
                ///	 If the dimension of the weight tensor is greater
                ///	than 2, it is reshaped to 2D in power iteration method to get spectral
                ///	norm.<br></br>
                ///	 This is implemented via a hook that calculates spectral norm and
                ///	rescales weight before every forward() call.<br></br>
                ///	
                ///	See Spectral Normalization for Generative Adversarial Networks .
                /// </summary>
                /// <param name="module">
                ///	containing module
                /// </param>
                /// <param name="name">
                ///	name of weight parameter
                /// </param>
                /// <param name="n_power_iterations">
                ///	number of power iterations to
                ///	calculate spectral norm
                /// </param>
                /// <param name="eps">
                ///	epsilon for numerical stability in
                ///	calculating norms
                /// </param>
                /// <param name="dim">
                ///	dimension corresponding to number of outputs,
                ///	the default is 0, except for modules that are instances of
                ///	ConvTranspose{1,2,3}d, when it is 1
                /// </param>
                public static void spectral_norm(nn.Module module, string name = "weight", int? n_power_iterations = 1, float? eps = 1e-12f, int? dim = null)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var utils = nn.GetAttr("utils");
                    var __self__=utils;
                    var pyargs=ToTuple(new object[]
                    {
                        module,
                    });
                    var kwargs=new PyDict();
                    if (name!="weight") kwargs["name"]=ToPython(name);
                    if (n_power_iterations!=1) kwargs["n_power_iterations"]=ToPython(n_power_iterations);
                    if (eps!=1e-12f) kwargs["eps"]=ToPython(eps);
                    if (dim!=null) kwargs["dim"]=ToPython(dim);
                    dynamic py = __self__.InvokeMethod("spectral_norm", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class utils {
                /// <summary>
                ///	Removes the spectral normalization reparameterization from a module.
                /// </summary>
                /// <param name="module">
                ///	containing module
                /// </param>
                /// <param name="name">
                ///	name of weight parameter
                /// </param>
                public static void remove_spectral_norm(Module module, string name = "weight")
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var utils = nn.GetAttr("utils");
                    var __self__=utils;
                    var pyargs=ToTuple(new object[]
                    {
                        module,
                    });
                    var kwargs=new PyDict();
                    if (name!="weight") kwargs["name"]=ToPython(name);
                    dynamic py = __self__.InvokeMethod("remove_spectral_norm", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class utils {
                public static partial class rnn {
                    /// <summary>
                    ///	Packs a Tensor containing padded sequences of variable length.<br></br>
                    ///	
                    ///	input can be of size T x B x * where T is the length of the
                    ///	longest sequence (equal to lengths[0]), B is the batch size, and
                    ///	* is any number of dimensions (including 0).<br></br>
                    ///	 If batch_first is
                    ///	True, B x T x * input is expected.<br></br>
                    ///	
                    ///	For unsorted sequences, use enforce_sorted = False.<br></br>
                    ///	 If enforce_sorted is
                    ///	True, the sequences should be sorted by length in a decreasing order, i.e.<br></br>
                    ///	
                    ///	input[:,0] should be the longest sequence, and input[:,B-1] the shortest
                    ///	one.<br></br>
                    ///	 enforce_sorted = True is only necessary for ONNX export.<br></br>
                    ///	
                    ///	Note
                    ///	This function accepts any input that has at least two dimensions.<br></br>
                    ///	 You
                    ///	can apply it to pack the labels, and use the output of the RNN with
                    ///	them to compute the loss directly.<br></br>
                    ///	 A Tensor can be retrieved from
                    ///	a PackedSequence object by accessing its .data attribute.
                    /// </summary>
                    /// <param name="input">
                    ///	padded batch of variable length sequences.
                    /// </param>
                    /// <param name="lengths">
                    ///	list of sequences lengths of each batch element.
                    /// </param>
                    /// <param name="batch_first">
                    ///	if True, the input is expected in B x T x *
                    ///	format.
                    /// </param>
                    /// <param name="enforce_sorted">
                    ///	if True, the input is expected to
                    ///	contain sequences sorted by length in a decreasing order.<br></br>
                    ///	If
                    ///	False, this condition is not checked.<br></br>
                    ///	Default: True.
                    /// </param>
                    public static PackedSequence pack_padded_sequence(Tensor input, Tensor lengths, bool? batch_first = false, bool? enforce_sorted = true)
                    {
                        //auto-generated code, do not change
                        var nn = self.GetAttr("nn");
                        var utils = nn.GetAttr("utils");
                        var rnn = utils.GetAttr("rnn");
                        var __self__=rnn;
                        var pyargs=ToTuple(new object[]
                        {
                            input,
                            lengths,
                        });
                        var kwargs=new PyDict();
                        if (batch_first!=false) kwargs["batch_first"]=ToPython(batch_first);
                        if (enforce_sorted!=true) kwargs["enforce_sorted"]=ToPython(enforce_sorted);
                        dynamic py = __self__.InvokeMethod("pack_padded_sequence", pyargs, kwargs);
                        return ToCsharp<PackedSequence>(py);
                    }
                }
            }
        }
        
        public static partial class nn {
            public static partial class utils {
                public static partial class rnn {
                    /// <summary>
                    ///	Pads a packed batch of variable length sequences.<br></br>
                    ///	
                    ///	It is an inverse operation to pack_padded_sequence().<br></br>
                    ///	
                    ///	The returned Tensor’s data will be of size T x B x *, where T is the length
                    ///	of the longest sequence and B is the batch size.<br></br>
                    ///	 If batch_first is True,
                    ///	the data will be transposed into B x T x * format.<br></br>
                    ///	
                    ///	Batch elements will be ordered decreasingly by their length.<br></br>
                    ///	
                    ///	Note
                    ///	total_length is useful to implement the
                    ///	pack sequence -&gt; recurrent network -&gt; unpack sequence pattern in a
                    ///	Module wrapped in DataParallel.<br></br>
                    ///	
                    ///	See this FAQ section for
                    ///	details.
                    /// </summary>
                    /// <param name="sequence">
                    ///	batch to pad
                    /// </param>
                    /// <param name="batch_first">
                    ///	if True, the output will be in B x T x *
                    ///	format.
                    /// </param>
                    /// <param name="padding_value">
                    ///	values for padded elements.
                    /// </param>
                    /// <param name="total_length">
                    ///	if not None, the output will be padded to
                    ///	have length total_length.<br></br>
                    ///	This method will throw ValueError
                    ///	if total_length is less than the max sequence length in
                    ///	sequence.
                    /// </param>
                    public static void pad_packed_sequence(PackedSequence sequence, bool? batch_first = false, float? padding_value = 0.0f, int? total_length = null)
                    {
                        //auto-generated code, do not change
                        var nn = self.GetAttr("nn");
                        var utils = nn.GetAttr("utils");
                        var rnn = utils.GetAttr("rnn");
                        var __self__=rnn;
                        var pyargs=ToTuple(new object[]
                        {
                            sequence,
                        });
                        var kwargs=new PyDict();
                        if (batch_first!=false) kwargs["batch_first"]=ToPython(batch_first);
                        if (padding_value!=0.0f) kwargs["padding_value"]=ToPython(padding_value);
                        if (total_length!=null) kwargs["total_length"]=ToPython(total_length);
                        dynamic py = __self__.InvokeMethod("pad_packed_sequence", pyargs, kwargs);
                    }
                }
            }
        }
        
        public static partial class nn {
            public static partial class utils {
                public static partial class rnn {
                    /// <summary>
                    ///	Pad a list of variable length Tensors with padding_value
                    ///	
                    ///	pad_sequence stacks a list of Tensors along a new dimension,
                    ///	and pads them to equal length.<br></br>
                    ///	 For example, if the input is list of
                    ///	sequences with size L x * and if batch_first is False, and T x B x *
                    ///	otherwise.<br></br>
                    ///	
                    ///	B is batch size.<br></br>
                    ///	 It is equal to the number of elements in sequences.<br></br>
                    ///	
                    ///	T is length of the longest sequence.<br></br>
                    ///	
                    ///	L is length of the sequence.<br></br>
                    ///	
                    ///	* is any number of trailing dimensions, including none.
                    /// </summary>
                    /// <param name="sequences">
                    ///	list of variable length sequences.
                    /// </param>
                    /// <param name="batch_first">
                    ///	output will be in B x T x * if True, or in
                    ///	T x B x * otherwise
                    /// </param>
                    /// <param name="padding_value">
                    ///	value for padded elements.<br></br>
                    ///	Default: 0.
                    /// </param>
                    public static void pad_sequence(Tensor[] sequences, bool? batch_first = false, float? padding_value = 0f)
                    {
                        //auto-generated code, do not change
                        var nn = self.GetAttr("nn");
                        var utils = nn.GetAttr("utils");
                        var rnn = utils.GetAttr("rnn");
                        var __self__=rnn;
                        var pyargs=ToTuple(new object[]
                        {
                            sequences,
                        });
                        var kwargs=new PyDict();
                        if (batch_first!=false) kwargs["batch_first"]=ToPython(batch_first);
                        if (padding_value!=0f) kwargs["padding_value"]=ToPython(padding_value);
                        dynamic py = __self__.InvokeMethod("pad_sequence", pyargs, kwargs);
                    }
                }
            }
        }
        
        public static partial class nn {
            public static partial class utils {
                public static partial class rnn {
                    /// <summary>
                    ///	Packs a list of variable length Tensors
                    ///	
                    ///	sequences should be a list of Tensors of size L x *, where L is
                    ///	the length of a sequence and * is any number of trailing dimensions,
                    ///	including zero.<br></br>
                    ///	
                    ///	For unsorted sequences, use enforce_sorted = False.<br></br>
                    ///	 If enforce_sorted
                    ///	is True, the sequences should be sorted in the order of decreasing length.<br></br>
                    ///	
                    ///	enforce_sorted = True is only necessary for ONNX export.
                    /// </summary>
                    /// <param name="sequences">
                    ///	A list of sequences of decreasing length.
                    /// </param>
                    /// <param name="enforce_sorted">
                    ///	if True, checks that the input
                    ///	contains sequences sorted by length in a decreasing order.<br></br>
                    ///	If
                    ///	False, this condition is not checked.<br></br>
                    ///	Default: True.
                    /// </param>
                    public static void pack_sequence(Tensor[] sequences, bool? enforce_sorted = true)
                    {
                        //auto-generated code, do not change
                        var nn = self.GetAttr("nn");
                        var utils = nn.GetAttr("utils");
                        var rnn = utils.GetAttr("rnn");
                        var __self__=rnn;
                        var pyargs=ToTuple(new object[]
                        {
                            sequences,
                        });
                        var kwargs=new PyDict();
                        if (enforce_sorted!=true) kwargs["enforce_sorted"]=ToPython(enforce_sorted);
                        dynamic py = __self__.InvokeMethod("pack_sequence", pyargs, kwargs);
                    }
                }
            }
        }
        
        
    }
}
