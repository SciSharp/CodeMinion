// Code generated by CodeMinion: https://github.com/SciSharp/CodeMinion

using System;
using System.Collections;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Runtime.InteropServices;
using System.Text;
using Python.Runtime;
using Numpy;
using Numpy.Models;

namespace Torch
{
    public static partial class torch
    {
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies a 1D convolution over an input signal composed of several input
                ///	planes.<br></br>
                ///	
                ///	See Conv1d for details and output shape.<br></br>
                ///	
                ///	Note
                ///	In some circumstances when using the CUDA backend with CuDNN, this operator
                ///	may select a nondeterministic algorithm to increase performance.<br></br>
                ///	 If this is
                ///	undesirable, you can try to make the operation deterministic (potentially at
                ///	a performance cost) by setting torch.backends.cudnn.deterministic =
                ///	True.<br></br>
                ///	
                ///	Please see the notes on Reproducibility for background.
                /// </summary>
                /// <param name="input">
                ///	input tensor of shape \((\text{minibatch} , \text{in\_channels} , iW)\)
                /// </param>
                /// <param name="weight">
                ///	filters of shape \((\text{out\_channels} , \frac{\text{in\_channels}}{\text{groups}} , kW)\)
                /// </param>
                /// <param name="bias">
                ///	optional bias of shape \((\text{out\_channels})\).<br></br>
                ///	Default: None
                /// </param>
                /// <param name="stride">
                ///	the stride of the convolving kernel.<br></br>
                ///	Can be a single number or
                ///	a one-element tuple (sW,).<br></br>
                ///	Default: 1
                /// </param>
                /// <param name="padding">
                ///	implicit paddings on both sides of the input.<br></br>
                ///	Can be a
                ///	single number or a one-element tuple (padW,).<br></br>
                ///	Default: 0
                /// </param>
                /// <param name="dilation">
                ///	the spacing between kernel elements.<br></br>
                ///	Can be a single number or
                ///	a one-element tuple (dW,).<br></br>
                ///	Default: 1
                /// </param>
                /// <param name="groups">
                ///	split input into groups, \(\text{in\_channels}\) should be divisible by
                ///	the number of groups.<br></br>
                ///	Default: 1
                /// </param>
                /// <param name="padding_mode">
                ///	the type of paddings applied to both sided can be: zeros or circular.<br></br>
                ///	Default: zeros
                /// </param>
                public static Tensor conv1d(Tensor input, double weight, Tensor bias = null, int stride = 1, int padding = 0, int dilation = 1, int groups = 1, string padding_mode = "zeros")
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        weight,
                    });
                    var kwargs=new PyDict();
                    if (bias!=null) kwargs["bias"]=ToPython(bias);
                    if (stride!=1) kwargs["stride"]=ToPython(stride);
                    if (padding!=0) kwargs["padding"]=ToPython(padding);
                    if (dilation!=1) kwargs["dilation"]=ToPython(dilation);
                    if (groups!=1) kwargs["groups"]=ToPython(groups);
                    if (padding_mode!="zeros") kwargs["padding_mode"]=ToPython(padding_mode);
                    dynamic py = __self__.InvokeMethod("conv1d", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies a 2D convolution over an input image composed of several input
                ///	planes.<br></br>
                ///	
                ///	See Conv2d for details and output shape.<br></br>
                ///	
                ///	Note
                ///	In some circumstances when using the CUDA backend with CuDNN, this operator
                ///	may select a nondeterministic algorithm to increase performance.<br></br>
                ///	 If this is
                ///	undesirable, you can try to make the operation deterministic (potentially at
                ///	a performance cost) by setting torch.backends.cudnn.deterministic =
                ///	True.<br></br>
                ///	
                ///	Please see the notes on Reproducibility for background.
                /// </summary>
                /// <param name="input">
                ///	input tensor of shape \((\text{minibatch} , \text{in\_channels} , iH , iW)\)
                /// </param>
                /// <param name="weight">
                ///	filters of shape \((\text{out\_channels} , \frac{\text{in\_channels}}{\text{groups}} , kH , kW)\)
                /// </param>
                /// <param name="bias">
                ///	optional bias tensor of shape \((\text{out\_channels})\).<br></br>
                ///	Default: None
                /// </param>
                /// <param name="stride">
                ///	the stride of the convolving kernel.<br></br>
                ///	Can be a single number or a
                ///	tuple (sH, sW).<br></br>
                ///	Default: 1
                /// </param>
                /// <param name="padding">
                ///	implicit paddings on both sides of the input.<br></br>
                ///	Can be a
                ///	single number or a tuple (padH, padW).<br></br>
                ///	Default: 0
                /// </param>
                /// <param name="dilation">
                ///	the spacing between kernel elements.<br></br>
                ///	Can be a single number or
                ///	a tuple (dH, dW).<br></br>
                ///	Default: 1
                /// </param>
                /// <param name="groups">
                ///	split input into groups, \(\text{in\_channels}\) should be divisible by the
                ///	number of groups.<br></br>
                ///	Default: 1
                /// </param>
                /// <param name="padding_mode">
                ///	the type of paddings applied to both sided can be: zeros or circular.<br></br>
                ///	Default: zeros
                /// </param>
                public static Tensor conv2d(Tensor input, double weight, Tensor bias = null, int stride = 1, int padding = 0, int dilation = 1, int groups = 1, string padding_mode = "zeros")
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        weight,
                    });
                    var kwargs=new PyDict();
                    if (bias!=null) kwargs["bias"]=ToPython(bias);
                    if (stride!=1) kwargs["stride"]=ToPython(stride);
                    if (padding!=0) kwargs["padding"]=ToPython(padding);
                    if (dilation!=1) kwargs["dilation"]=ToPython(dilation);
                    if (groups!=1) kwargs["groups"]=ToPython(groups);
                    if (padding_mode!="zeros") kwargs["padding_mode"]=ToPython(padding_mode);
                    dynamic py = __self__.InvokeMethod("conv2d", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies a 3D convolution over an input image composed of several input
                ///	planes.<br></br>
                ///	
                ///	See Conv3d for details and output shape.<br></br>
                ///	
                ///	Note
                ///	In some circumstances when using the CUDA backend with CuDNN, this operator
                ///	may select a nondeterministic algorithm to increase performance.<br></br>
                ///	 If this is
                ///	undesirable, you can try to make the operation deterministic (potentially at
                ///	a performance cost) by setting torch.backends.cudnn.deterministic =
                ///	True.<br></br>
                ///	
                ///	Please see the notes on Reproducibility for background.
                /// </summary>
                /// <param name="input">
                ///	input tensor of shape \((\text{minibatch} , \text{in\_channels} , iT , iH , iW)\)
                /// </param>
                /// <param name="weight">
                ///	filters of shape \((\text{out\_channels} , \frac{\text{in\_channels}}{\text{groups}} , kT , kH , kW)\)
                /// </param>
                /// <param name="bias">
                ///	optional bias tensor of shape \((\text{out\_channels})\).<br></br>
                ///	Default: None
                /// </param>
                /// <param name="stride">
                ///	the stride of the convolving kernel.<br></br>
                ///	Can be a single number or a
                ///	tuple (sT, sH, sW).<br></br>
                ///	Default: 1
                /// </param>
                /// <param name="padding">
                ///	implicit paddings on both sides of the input.<br></br>
                ///	Can be a
                ///	single number or a tuple (padT, padH, padW).<br></br>
                ///	Default: 0
                /// </param>
                /// <param name="dilation">
                ///	the spacing between kernel elements.<br></br>
                ///	Can be a single number or
                ///	a tuple (dT, dH, dW).<br></br>
                ///	Default: 1
                /// </param>
                /// <param name="groups">
                ///	split input into groups, \(\text{in\_channels}\) should be divisible by
                ///	the number of groups.<br></br>
                ///	Default: 1
                /// </param>
                /// <param name="padding_mode">
                ///	the type of paddings applied to both sided can be: zeros or circular.<br></br>
                ///	Default: zeros
                /// </param>
                public static Tensor conv3d(Tensor input, double weight, Tensor bias = null, int stride = 1, int padding = 0, int dilation = 1, int groups = 1, string padding_mode = "zeros")
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        weight,
                    });
                    var kwargs=new PyDict();
                    if (bias!=null) kwargs["bias"]=ToPython(bias);
                    if (stride!=1) kwargs["stride"]=ToPython(stride);
                    if (padding!=0) kwargs["padding"]=ToPython(padding);
                    if (dilation!=1) kwargs["dilation"]=ToPython(dilation);
                    if (groups!=1) kwargs["groups"]=ToPython(groups);
                    if (padding_mode!="zeros") kwargs["padding_mode"]=ToPython(padding_mode);
                    dynamic py = __self__.InvokeMethod("conv3d", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies a 1D transposed convolution operator over an input signal
                ///	composed of several input planes, sometimes also called “deconvolution”.
                ///	
                ///	See ConvTranspose1d for details and output shape.<br></br>
                ///	
                ///	Note
                ///	In some circumstances when using the CUDA backend with CuDNN, this operator
                ///	may select a nondeterministic algorithm to increase performance.<br></br>
                ///	 If this is
                ///	undesirable, you can try to make the operation deterministic (potentially at
                ///	a performance cost) by setting torch.backends.cudnn.deterministic =
                ///	True.<br></br>
                ///	
                ///	Please see the notes on Reproducibility for background.
                /// </summary>
                /// <param name="input">
                ///	input tensor of shape \((\text{minibatch} , \text{in\_channels} , iW)\)
                /// </param>
                /// <param name="weight">
                ///	filters of shape \((\text{in\_channels} , \frac{\text{out\_channels}}{\text{groups}} , kW)\)
                /// </param>
                /// <param name="bias">
                ///	optional bias of shape \((\text{out\_channels})\).<br></br>
                ///	Default: None
                /// </param>
                /// <param name="stride">
                ///	the stride of the convolving kernel.<br></br>
                ///	Can be a single number or a
                ///	tuple (sW,).<br></br>
                ///	Default: 1
                /// </param>
                /// <param name="padding">
                ///	dilation * (kernel_size - 1) - padding zero-padding will be added to both
                ///	sides of each dimension in the input.<br></br>
                ///	Can be a single number or a tuple
                ///	(padW,).<br></br>
                ///	Default: 0
                /// </param>
                /// <param name="output_padding">
                ///	additional size added to one side of each dimension in the
                ///	output shape.<br></br>
                ///	Can be a single number or a tuple (out_padW).<br></br>
                ///	Default: 0
                /// </param>
                /// <param name="groups">
                ///	split input into groups, \(\text{in\_channels}\) should be divisible by the
                ///	number of groups.<br></br>
                ///	Default: 1
                /// </param>
                /// <param name="dilation">
                ///	the spacing between kernel elements.<br></br>
                ///	Can be a single number or
                ///	a tuple (dW,).<br></br>
                ///	Default: 1
                /// </param>
                public static Tensor conv_transpose1d(Tensor input, double weight, Tensor bias = null, int stride = 1, int padding = 0, int output_padding = 0, int groups = 1, int dilation = 1)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        weight,
                    });
                    var kwargs=new PyDict();
                    if (bias!=null) kwargs["bias"]=ToPython(bias);
                    if (stride!=1) kwargs["stride"]=ToPython(stride);
                    if (padding!=0) kwargs["padding"]=ToPython(padding);
                    if (output_padding!=0) kwargs["output_padding"]=ToPython(output_padding);
                    if (groups!=1) kwargs["groups"]=ToPython(groups);
                    if (dilation!=1) kwargs["dilation"]=ToPython(dilation);
                    dynamic py = __self__.InvokeMethod("conv_transpose1d", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies a 2D transposed convolution operator over an input image
                ///	composed of several input planes, sometimes also called “deconvolution”.
                ///	
                ///	See ConvTranspose2d for details and output shape.<br></br>
                ///	
                ///	Note
                ///	In some circumstances when using the CUDA backend with CuDNN, this operator
                ///	may select a nondeterministic algorithm to increase performance.<br></br>
                ///	 If this is
                ///	undesirable, you can try to make the operation deterministic (potentially at
                ///	a performance cost) by setting torch.backends.cudnn.deterministic =
                ///	True.<br></br>
                ///	
                ///	Please see the notes on Reproducibility for background.
                /// </summary>
                /// <param name="input">
                ///	input tensor of shape \((\text{minibatch} , \text{in\_channels} , iH , iW)\)
                /// </param>
                /// <param name="weight">
                ///	filters of shape \((\text{in\_channels} , \frac{\text{out\_channels}}{\text{groups}} , kH , kW)\)
                /// </param>
                /// <param name="bias">
                ///	optional bias of shape \((\text{out\_channels})\).<br></br>
                ///	Default: None
                /// </param>
                /// <param name="stride">
                ///	the stride of the convolving kernel.<br></br>
                ///	Can be a single number or a
                ///	tuple (sH, sW).<br></br>
                ///	Default: 1
                /// </param>
                /// <param name="padding">
                ///	dilation * (kernel_size - 1) - padding zero-padding will be added to both
                ///	sides of each dimension in the input.<br></br>
                ///	Can be a single number or a tuple
                ///	(padH, padW).<br></br>
                ///	Default: 0
                /// </param>
                /// <param name="output_padding">
                ///	additional size added to one side of each dimension in the
                ///	output shape.<br></br>
                ///	Can be a single number or a tuple (out_padH, out_padW).<br></br>
                ///	
                ///	Default: 0
                /// </param>
                /// <param name="groups">
                ///	split input into groups, \(\text{in\_channels}\) should be divisible by the
                ///	number of groups.<br></br>
                ///	Default: 1
                /// </param>
                /// <param name="dilation">
                ///	the spacing between kernel elements.<br></br>
                ///	Can be a single number or
                ///	a tuple (dH, dW).<br></br>
                ///	Default: 1
                /// </param>
                public static Tensor conv_transpose2d(Tensor input, double weight, Tensor bias = null, int stride = 1, int padding = 0, int output_padding = 0, int groups = 1, int dilation = 1)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        weight,
                    });
                    var kwargs=new PyDict();
                    if (bias!=null) kwargs["bias"]=ToPython(bias);
                    if (stride!=1) kwargs["stride"]=ToPython(stride);
                    if (padding!=0) kwargs["padding"]=ToPython(padding);
                    if (output_padding!=0) kwargs["output_padding"]=ToPython(output_padding);
                    if (groups!=1) kwargs["groups"]=ToPython(groups);
                    if (dilation!=1) kwargs["dilation"]=ToPython(dilation);
                    dynamic py = __self__.InvokeMethod("conv_transpose2d", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies a 3D transposed convolution operator over an input image
                ///	composed of several input planes, sometimes also called “deconvolution”
                ///	
                ///	See ConvTranspose3d for details and output shape.<br></br>
                ///	
                ///	Note
                ///	In some circumstances when using the CUDA backend with CuDNN, this operator
                ///	may select a nondeterministic algorithm to increase performance.<br></br>
                ///	 If this is
                ///	undesirable, you can try to make the operation deterministic (potentially at
                ///	a performance cost) by setting torch.backends.cudnn.deterministic =
                ///	True.<br></br>
                ///	
                ///	Please see the notes on Reproducibility for background.
                /// </summary>
                /// <param name="input">
                ///	input tensor of shape \((\text{minibatch} , \text{in\_channels} , iT , iH , iW)\)
                /// </param>
                /// <param name="weight">
                ///	filters of shape \((\text{in\_channels} , \frac{\text{out\_channels}}{\text{groups}} , kT , kH , kW)\)
                /// </param>
                /// <param name="bias">
                ///	optional bias of shape \((\text{out\_channels})\).<br></br>
                ///	Default: None
                /// </param>
                /// <param name="stride">
                ///	the stride of the convolving kernel.<br></br>
                ///	Can be a single number or a
                ///	tuple (sT, sH, sW).<br></br>
                ///	Default: 1
                /// </param>
                /// <param name="padding">
                ///	dilation * (kernel_size - 1) - padding zero-padding will be added to both
                ///	sides of each dimension in the input.<br></br>
                ///	Can be a single number or a tuple
                ///	(padT, padH, padW).<br></br>
                ///	Default: 0
                /// </param>
                /// <param name="output_padding">
                ///	additional size added to one side of each dimension in the
                ///	output shape.<br></br>
                ///	Can be a single number or a tuple
                ///	(out_padT, out_padH, out_padW).<br></br>
                ///	Default: 0
                /// </param>
                /// <param name="groups">
                ///	split input into groups, \(\text{in\_channels}\) should be divisible by the
                ///	number of groups.<br></br>
                ///	Default: 1
                /// </param>
                /// <param name="dilation">
                ///	the spacing between kernel elements.<br></br>
                ///	Can be a single number or
                ///	a tuple (dT, dH, dW).<br></br>
                ///	Default: 1
                /// </param>
                public static Tensor conv_transpose3d(Tensor input, double weight, Tensor bias = null, int stride = 1, int padding = 0, int output_padding = 0, int groups = 1, int dilation = 1)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        weight,
                    });
                    var kwargs=new PyDict();
                    if (bias!=null) kwargs["bias"]=ToPython(bias);
                    if (stride!=1) kwargs["stride"]=ToPython(stride);
                    if (padding!=0) kwargs["padding"]=ToPython(padding);
                    if (output_padding!=0) kwargs["output_padding"]=ToPython(output_padding);
                    if (groups!=1) kwargs["groups"]=ToPython(groups);
                    if (dilation!=1) kwargs["dilation"]=ToPython(dilation);
                    dynamic py = __self__.InvokeMethod("conv_transpose3d", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Extracts sliding local blocks from an batched input tensor.<br></br>
                ///	
                ///	Warning
                ///	Currently, only 4-D input tensors (batched image-like tensors) are
                ///	supported.<br></br>
                ///	
                ///	Warning
                ///	More than one element of the unfolded tensor may refer to a single
                ///	memory location.<br></br>
                ///	 As a result, in-place operations (especially ones that
                ///	are vectorized) may result in incorrect behavior.<br></br>
                ///	 If you need to write
                ///	to the tensor, please clone it first.<br></br>
                ///	
                ///	See torch.nn.Unfold for details
                /// </summary>
                public static void unfold(Tensor input, Shape kernel_size, int dilation = 1, int padding = 0, int stride = 1)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        kernel_size,
                    });
                    var kwargs=new PyDict();
                    if (dilation!=1) kwargs["dilation"]=ToPython(dilation);
                    if (padding!=0) kwargs["padding"]=ToPython(padding);
                    if (stride!=1) kwargs["stride"]=ToPython(stride);
                    dynamic py = __self__.InvokeMethod("unfold", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Combines an array of sliding local blocks into a large containing
                ///	tensor.<br></br>
                ///	
                ///	Warning
                ///	Currently, only 4-D output tensors (batched image-like tensors) are
                ///	supported.<br></br>
                ///	
                ///	See torch.nn.Fold for details
                /// </summary>
                public static void fold(Tensor input, Shape output_size, Shape kernel_size, int dilation = 1, int padding = 0, int stride = 1)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        output_size,
                        kernel_size,
                    });
                    var kwargs=new PyDict();
                    if (dilation!=1) kwargs["dilation"]=ToPython(dilation);
                    if (padding!=0) kwargs["padding"]=ToPython(padding);
                    if (stride!=1) kwargs["stride"]=ToPython(stride);
                    dynamic py = __self__.InvokeMethod("fold", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies a 1D average pooling over an input signal composed of several
                ///	input planes.<br></br>
                ///	
                ///	See AvgPool1d for details and output shape.
                /// </summary>
                /// <param name="input">
                ///	input tensor of shape \((\text{minibatch} , \text{in\_channels} , iW)\)
                /// </param>
                /// <param name="kernel_size">
                ///	the size of the window.<br></br>
                ///	Can be a single number or a
                ///	tuple (kW,)
                /// </param>
                /// <param name="stride">
                ///	the stride of the window.<br></br>
                ///	Can be a single number or a tuple
                ///	(sW,).<br></br>
                ///	Default: kernel_size
                /// </param>
                /// <param name="padding">
                ///	implicit zero paddings on both sides of the input.<br></br>
                ///	Can be a
                ///	single number or a tuple (padW,).<br></br>
                ///	Default: 0
                /// </param>
                /// <param name="ceil_mode">
                ///	when True, will use ceil instead of floor to compute the
                ///	output shape.<br></br>
                ///	Default: False
                /// </param>
                /// <param name="count_include_pad">
                ///	when True, will include the zero-padding in the
                ///	averaging calculation.<br></br>
                ///	Default: True
                /// </param>
                public static Tensor avg_pool1d(Tensor input, Shape kernel_size, int? stride = null, int padding = 0, bool ceil_mode = false, bool count_include_pad = true)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        kernel_size,
                    });
                    var kwargs=new PyDict();
                    if (stride!=null) kwargs["stride"]=ToPython(stride);
                    if (padding!=0) kwargs["padding"]=ToPython(padding);
                    if (ceil_mode!=false) kwargs["ceil_mode"]=ToPython(ceil_mode);
                    if (count_include_pad!=true) kwargs["count_include_pad"]=ToPython(count_include_pad);
                    dynamic py = __self__.InvokeMethod("avg_pool1d", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies 2D average-pooling operation in \(kH \times kW\) regions by step size
                ///	\(sH \times sW\) steps.<br></br>
                ///	 The number of output features is equal to the number of
                ///	input planes.<br></br>
                ///	
                ///	See AvgPool2d for details and output shape.
                /// </summary>
                /// <param name="input">
                ///	input tensor \((\text{minibatch} , \text{in\_channels} , iH , iW)\)
                /// </param>
                /// <param name="kernel_size">
                ///	size of the pooling region.<br></br>
                ///	Can be a single number or a
                ///	tuple (kH, kW)
                /// </param>
                /// <param name="stride">
                ///	stride of the pooling operation.<br></br>
                ///	Can be a single number or a
                ///	tuple (sH, sW).<br></br>
                ///	Default: kernel_size
                /// </param>
                /// <param name="padding">
                ///	implicit zero paddings on both sides of the input.<br></br>
                ///	Can be a
                ///	single number or a tuple (padH, padW).<br></br>
                ///	Default: 0
                /// </param>
                /// <param name="ceil_mode">
                ///	when True, will use ceil instead of floor in the formula
                ///	to compute the output shape.<br></br>
                ///	Default: False
                /// </param>
                /// <param name="count_include_pad">
                ///	when True, will include the zero-padding in the
                ///	averaging calculation.<br></br>
                ///	Default: True
                /// </param>
                public static Tensor avg_pool2d(Tensor input, Shape kernel_size, Shape stride = null, int padding = 0, bool ceil_mode = false, bool count_include_pad = true)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        kernel_size,
                    });
                    var kwargs=new PyDict();
                    if (stride!=null) kwargs["stride"]=ToPython(stride);
                    if (padding!=0) kwargs["padding"]=ToPython(padding);
                    if (ceil_mode!=false) kwargs["ceil_mode"]=ToPython(ceil_mode);
                    if (count_include_pad!=true) kwargs["count_include_pad"]=ToPython(count_include_pad);
                    dynamic py = __self__.InvokeMethod("avg_pool2d", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies 3D average-pooling operation in \(kT \times kH \times kW\) regions by step
                ///	size \(sT \times sH \times sW\) steps.<br></br>
                ///	 The number of output features is equal to
                ///	\(\lfloor\frac{\text{input planes}}{sT}\rfloor\).<br></br>
                ///	
                ///	See AvgPool3d for details and output shape.
                /// </summary>
                /// <param name="input">
                ///	input tensor \((\text{minibatch} , \text{in\_channels} , iT \times iH , iW)\)
                /// </param>
                /// <param name="kernel_size">
                ///	size of the pooling region.<br></br>
                ///	Can be a single number or a
                ///	tuple (kT, kH, kW)
                /// </param>
                /// <param name="stride">
                ///	stride of the pooling operation.<br></br>
                ///	Can be a single number or a
                ///	tuple (sT, sH, sW).<br></br>
                ///	Default: kernel_size
                /// </param>
                /// <param name="padding">
                ///	implicit zero paddings on both sides of the input.<br></br>
                ///	Can be a
                ///	single number or a tuple (padT, padH, padW), Default: 0
                /// </param>
                /// <param name="ceil_mode">
                ///	when True, will use ceil instead of floor in the formula
                ///	to compute the output shape
                /// </param>
                /// <param name="count_include_pad">
                ///	when True, will include the zero-padding in the
                ///	averaging calculation
                /// </param>
                public static Tensor avg_pool3d(Tensor input, Shape kernel_size, Shape stride = null, int padding = 0, bool ceil_mode = false, bool count_include_pad = true)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        kernel_size,
                    });
                    var kwargs=new PyDict();
                    if (stride!=null) kwargs["stride"]=ToPython(stride);
                    if (padding!=0) kwargs["padding"]=ToPython(padding);
                    if (ceil_mode!=false) kwargs["ceil_mode"]=ToPython(ceil_mode);
                    if (count_include_pad!=true) kwargs["count_include_pad"]=ToPython(count_include_pad);
                    dynamic py = __self__.InvokeMethod("avg_pool3d", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies a 1D max pooling over an input signal composed of several input
                ///	planes.<br></br>
                ///	
                ///	See MaxPool1d for details.
                /// </summary>
                /// <param name="kernel_size">
                ///	the size of the window to take a max over
                /// </param>
                /// <param name="stride">
                ///	the stride of the window.<br></br>
                ///	Default value is kernel_size
                /// </param>
                /// <param name="padding">
                ///	implicit zero padding to be added on both sides
                /// </param>
                /// <param name="dilation">
                ///	a parameter that controls the stride of elements in the window
                /// </param>
                /// <param name="return_indices">
                ///	if True, will return the max indices along with the outputs.<br></br>
                ///	
                ///	Useful for torch.nn.MaxUnpool1d later
                /// </param>
                /// <param name="ceil_mode">
                ///	when True, will use ceil instead of floor to compute the output shape
                /// </param>
                public static void max_pool1d(Tensor input, Shape kernel_size, int stride = 1, int padding = 0, int dilation = 1, bool return_indices = false, bool ceil_mode = false)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        kernel_size,
                    });
                    var kwargs=new PyDict();
                    if (stride!=1) kwargs["stride"]=ToPython(stride);
                    if (padding!=0) kwargs["padding"]=ToPython(padding);
                    if (dilation!=1) kwargs["dilation"]=ToPython(dilation);
                    if (return_indices!=false) kwargs["return_indices"]=ToPython(return_indices);
                    if (ceil_mode!=false) kwargs["ceil_mode"]=ToPython(ceil_mode);
                    dynamic py = __self__.InvokeMethod("max_pool1d", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies a 2D max pooling over an input signal composed of several input
                ///	planes.<br></br>
                ///	
                ///	See MaxPool2d for details.
                /// </summary>
                /// <param name="kernel_size">
                ///	the size of the window to take a max over
                /// </param>
                /// <param name="stride">
                ///	the stride of the window.<br></br>
                ///	Default value is kernel_size
                /// </param>
                /// <param name="padding">
                ///	implicit zero padding to be added on both sides
                /// </param>
                /// <param name="dilation">
                ///	a parameter that controls the stride of elements in the window
                /// </param>
                /// <param name="return_indices">
                ///	if True, will return the max indices along with the outputs.<br></br>
                ///	
                ///	Useful for torch.nn.MaxUnpool2d later
                /// </param>
                /// <param name="ceil_mode">
                ///	when True, will use ceil instead of floor to compute the output shape
                /// </param>
                public static void max_pool2d(Tensor input, Shape kernel_size, int[] stride, int padding = 0, int dilation = 1, bool return_indices = false, bool ceil_mode = false)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        kernel_size,
                        stride,
                    });
                    var kwargs=new PyDict();
                    if (padding!=0) kwargs["padding"]=ToPython(padding);
                    if (dilation!=1) kwargs["dilation"]=ToPython(dilation);
                    if (return_indices!=false) kwargs["return_indices"]=ToPython(return_indices);
                    if (ceil_mode!=false) kwargs["ceil_mode"]=ToPython(ceil_mode);
                    dynamic py = __self__.InvokeMethod("max_pool2d", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies a 3D max pooling over an input signal composed of several input
                ///	planes.<br></br>
                ///	
                ///	See MaxPool3d for details.
                /// </summary>
                /// <param name="kernel_size">
                ///	the size of the window to take a max over
                /// </param>
                /// <param name="stride">
                ///	the stride of the window.<br></br>
                ///	Default value is kernel_size
                /// </param>
                /// <param name="padding">
                ///	implicit zero padding to be added on all three sides
                /// </param>
                /// <param name="dilation">
                ///	a parameter that controls the stride of elements in the window
                /// </param>
                /// <param name="return_indices">
                ///	if True, will return the max indices along with the outputs.<br></br>
                ///	
                ///	Useful for torch.nn.MaxUnpool3d later
                /// </param>
                /// <param name="ceil_mode">
                ///	when True, will use ceil instead of floor to compute the output shape
                /// </param>
                public static void max_pool3d(Tensor input, Shape kernel_size, int[] stride, int padding = 0, int dilation = 1, bool return_indices = false, bool ceil_mode = false)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        kernel_size,
                        stride,
                    });
                    var kwargs=new PyDict();
                    if (padding!=0) kwargs["padding"]=ToPython(padding);
                    if (dilation!=1) kwargs["dilation"]=ToPython(dilation);
                    if (return_indices!=false) kwargs["return_indices"]=ToPython(return_indices);
                    if (ceil_mode!=false) kwargs["ceil_mode"]=ToPython(ceil_mode);
                    dynamic py = __self__.InvokeMethod("max_pool3d", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Computes a partial inverse of MaxPool1d.<br></br>
                ///	
                ///	See MaxUnpool1d for details.
                /// </summary>
                /// <param name="kernel_size">
                ///	Size of the max pooling window.
                /// </param>
                /// <param name="stride">
                ///	Stride of the max pooling window.<br></br>
                ///	
                ///	It is set to kernel_size by default.
                /// </param>
                /// <param name="padding">
                ///	Padding that was added to the input
                /// </param>
                public static void max_unpool1d(Tensor input, Shape kernel_size, int stride = 1, int padding = 0)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        kernel_size,
                    });
                    var kwargs=new PyDict();
                    if (stride!=1) kwargs["stride"]=ToPython(stride);
                    if (padding!=0) kwargs["padding"]=ToPython(padding);
                    dynamic py = __self__.InvokeMethod("max_unpool1d", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Computes a partial inverse of MaxPool2d.<br></br>
                ///	
                ///	See MaxUnpool2d for details.
                /// </summary>
                /// <param name="kernel_size">
                ///	Size of the max pooling window.
                /// </param>
                /// <param name="stride">
                ///	Stride of the max pooling window.<br></br>
                ///	
                ///	It is set to kernel_size by default.
                /// </param>
                /// <param name="padding">
                ///	Padding that was added to the input
                /// </param>
                public static void max_unpool2d(Tensor input, Shape kernel_size, int[] stride, int padding = 0)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        kernel_size,
                        stride,
                    });
                    var kwargs=new PyDict();
                    if (padding!=0) kwargs["padding"]=ToPython(padding);
                    dynamic py = __self__.InvokeMethod("max_unpool2d", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Computes a partial inverse of MaxPool3d.<br></br>
                ///	
                ///	See MaxUnpool3d for details.
                /// </summary>
                /// <param name="kernel_size">
                ///	Size of the max pooling window.
                /// </param>
                /// <param name="stride">
                ///	Stride of the max pooling window.<br></br>
                ///	
                ///	It is set to kernel_size by default.
                /// </param>
                /// <param name="padding">
                ///	Padding that was added to the input
                /// </param>
                public static void max_unpool3d(Tensor input, Shape kernel_size, int[] stride, int padding = 0)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        kernel_size,
                        stride,
                    });
                    var kwargs=new PyDict();
                    if (padding!=0) kwargs["padding"]=ToPython(padding);
                    dynamic py = __self__.InvokeMethod("max_unpool3d", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies a 1D power-average pooling over an input signal composed of
                ///	several input planes.<br></br>
                ///	 If the sum of all inputs to the power of p is
                ///	zero, the gradient is set to zero as well.<br></br>
                ///	
                ///	See LPPool1d for details.
                /// </summary>
                /// <param name="kernel_size">
                ///	a single int, the size of the window
                /// </param>
                /// <param name="stride">
                ///	a single int, the stride of the window.<br></br>
                ///	Default value is kernel_size
                /// </param>
                /// <param name="ceil_mode">
                ///	when True, will use ceil instead of floor to compute the output shape
                /// </param>
                public static void lp_pool1d(Tensor input, Shape kernel_size, int stride = 1, bool ceil_mode = false)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        kernel_size,
                    });
                    var kwargs=new PyDict();
                    if (stride!=1) kwargs["stride"]=ToPython(stride);
                    if (ceil_mode!=false) kwargs["ceil_mode"]=ToPython(ceil_mode);
                    dynamic py = __self__.InvokeMethod("lp_pool1d", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies a 2D power-average pooling over an input signal composed of
                ///	several input planes.<br></br>
                ///	 If the sum of all inputs to the power of p is
                ///	zero, the gradient is set to zero as well.<br></br>
                ///	
                ///	See LPPool2d for details.
                /// </summary>
                /// <param name="kernel_size">
                ///	the size of the window
                /// </param>
                /// <param name="stride">
                ///	the stride of the window.<br></br>
                ///	Default value is kernel_size
                /// </param>
                /// <param name="ceil_mode">
                ///	when True, will use ceil instead of floor to compute the output shape
                /// </param>
                public static void lp_pool2d(Tensor input, Shape kernel_size, int[] stride, bool ceil_mode = false)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        kernel_size,
                        stride,
                    });
                    var kwargs=new PyDict();
                    if (ceil_mode!=false) kwargs["ceil_mode"]=ToPython(ceil_mode);
                    dynamic py = __self__.InvokeMethod("lp_pool2d", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies a 1D adaptive max pooling over an input signal composed of
                ///	several input planes.<br></br>
                ///	
                ///	See AdaptiveMaxPool1d for details and output shape.
                /// </summary>
                /// <param name="output_size">
                ///	the target output size (single integer)
                /// </param>
                /// <param name="return_indices">
                ///	whether to return pooling indices.<br></br>
                ///	Default: False
                /// </param>
                public static void adaptive_max_pool1d(Shape output_size, bool return_indices = false)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        output_size,
                    });
                    var kwargs=new PyDict();
                    if (return_indices!=false) kwargs["return_indices"]=ToPython(return_indices);
                    dynamic py = __self__.InvokeMethod("adaptive_max_pool1d", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies a 2D adaptive max pooling over an input signal composed of
                ///	several input planes.<br></br>
                ///	
                ///	See AdaptiveMaxPool2d for details and output shape.
                /// </summary>
                /// <param name="output_size">
                ///	the target output size (single integer or
                ///	double-integer tuple)
                /// </param>
                /// <param name="return_indices">
                ///	whether to return pooling indices.<br></br>
                ///	Default: False
                /// </param>
                public static void adaptive_max_pool2d(Shape output_size, bool return_indices = false)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        output_size,
                    });
                    var kwargs=new PyDict();
                    if (return_indices!=false) kwargs["return_indices"]=ToPython(return_indices);
                    dynamic py = __self__.InvokeMethod("adaptive_max_pool2d", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies a 3D adaptive max pooling over an input signal composed of
                ///	several input planes.<br></br>
                ///	
                ///	See AdaptiveMaxPool3d for details and output shape.
                /// </summary>
                /// <param name="output_size">
                ///	the target output size (single integer or
                ///	triple-integer tuple)
                /// </param>
                /// <param name="return_indices">
                ///	whether to return pooling indices.<br></br>
                ///	Default: False
                /// </param>
                public static void adaptive_max_pool3d(Shape output_size, bool return_indices = false)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        output_size,
                    });
                    var kwargs=new PyDict();
                    if (return_indices!=false) kwargs["return_indices"]=ToPython(return_indices);
                    dynamic py = __self__.InvokeMethod("adaptive_max_pool3d", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies a 1D adaptive average pooling over an input signal composed of
                ///	several input planes.<br></br>
                ///	
                ///	See AdaptiveAvgPool1d for details and output shape.
                /// </summary>
                public static Tensor adaptive_avg_pool1d(Shape output_size)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        output_size,
                    });
                    var kwargs=new PyDict();
                    dynamic py = __self__.InvokeMethod("adaptive_avg_pool1d", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies a 2D adaptive average pooling over an input signal composed of
                ///	several input planes.<br></br>
                ///	
                ///	See AdaptiveAvgPool2d for details and output shape.
                /// </summary>
                public static void adaptive_avg_pool2d(Shape output_size)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        output_size,
                    });
                    var kwargs=new PyDict();
                    dynamic py = __self__.InvokeMethod("adaptive_avg_pool2d", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies a 3D adaptive average pooling over an input signal composed of
                ///	several input planes.<br></br>
                ///	
                ///	See AdaptiveAvgPool3d for details and output shape.
                /// </summary>
                public static void adaptive_avg_pool3d(Shape output_size)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        output_size,
                    });
                    var kwargs=new PyDict();
                    dynamic py = __self__.InvokeMethod("adaptive_avg_pool3d", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Thresholds each element of the input Tensor.<br></br>
                ///	
                ///	See Threshold for more details.
                /// </summary>
                /// <param name="threshold">
                ///	The value to threshold at
                /// </param>
                /// <param name="value">
                ///	The value to replace with
                /// </param>
                /// <param name="inplace">
                ///	can optionally do the operation in-place.<br></br>
                ///	Default: False
                /// </param>
                public static void threshold(Tensor input, double threshold, double @value, bool inplace = false)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        threshold,
                        @value,
                    });
                    var kwargs=new PyDict();
                    if (inplace!=false) kwargs["inplace"]=ToPython(inplace);
                    dynamic py = __self__.InvokeMethod("threshold", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	In-place version of threshold().
                /// </summary>
                public static Tensor threshold_(Tensor input, double threshold, double @value)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        threshold,
                        @value,
                    });
                    var kwargs=new PyDict();
                    dynamic py = __self__.InvokeMethod("threshold_", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies the rectified linear unit function element-wise.<br></br>
                ///	 See
                ///	ReLU for more details.
                /// </summary>
                /// <param name="negative_slope">
                ///	Controls the angle of the negative slope.<br></br>
                ///	Default: 1e-2
                /// </param>
                /// <param name="inplace">
                ///	can optionally do the operation in-place.<br></br>
                ///	Default: False
                /// </param>
                public static Tensor relu(Tensor input, double negative_slope = 0.01, bool inplace = false)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (negative_slope!=0.01) kwargs["negative_slope"]=ToPython(negative_slope);
                    if (inplace!=false) kwargs["inplace"]=ToPython(inplace);
                    dynamic py = __self__.InvokeMethod("relu", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	In-place version of relu().
                /// </summary>
                public static Tensor relu_(Tensor input)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    dynamic py = __self__.InvokeMethod("relu_", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies the HardTanh function element-wise.<br></br>
                ///	 See Hardtanh for more
                ///	details.
                /// </summary>
                /// <param name="min_val">
                ///	minimum value of the linear region range.<br></br>
                ///	Default: -1
                /// </param>
                /// <param name="max_val">
                ///	maximum value of the linear region range.<br></br>
                ///	Default: 1
                /// </param>
                /// <param name="inplace">
                ///	can optionally do the operation in-place.<br></br>
                ///	Default: False
                /// </param>
                public static Tensor hardtanh(Tensor input, double min_val = -1.0, double max_val = 1.0, bool inplace = false)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (min_val!=-1.0) kwargs["min_val"]=ToPython(min_val);
                    if (max_val!=1.0) kwargs["max_val"]=ToPython(max_val);
                    if (inplace!=false) kwargs["inplace"]=ToPython(inplace);
                    dynamic py = __self__.InvokeMethod("hardtanh", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	In-place version of hardtanh().
                /// </summary>
                public static Tensor hardtanh_(Tensor input, double min_val = -1.0, double max_val = 1.0)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (min_val!=-1.0) kwargs["min_val"]=ToPython(min_val);
                    if (max_val!=1.0) kwargs["max_val"]=ToPython(max_val);
                    dynamic py = __self__.InvokeMethod("hardtanh_", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies the element-wise function \(\text{ReLU6}(x) = \min(\max(0,x), 6)\).<br></br>
                ///	
                ///	See ReLU6 for more details.
                /// </summary>
                public static Tensor relu6(Tensor input, bool? inplace = false)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (inplace!=false) kwargs["inplace"]=ToPython(inplace);
                    dynamic py = __self__.InvokeMethod("relu6", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies element-wise,
                ///	\(\text{ELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x) - 1))\).<br></br>
                ///	
                ///	See ELU for more details.
                /// </summary>
                /// <param name="alpha">
                ///	the \(\alpha\) value for the ELU formulation.<br></br>
                ///	Default: 1.0
                /// </param>
                /// <param name="inplace">
                ///	can optionally do the operation in-place.<br></br>
                ///	Default: False
                /// </param>
                public static void elu(Tensor input, double alpha = 1.0, bool inplace = false)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (alpha!=1.0) kwargs["alpha"]=ToPython(alpha);
                    if (inplace!=false) kwargs["inplace"]=ToPython(inplace);
                    dynamic py = __self__.InvokeMethod("elu", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	In-place version of elu().
                /// </summary>
                public static Tensor elu_(Tensor input, double alpha = 1.0)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (alpha!=1.0) kwargs["alpha"]=ToPython(alpha);
                    dynamic py = __self__.InvokeMethod("elu_", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies element-wise,
                ///	\(\text{SELU}(x) = scale * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))\),
                ///	with \(\alpha=1.6732632423543772848170429916717\) and
                ///	\(scale=1.0507009873554804934193349852946\).<br></br>
                ///	
                ///	See SELU for more details.
                /// </summary>
                public static Tensor selu(Tensor input, bool? inplace = false)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (inplace!=false) kwargs["inplace"]=ToPython(inplace);
                    dynamic py = __self__.InvokeMethod("selu", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies element-wise,
                ///	\(\text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))\).<br></br>
                ///	
                ///	See CELU for more details.
                /// </summary>
                /// <param name="alpha">
                ///	the \(\alpha\) value for the CELU formulation.<br></br>
                ///	Default: 1.0
                /// </param>
                /// <param name="inplace">
                ///	can optionally do the operation in-place.<br></br>
                ///	Default: False
                /// </param>
                public static Tensor celu(Tensor input, double alpha = 1.0, bool inplace = false)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (alpha!=1.0) kwargs["alpha"]=ToPython(alpha);
                    if (inplace!=false) kwargs["inplace"]=ToPython(inplace);
                    dynamic py = __self__.InvokeMethod("celu", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies element-wise,
                ///	\(\text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope} * \min(0, x)\)
                ///	
                ///	See LeakyReLU for more details.
                /// </summary>
                /// <param name="negative_slope">
                ///	Controls the angle of the negative slope.<br></br>
                ///	Default: 1e-2
                /// </param>
                /// <param name="inplace">
                ///	can optionally do the operation in-place.<br></br>
                ///	Default: False
                /// </param>
                public static Tensor leaky_relu(Tensor input, double negative_slope = 0.01, bool inplace = false)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (negative_slope!=0.01) kwargs["negative_slope"]=ToPython(negative_slope);
                    if (inplace!=false) kwargs["inplace"]=ToPython(inplace);
                    dynamic py = __self__.InvokeMethod("leaky_relu", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	In-place version of leaky_relu().
                /// </summary>
                public static Tensor leaky_relu_(Tensor input, double negative_slope = 0.01)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (negative_slope!=0.01) kwargs["negative_slope"]=ToPython(negative_slope);
                    dynamic py = __self__.InvokeMethod("leaky_relu_", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies element-wise the function
                ///	\(\text{PReLU}(x) = \max(0,x) + \text{weight} * \min(0,x)\) where weight is a
                ///	learnable parameter.<br></br>
                ///	
                ///	See PReLU for more details.
                /// </summary>
                /// <param name="num_parameters">
                ///	number of \(a\) to learn.<br></br>
                ///	
                ///	Although it takes an int as input, there is only two values are legitimate:
                ///	1, or the number of channels at input.<br></br>
                ///	Default: 1
                /// </param>
                /// <param name="init">
                ///	the initial value of \(a\).<br></br>
                ///	Default: 0.25
                /// </param>
                public static Tensor prelu(Tensor input, int num_parameters = 1, float init = 0.25f)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (num_parameters!=1) kwargs["num_parameters"]=ToPython(num_parameters);
                    if (init!=0.25f) kwargs["init"]=ToPython(init);
                    dynamic py = __self__.InvokeMethod("prelu", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Randomized leaky ReLU.<br></br>
                ///	
                ///	See RReLU for more details.
                /// </summary>
                /// <param name="lower">
                ///	lower bound of the uniform distribution.<br></br>
                ///	Default: \(\frac{1}{8}\)
                /// </param>
                /// <param name="upper">
                ///	upper bound of the uniform distribution.<br></br>
                ///	Default: \(\frac{1}{3}\)
                /// </param>
                /// <param name="inplace">
                ///	can optionally do the operation in-place.<br></br>
                ///	Default: False
                /// </param>
                public static Tensor rrelu(Tensor input, double? lower = 1.0/80, double? upper = 1.0/30, bool inplace = false)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (lower!=1.0/80) kwargs["lower"]=ToPython(lower);
                    if (upper!=1.0/30) kwargs["upper"]=ToPython(upper);
                    if (inplace!=false) kwargs["inplace"]=ToPython(inplace);
                    dynamic py = __self__.InvokeMethod("rrelu", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	In-place version of rrelu().
                /// </summary>
                public static Tensor rrelu_(Tensor input, double lower = 1.0/80, double upper = 1.0/30, bool training = false)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (lower!=1.0/80) kwargs["lower"]=ToPython(lower);
                    if (upper!=1.0/30) kwargs["upper"]=ToPython(upper);
                    if (training!=false) kwargs["training"]=ToPython(training);
                    dynamic py = __self__.InvokeMethod("rrelu_", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	The gated linear unit.<br></br>
                ///	 Computes:
                ///	
                ///	\[\text{GLU}(a, b) = a \otimes \sigma(b)
                ///	
                ///	\]
                ///	
                ///	where input is split in half along dim to form a and b, \(\sigma\)
                ///	is the sigmoid function and \(\otimes\) is the element-wise product between matrices.<br></br>
                ///	
                ///	See Language Modeling with Gated Convolutional Networks.
                /// </summary>
                /// <param name="input">
                ///	input tensor
                /// </param>
                /// <param name="dim">
                ///	dimension on which to split the input.<br></br>
                ///	Default: -1
                /// </param>
                public static Tensor glu(Tensor input, int dim = -1)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (dim!=-1) kwargs["dim"]=ToPython(dim);
                    dynamic py = __self__.InvokeMethod("glu", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies element-wise \(\text{LogSigmoid}(x_i) = \log \left(\frac{1}{1 + \exp(-x_i)}\right)\)
                ///	
                ///	See LogSigmoid for more details.
                /// </summary>
                public static Tensor logsigmoid(Tensor input)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    dynamic py = __self__.InvokeMethod("logsigmoid", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies the hard shrinkage function element-wise
                ///	
                ///	See Hardshrink for more details.
                /// </summary>
                public static Tensor hardshrink(Tensor input, double lambd = 0.5)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (lambd!=0.5) kwargs["lambd"]=ToPython(lambd);
                    dynamic py = __self__.InvokeMethod("hardshrink", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies element-wise, \(\text{Tanhshrink}(x) = x - \text{Tanh}(x)\)
                ///	
                ///	See Tanhshrink for more details.
                /// </summary>
                public static Tensor tanhshrink(Tensor input)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    dynamic py = __self__.InvokeMethod("tanhshrink", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies element-wise, the function \(\text{SoftSign}(x) = \frac{x}{1 + |x|}\)
                ///	
                ///	See Softsign for more details.
                /// </summary>
                public static Tensor softsign(Tensor input)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    dynamic py = __self__.InvokeMethod("softsign", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                public static Tensor softplus(Tensor input, int beta = 1, int threshold = 20)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (beta!=1) kwargs["beta"]=ToPython(beta);
                    if (threshold!=20) kwargs["threshold"]=ToPython(threshold);
                    dynamic py = __self__.InvokeMethod("softplus", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies a softmin function.<br></br>
                ///	
                ///	Note that \(\text{Softmin}(x) = \text{Softmax}(-x)\).<br></br>
                ///	 See softmax definition for mathematical formula.<br></br>
                ///	
                ///	See Softmin for more details.
                /// </summary>
                /// <param name="input">
                ///	input
                /// </param>
                /// <param name="dim">
                ///	A dimension along which softmin will be computed (so every slice
                ///	along dim will sum to 1).
                /// </param>
                /// <param name="dtype">
                ///	the desired data type of returned tensor.<br></br>
                ///	
                ///	If specified, the input tensor is casted to dtype before the operation
                ///	is performed.<br></br>
                ///	This is useful for preventing data type overflows.<br></br>
                ///	Default: None.
                /// </param>
                public static void softmin(Tensor input, int? dim = null, Dtype dtype = null)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (dim!=null) kwargs["dim"]=ToPython(dim);
                    if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
                    dynamic py = __self__.InvokeMethod("softmin", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies a softmax function.<br></br>
                ///	
                ///	Softmax is defined as:
                ///	
                ///	\(\text{Softmax}(x_{i}) = \frac{exp(x_i)}{\sum_j exp(x_j)}\)
                ///	
                ///	It is applied to all slices along dim, and will re-scale them so that the elements
                ///	lie in the range [0, 1] and sum to 1.<br></br>
                ///	
                ///	See Softmax for more details.
                /// </summary>
                /// <param name="input">
                ///	input
                /// </param>
                /// <param name="dim">
                ///	A dimension along which softmax will be computed.
                /// </param>
                /// <param name="dtype">
                ///	the desired data type of returned tensor.<br></br>
                ///	
                ///	If specified, the input tensor is casted to dtype before the operation
                ///	is performed.<br></br>
                ///	This is useful for preventing data type overflows.<br></br>
                ///	Default: None.
                /// </param>
                public static void softmax(Tensor input, int? dim = null, Dtype dtype = null)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (dim!=null) kwargs["dim"]=ToPython(dim);
                    if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
                    dynamic py = __self__.InvokeMethod("softmax", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies the soft shrinkage function elementwise
                ///	
                ///	See Softshrink for more details.
                /// </summary>
                public static Tensor softshrink(Tensor input, double lambd = 0.5)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (lambd!=0.5) kwargs["lambd"]=ToPython(lambd);
                    dynamic py = __self__.InvokeMethod("softshrink", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Samples from the Gumbel-Softmax distribution and optionally discretizes.
                /// </summary>
                /// <param name="logits">
                ///	[…, num_features] unnormalized log probabilities
                /// </param>
                /// <param name="tau">
                ///	non-negative scalar temperature
                /// </param>
                /// <param name="hard">
                ///	if True, the returned samples will be discretized as one-hot vectors,
                ///	but will be differentiated as if it is the soft sample in autograd
                /// </param>
                /// <param name="dim">
                ///	A dimension along which softmax will be computed.<br></br>
                ///	Default: -1.
                /// </param>
                public static void gumbel_softmax(Tensor logits, int tau = 1, bool hard = false, int dim = -1)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        logits,
                    });
                    var kwargs=new PyDict();
                    if (tau!=1) kwargs["tau"]=ToPython(tau);
                    if (hard!=false) kwargs["hard"]=ToPython(hard);
                    if (dim!=-1) kwargs["dim"]=ToPython(dim);
                    dynamic py = __self__.InvokeMethod("gumbel_softmax", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies a softmax followed by a logarithm.<br></br>
                ///	
                ///	While mathematically equivalent to log(softmax(x)), doing these two
                ///	operations separately is slower, and numerically unstable.<br></br>
                ///	 This function
                ///	uses an alternative formulation to compute the output and gradient correctly.<br></br>
                ///	
                ///	See LogSoftmax for more details.
                /// </summary>
                /// <param name="input">
                ///	input
                /// </param>
                /// <param name="dim">
                ///	A dimension along which log_softmax will be computed.
                /// </param>
                /// <param name="dtype">
                ///	the desired data type of returned tensor.<br></br>
                ///	
                ///	If specified, the input tensor is casted to dtype before the operation
                ///	is performed.<br></br>
                ///	This is useful for preventing data type overflows.<br></br>
                ///	Default: None.
                /// </param>
                public static void log_softmax(Tensor input, int? dim = null, Dtype dtype = null)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (dim!=null) kwargs["dim"]=ToPython(dim);
                    if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
                    dynamic py = __self__.InvokeMethod("log_softmax", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies element-wise,
                ///	\(\text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}\)
                ///	
                ///	See Tanh for more details.
                /// </summary>
                public static Tensor tanh(Tensor input)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    dynamic py = __self__.InvokeMethod("tanh", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies the element-wise function \(\text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}\)
                ///	
                ///	See Sigmoid for more details.
                /// </summary>
                public static Tensor sigmoid(Tensor input)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    dynamic py = __self__.InvokeMethod("sigmoid", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies Batch Normalization for each channel across a batch of data.<br></br>
                ///	
                ///	See BatchNorm1d, BatchNorm2d,
                ///	BatchNorm3d for details.
                /// </summary>
                /// <param name="num_features">
                ///	\(C\) from an expected input of size
                ///	\((N, C, L)\) or \(L\) from input of size \((N, L)\)
                /// </param>
                /// <param name="eps">
                ///	a value added to the denominator for numerical stability.<br></br>
                ///	
                ///	Default: 1e-5
                /// </param>
                /// <param name="momentum">
                ///	the value used for the running_mean and running_var
                ///	computation.<br></br>
                ///	Can be set to None for cumulative moving average
                ///	(i.e.<br></br>
                ///	simple average).<br></br>
                ///	Default: 0.1
                /// </param>
                /// <param name="affine">
                ///	a boolean value that when set to True, this module has
                ///	learnable affine parameters.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="track_running_stats">
                ///	a boolean value that when set to True, this
                ///	module tracks the running mean and variance, and when set to False,
                ///	this module does not track such statistics and always uses batch
                ///	statistics in both training and eval modes.<br></br>
                ///	Default: True
                /// </param>
                public static void batch_norm(Tensor input, int num_features, double eps = 1.0e-5, double? momentum = 0.1, bool affine = true, bool track_running_stats = true)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        num_features,
                    });
                    var kwargs=new PyDict();
                    if (eps!=1.0e-5) kwargs["eps"]=ToPython(eps);
                    if (momentum!=0.1) kwargs["momentum"]=ToPython(momentum);
                    if (affine!=true) kwargs["affine"]=ToPython(affine);
                    if (track_running_stats!=true) kwargs["track_running_stats"]=ToPython(track_running_stats);
                    dynamic py = __self__.InvokeMethod("batch_norm", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies Instance Normalization for each channel in each data sample in a
                ///	batch.<br></br>
                ///	
                ///	See InstanceNorm1d, InstanceNorm2d,
                ///	InstanceNorm3d for details.
                /// </summary>
                /// <param name="num_features">
                ///	\(C\) from an expected input of size
                ///	\((N, C, L)\) or \(L\) from input of size \((N, L)\)
                /// </param>
                /// <param name="eps">
                ///	a value added to the denominator for numerical stability.<br></br>
                ///	Default: 1e-5
                /// </param>
                /// <param name="momentum">
                ///	the value used for the running_mean and running_var computation.<br></br>
                ///	Default: 0.1
                /// </param>
                /// <param name="affine">
                ///	a boolean value that when set to True, this module has
                ///	learnable affine parameters, initialized the same way as done for batch normalization.<br></br>
                ///	
                ///	Default: False.
                /// </param>
                /// <param name="track_running_stats">
                ///	a boolean value that when set to True, this
                ///	module tracks the running mean and variance, and when set to False,
                ///	this module does not track such statistics and always uses batch
                ///	statistics in both training and eval modes.<br></br>
                ///	Default: False
                /// </param>
                public static void instance_norm(Tensor input, int num_features, double eps = 1.0e-5, double momentum = 0.1, bool affine = true, bool track_running_stats = false)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        num_features,
                    });
                    var kwargs=new PyDict();
                    if (eps!=1.0e-5) kwargs["eps"]=ToPython(eps);
                    if (momentum!=0.1) kwargs["momentum"]=ToPython(momentum);
                    if (affine!=true) kwargs["affine"]=ToPython(affine);
                    if (track_running_stats!=false) kwargs["track_running_stats"]=ToPython(track_running_stats);
                    dynamic py = __self__.InvokeMethod("instance_norm", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies Layer Normalization for last certain number of dimensions.<br></br>
                ///	
                ///	See LayerNorm for details.
                /// </summary>
                /// <param name="normalized_shape">
                ///	input shape from an expected input
                ///	of size
                ///	
                ///	\[[* \times \text{normalized\_shape}[0] \times \text{normalized\_shape}[1]
                ///	\times \ldots \times \text{normalized\_shape}[-1]]
                ///	
                ///	\]
                ///	If a single integer is used, it is treated as a singleton list, and this module will
                ///	normalize over the last dimension which is expected to be of that specific size.
                /// </param>
                /// <param name="eps">
                ///	a value added to the denominator for numerical stability.<br></br>
                ///	Default: 1e-5
                /// </param>
                /// <param name="elementwise_affine">
                ///	a boolean value that when set to True, this module
                ///	has learnable per-element affine parameters initialized to ones (for weights)
                ///	and zeros (for biases).<br></br>
                ///	Default: True.
                /// </param>
                public static void layer_norm(Tensor input, Shape normalized_shape, double eps = 1.0e-5, bool elementwise_affine = true)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        normalized_shape,
                    });
                    var kwargs=new PyDict();
                    if (eps!=1.0e-5) kwargs["eps"]=ToPython(eps);
                    if (elementwise_affine!=true) kwargs["elementwise_affine"]=ToPython(elementwise_affine);
                    dynamic py = __self__.InvokeMethod("layer_norm", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies local response normalization over an input signal composed of
                ///	several input planes, where channels occupy the second dimension.<br></br>
                ///	
                ///	Applies normalization across channels.<br></br>
                ///	
                ///	See LocalResponseNorm for details.
                /// </summary>
                /// <param name="size">
                ///	amount of neighbouring channels used for normalization
                /// </param>
                /// <param name="alpha">
                ///	multiplicative factor.<br></br>
                ///	Default: 0.0001
                /// </param>
                /// <param name="beta">
                ///	exponent.<br></br>
                ///	Default: 0.75
                /// </param>
                /// <param name="k">
                ///	additive factor.<br></br>
                ///	Default: 1
                /// </param>
                public static void local_response_norm(Tensor input, int size, double alpha = 0.0001, double beta = 0.75, double k = 1.0)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        size,
                    });
                    var kwargs=new PyDict();
                    if (alpha!=0.0001) kwargs["alpha"]=ToPython(alpha);
                    if (beta!=0.75) kwargs["beta"]=ToPython(beta);
                    if (k!=1.0) kwargs["k"]=ToPython(k);
                    dynamic py = __self__.InvokeMethod("local_response_norm", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Performs \(L_p\) normalization of inputs over specified dimension.<br></br>
                ///	
                ///	For a tensor input of sizes \((n_0, ..., n_{dim}, ..., n_k)\), each
                ///	\(n_{dim}\) -element vector \(v\) along dimension dim is transformed as
                ///	
                ///	\[v = \frac{v}{\max(\lVert v \rVert_p, \epsilon)}.
                ///	
                ///	\]
                ///	
                ///	With the default arguments it uses the Euclidean norm over vectors along dimension \(1\) for normalization.
                /// </summary>
                /// <param name="input">
                ///	input tensor of any shape
                /// </param>
                /// <param name="p">
                ///	the exponent value in the norm formulation.<br></br>
                ///	Default: 2
                /// </param>
                /// <param name="dim">
                ///	the dimension to reduce.<br></br>
                ///	Default: 1
                /// </param>
                /// <param name="eps">
                ///	small value to avoid division by zero.<br></br>
                ///	Default: 1e-12
                /// </param>
                /// <param name="out">
                ///	the output tensor.<br></br>
                ///	If out is used, this
                ///	operation won’t be differentiable.
                /// </param>
                public static void normalize(Tensor input, float p = 2, int dim = 1, float eps = 1.0e-12f, Tensor @out = null)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (p!=2) kwargs["p"]=ToPython(p);
                    if (dim!=1) kwargs["dim"]=ToPython(dim);
                    if (eps!=1.0e-12f) kwargs["eps"]=ToPython(eps);
                    if (@out!=null) kwargs["out"]=ToPython(@out);
                    dynamic py = __self__.InvokeMethod("normalize", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies a linear transformation to the incoming data: \(y = xA^T + b\).<br></br>
                ///	
                ///	Shape:
                ///	
                ///	Input: \((N, *, in\_features)\) where * means any number of
                ///	additional dimensions
                ///	Weight: \((out\_features, in\_features)\)
                ///	Bias: \((out\_features)\)
                ///	Output: \((N, *, out\_features)\)
                /// </summary>
                public static void linear(Tensor input, double weight, Tensor bias = null)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        weight,
                    });
                    var kwargs=new PyDict();
                    if (bias!=null) kwargs["bias"]=ToPython(bias);
                    dynamic py = __self__.InvokeMethod("linear", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                public static void bilinear(Tensor input1, Tensor input2, double weight, Tensor bias = null)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input1,
                        input2,
                        weight,
                    });
                    var kwargs=new PyDict();
                    if (bias!=null) kwargs["bias"]=ToPython(bias);
                    dynamic py = __self__.InvokeMethod("bilinear", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	During training, randomly zeroes some of the elements of the input
                ///	tensor with probability p using samples from a Bernoulli
                ///	distribution.<br></br>
                ///	
                ///	See Dropout for details.
                /// </summary>
                /// <param name="p">
                ///	probability of an element to be zeroed.<br></br>
                ///	Default: 0.5
                /// </param>
                /// <param name="training">
                ///	apply dropout if is True.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="inplace">
                ///	If set to True, will do this operation in-place.<br></br>
                ///	Default: False
                /// </param>
                public static void dropout(double p = 0.5, bool training = true, bool inplace = false)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                    });
                    var kwargs=new PyDict();
                    if (p!=0.5) kwargs["p"]=ToPython(p);
                    if (training!=true) kwargs["training"]=ToPython(training);
                    if (inplace!=false) kwargs["inplace"]=ToPython(inplace);
                    dynamic py = __self__.InvokeMethod("dropout", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Applies alpha dropout to the input.<br></br>
                ///	
                ///	See AlphaDropout for details.
                /// </summary>
                /// <param name="p">
                ///	probability of an element to be dropped.<br></br>
                ///	Default: 0.5
                /// </param>
                /// <param name="inplace">
                ///	If set to True, will do this operation
                ///	in-place
                /// </param>
                public static void alpha_dropout(Tensor input, float p = 0.5f, bool? inplace = false)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (p!=0.5f) kwargs["p"]=ToPython(p);
                    if (inplace!=false) kwargs["inplace"]=ToPython(inplace);
                    dynamic py = __self__.InvokeMethod("alpha_dropout", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Randomly zero out entire channels (a channel is a 2D feature map,
                ///	e.g., the \(j\)-th channel of the \(i\)-th sample in the
                ///	batched input is a 2D tensor \(\text{input}[i, j]\)) of the input tensor).<br></br>
                ///	
                ///	Each channel will be zeroed out independently on every forward call with
                ///	probability p using samples from a Bernoulli distribution.<br></br>
                ///	
                ///	See Dropout2d for details.
                /// </summary>
                /// <param name="p">
                ///	probability of a channel to be zeroed.<br></br>
                ///	Default: 0.5
                /// </param>
                /// <param name="training">
                ///	apply dropout if is True.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="inplace">
                ///	If set to True, will do this operation in-place.<br></br>
                ///	Default: False
                /// </param>
                public static void dropout2d(double p = 0.5, bool training = true, bool inplace = false)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                    });
                    var kwargs=new PyDict();
                    if (p!=0.5) kwargs["p"]=ToPython(p);
                    if (training!=true) kwargs["training"]=ToPython(training);
                    if (inplace!=false) kwargs["inplace"]=ToPython(inplace);
                    dynamic py = __self__.InvokeMethod("dropout2d", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Randomly zero out entire channels (a channel is a 3D feature map,
                ///	e.g., the \(j\)-th channel of the \(i\)-th sample in the
                ///	batched input is a 3D tensor \(\text{input}[i, j]\)) of the input tensor).<br></br>
                ///	
                ///	Each channel will be zeroed out independently on every forward call with
                ///	probability p using samples from a Bernoulli distribution.<br></br>
                ///	
                ///	See Dropout3d for details.
                /// </summary>
                /// <param name="p">
                ///	probability of a channel to be zeroed.<br></br>
                ///	Default: 0.5
                /// </param>
                /// <param name="training">
                ///	apply dropout if is True.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="inplace">
                ///	If set to True, will do this operation in-place.<br></br>
                ///	Default: False
                /// </param>
                public static void dropout3d(double p = 0.5, bool training = true, bool inplace = false)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                    });
                    var kwargs=new PyDict();
                    if (p!=0.5) kwargs["p"]=ToPython(p);
                    if (training!=true) kwargs["training"]=ToPython(training);
                    if (inplace!=false) kwargs["inplace"]=ToPython(inplace);
                    dynamic py = __self__.InvokeMethod("dropout3d", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	A simple lookup table that looks up embeddings in a fixed dictionary and size.<br></br>
                ///	
                ///	This module is often used to retrieve word embeddings using indices.<br></br>
                ///	
                ///	The input to the module is a list of indices, and the embedding matrix,
                ///	and the output is the corresponding word embeddings.<br></br>
                ///	
                ///	See torch.nn.Embedding for more details.
                /// </summary>
                /// <param name="input">
                ///	Tensor containing indices into the embedding matrix
                /// </param>
                /// <param name="weight">
                ///	The embedding matrix with number of rows equal to the maximum possible index + 1,
                ///	and number of columns equal to the embedding size
                /// </param>
                /// <param name="padding_idx">
                ///	If given, pads the output with the embedding vector at padding_idx
                ///	(initialized to zeros) whenever it encounters the index.
                /// </param>
                /// <param name="max_norm">
                ///	If given, each embedding vector with norm larger than max_norm
                ///	is renormalized to have norm max_norm.<br></br>
                ///	
                ///	Note: this will modify weight in-place.
                /// </param>
                /// <param name="norm_type">
                ///	The p of the p-norm to compute for the max_norm option.<br></br>
                ///	Default 2.
                /// </param>
                /// <param name="scale_grad_by_freq">
                ///	If given, this will scale gradients by the inverse of frequency of
                ///	the words in the mini-batch.<br></br>
                ///	Default False.
                /// </param>
                /// <param name="sparse">
                ///	If True, gradient w.r.t.<br></br>
                ///	weight will be a sparse tensor.<br></br>
                ///	See Notes under
                ///	torch.nn.Embedding for more details regarding sparse gradients.
                /// </param>
                public static void embedding(Tensor<long> input, double weight, int? padding_idx = null, float? max_norm = null, float? norm_type = 2.0f, bool? scale_grad_by_freq = false, bool? sparse = false)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        weight,
                    });
                    var kwargs=new PyDict();
                    if (padding_idx!=null) kwargs["padding_idx"]=ToPython(padding_idx);
                    if (max_norm!=null) kwargs["max_norm"]=ToPython(max_norm);
                    if (norm_type!=2.0f) kwargs["norm_type"]=ToPython(norm_type);
                    if (scale_grad_by_freq!=false) kwargs["scale_grad_by_freq"]=ToPython(scale_grad_by_freq);
                    if (sparse!=false) kwargs["sparse"]=ToPython(sparse);
                    dynamic py = __self__.InvokeMethod("embedding", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Computes sums, means or maxes of bags of embeddings, without instantiating the
                ///	intermediate embeddings.<br></br>
                ///	
                ///	See torch.nn.EmbeddingBag for more details.<br></br>
                ///	
                ///	Note
                ///	When using the CUDA backend, this operation may induce nondeterministic
                ///	behaviour in be backward that is not easily switched off.<br></br>
                ///	
                ///	Please see the notes on Reproducibility for background.
                /// </summary>
                /// <param name="input">
                ///	Tensor containing bags of indices into the embedding matrix
                /// </param>
                /// <param name="weight">
                ///	The embedding matrix with number of rows equal to the maximum possible index + 1,
                ///	and number of columns equal to the embedding size
                /// </param>
                /// <param name="offsets">
                ///	Only used when input is 1D.<br></br>
                ///	offsets determines
                ///	the starting index position of each bag (sequence) in input.
                /// </param>
                /// <param name="max_norm">
                ///	If given, each embedding vector with norm larger than max_norm
                ///	is renormalized to have norm max_norm.<br></br>
                ///	
                ///	Note: this will modify weight in-place.
                /// </param>
                /// <param name="norm_type">
                ///	The p in the p-norm to compute for the max_norm option.<br></br>
                ///	
                ///	Default 2.
                /// </param>
                /// <param name="scale_grad_by_freq">
                ///	if given, this will scale gradients by the inverse of frequency of
                ///	the words in the mini-batch.<br></br>
                ///	Default False.<br></br>
                ///	
                ///	Note: this option is not supported when mode=&quot;max&quot;.
                /// </param>
                /// <param name="mode">
                ///	&quot;sum&quot;, &quot;mean&quot; or &quot;max&quot;. Specifies the way to reduce the bag.<br></br>
                ///	
                ///	Default: &quot;mean&quot;
                /// </param>
                /// <param name="sparse">
                ///	if True, gradient w.r.t.<br></br>
                ///	weight will be a sparse tensor.<br></br>
                ///	See Notes under
                ///	torch.nn.Embedding for more details regarding sparse gradients.<br></br>
                ///	
                ///	Note: this option is not supported when mode=&quot;max&quot;.
                /// </param>
                /// <param name="per_sample_weights">
                ///	a tensor of float / double weights, or None
                ///	to indicate all weights should be taken to be 1.<br></br>
                ///	If specified, per_sample_weights
                ///	must have exactly the same shape as input and is treated as having the same
                ///	offsets, if those are not None.
                /// </param>
                public static void embedding_bag(Tensor<long> input, double weight, Tensor<long> offsets = null, float? max_norm = null, float? norm_type = 2, bool? scale_grad_by_freq = false, string mode = "mean", bool? sparse = false, Tensor per_sample_weights = null)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        weight,
                    });
                    var kwargs=new PyDict();
                    if (offsets!=null) kwargs["offsets"]=ToPython(offsets);
                    if (max_norm!=null) kwargs["max_norm"]=ToPython(max_norm);
                    if (norm_type!=2) kwargs["norm_type"]=ToPython(norm_type);
                    if (scale_grad_by_freq!=false) kwargs["scale_grad_by_freq"]=ToPython(scale_grad_by_freq);
                    if (mode!="mean") kwargs["mode"]=ToPython(mode);
                    if (sparse!=false) kwargs["sparse"]=ToPython(sparse);
                    if (per_sample_weights!=null) kwargs["per_sample_weights"]=ToPython(per_sample_weights);
                    dynamic py = __self__.InvokeMethod("embedding_bag", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Takes LongTensor with index values of shape (*) and returns a tensor
                ///	of shape (*, num_classes) that have zeros everywhere except where the
                ///	index of last dimension matches the corresponding value of the input tensor,
                ///	in which case it will be 1.<br></br>
                ///	
                ///	See also One-hot on Wikipedia .
                /// </summary>
                /// <param name="tensor">
                ///	class values of any shape.
                /// </param>
                /// <param name="num_classes">
                ///	Total number of classes.<br></br>
                ///	If set to -1, the number
                ///	of classes will be inferred as one greater than the largest class
                ///	value in the input tensor.
                /// </param>
                public static Tensor<long> one_hot(Tensor<long> tensor, int num_classes = 0)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        tensor,
                    });
                    var kwargs=new PyDict();
                    if (num_classes!=0) kwargs["num_classes"]=ToPython(num_classes);
                    dynamic py = __self__.InvokeMethod("one_hot", pyargs, kwargs);
                    return ToCsharp<Tensor<long>>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	See torch.nn.PairwiseDistance for details
                /// </summary>
                public static void pairwise_distance(Tensor x1, Tensor x2, double p = 2.0, double eps = 1.0e-06, bool keepdim = false)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        x1,
                        x2,
                    });
                    var kwargs=new PyDict();
                    if (p!=2.0) kwargs["p"]=ToPython(p);
                    if (eps!=1.0e-06) kwargs["eps"]=ToPython(eps);
                    if (keepdim!=false) kwargs["keepdim"]=ToPython(keepdim);
                    dynamic py = __self__.InvokeMethod("pairwise_distance", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Returns cosine similarity between x1 and x2, computed along dim.<br></br>
                ///	
                ///	\[\text{similarity} = \dfrac{x_1 \cdot x_2}{\max(\Vert x_1 \Vert _2 \cdot \Vert x_2 \Vert _2, \epsilon)}
                ///	
                ///	\]
                /// </summary>
                /// <param name="x1">
                ///	First input.
                /// </param>
                /// <param name="x2">
                ///	Second input (of size matching x1).
                /// </param>
                /// <param name="dim">
                ///	Dimension of vectors.<br></br>
                ///	Default: 1
                /// </param>
                /// <param name="eps">
                ///	Small value to avoid division by zero.<br></br>
                ///	
                ///	Default: 1e-8
                /// </param>
                public static Tensor cosine_similarity(Tensor x1, Tensor x2, int? dim = 1, float? eps = 1.0e-8f)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        x1,
                        x2,
                    });
                    var kwargs=new PyDict();
                    if (dim!=1) kwargs["dim"]=ToPython(dim);
                    if (eps!=1.0e-8f) kwargs["eps"]=ToPython(eps);
                    dynamic py = __self__.InvokeMethod("cosine_similarity", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Computes the p-norm distance between every pair of row vectors in the input.<br></br>
                ///	
                ///	This is identical to the upper triangular portion, excluding the diagonal, of
                ///	torch.norm(input[:, None] - input, dim=2, p=p).<br></br>
                ///	 This function will be faster
                ///	if the rows are contiguous.<br></br>
                ///	
                ///	If input has shape \(N \times M\) then the output will have shape
                ///	\(\frac{1}{2} N (N - 1)\).<br></br>
                ///	
                ///	This function is equivalent to scipy.spatial.distance.pdist(input,
                ///	‘minkowski’, p=p) if \(p \in (0, \infty)\).<br></br>
                ///	 When \(p = 0\) it is
                ///	equivalent to scipy.spatial.distance.pdist(input, ‘hamming’) * M.<br></br>
                ///	
                ///	When \(p = \infty\), the closest scipy function is
                ///	scipy.spatial.distance.pdist(xn, lambda x, y: np.abs(x - y).max()).
                /// </summary>
                /// <param name="input">
                ///	input tensor of shape \(N \times M\).
                /// </param>
                /// <param name="p">
                ///	p value for the p-norm distance to calculate between each vector pair
                ///	\(\in [0, \infty]\).
                /// </param>
                public static Tensor pdist(Tensor input, double p = 2)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (p!=2) kwargs["p"]=ToPython(p);
                    dynamic py = __self__.InvokeMethod("pdist", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Function that measures the Binary Cross Entropy
                ///	between the target and the output.<br></br>
                ///	
                ///	See BCELoss for details.
                /// </summary>
                /// <param name="input">
                ///	Tensor of arbitrary shape
                /// </param>
                /// <param name="target">
                ///	Tensor of the same shape as input
                /// </param>
                /// <param name="weight">
                ///	a manual rescaling weight
                ///	if provided it’s repeated to match input tensor shape
                /// </param>
                /// <param name="size_average">
                ///	Deprecated (see reduction).<br></br>
                ///	By default,
                ///	the losses are averaged over each loss element in the batch.<br></br>
                ///	Note that for
                ///	some losses, there multiple elements per sample.<br></br>
                ///	If the field size_average
                ///	is set to False, the losses are instead summed for each minibatch.<br></br>
                ///	Ignored
                ///	when reduce is False.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="reduce">
                ///	Deprecated (see reduction).<br></br>
                ///	By default, the
                ///	losses are averaged or summed over observations for each minibatch depending
                ///	on size_average.<br></br>
                ///	When reduce is False, returns a loss per
                ///	batch element instead and ignores size_average.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="reduction">
                ///	Specifies the reduction to apply to the output:
                ///	'none' | 'mean' | 'sum'. 'none': no reduction will be applied,
                ///	'mean': the sum of the output will be divided by the number of
                ///	elements in the output, 'sum': the output will be summed.<br></br>
                ///	Note: size_average
                ///	and reduce are in the process of being deprecated, and in the meantime,
                ///	specifying either of those two args will override reduction.<br></br>
                ///	Default: 'mean'
                /// </param>
                public static void binary_cross_entropy(Tensor input, Tensor target, double? weight = null, bool? size_average = null, bool? reduce = null, string reduction = "mean")
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        target,
                    });
                    var kwargs=new PyDict();
                    if (weight!=null) kwargs["weight"]=ToPython(weight);
                    if (size_average!=null) kwargs["size_average"]=ToPython(size_average);
                    if (reduce!=null) kwargs["reduce"]=ToPython(reduce);
                    if (reduction!="mean") kwargs["reduction"]=ToPython(reduction);
                    dynamic py = __self__.InvokeMethod("binary_cross_entropy", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Function that measures Binary Cross Entropy between target and output
                ///	logits.<br></br>
                ///	
                ///	See BCEWithLogitsLoss for details.
                /// </summary>
                /// <param name="input">
                ///	Tensor of arbitrary shape
                /// </param>
                /// <param name="target">
                ///	Tensor of the same shape as input
                /// </param>
                /// <param name="weight">
                ///	a manual rescaling weight
                ///	if provided it’s repeated to match input tensor shape
                /// </param>
                /// <param name="size_average">
                ///	Deprecated (see reduction).<br></br>
                ///	By default,
                ///	the losses are averaged over each loss element in the batch.<br></br>
                ///	Note that for
                ///	some losses, there multiple elements per sample.<br></br>
                ///	If the field size_average
                ///	is set to False, the losses are instead summed for each minibatch.<br></br>
                ///	Ignored
                ///	when reduce is False.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="reduce">
                ///	Deprecated (see reduction).<br></br>
                ///	By default, the
                ///	losses are averaged or summed over observations for each minibatch depending
                ///	on size_average.<br></br>
                ///	When reduce is False, returns a loss per
                ///	batch element instead and ignores size_average.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="reduction">
                ///	Specifies the reduction to apply to the output:
                ///	'none' | 'mean' | 'sum'. 'none': no reduction will be applied,
                ///	'mean': the sum of the output will be divided by the number of
                ///	elements in the output, 'sum': the output will be summed.<br></br>
                ///	Note: size_average
                ///	and reduce are in the process of being deprecated, and in the meantime,
                ///	specifying either of those two args will override reduction.<br></br>
                ///	Default: 'mean'
                /// </param>
                /// <param name="pos_weight">
                ///	a weight of positive examples.<br></br>
                ///	
                ///	Must be a vector with length equal to the number of classes.
                /// </param>
                public static void binary_cross_entropy_with_logits(Tensor input, Tensor target, double? weight = null, bool? size_average = null, bool? reduce = null, string reduction = "mean", Tensor pos_weight = null)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        target,
                    });
                    var kwargs=new PyDict();
                    if (weight!=null) kwargs["weight"]=ToPython(weight);
                    if (size_average!=null) kwargs["size_average"]=ToPython(size_average);
                    if (reduce!=null) kwargs["reduce"]=ToPython(reduce);
                    if (reduction!="mean") kwargs["reduction"]=ToPython(reduction);
                    if (pos_weight!=null) kwargs["pos_weight"]=ToPython(pos_weight);
                    dynamic py = __self__.InvokeMethod("binary_cross_entropy_with_logits", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Poisson negative log likelihood loss.<br></br>
                ///	
                ///	See PoissonNLLLoss for details.
                /// </summary>
                /// <param name="input">
                ///	expectation of underlying Poisson distribution.
                /// </param>
                /// <param name="target">
                ///	random sample \(target \sim \text{Poisson}(input)\).
                /// </param>
                /// <param name="log_input">
                ///	if True the loss is computed as
                ///	\(\exp(\text{input}) - \text{target} * \text{input}\), if False then loss is
                ///	\(\text{input} - \text{target} * \log(\text{input}+\text{eps})\).<br></br>
                ///	Default: True
                /// </param>
                /// <param name="full">
                ///	whether to compute full loss, i.<br></br>
                ///	e.<br></br>
                ///	to add the Stirling
                ///	approximation term.<br></br>
                ///	Default: False
                ///	\(\text{target} * \log(\text{target}) - \text{target} + 0.5 * \log(2 * \pi * \text{target})\).
                /// </param>
                /// <param name="size_average">
                ///	Deprecated (see reduction).<br></br>
                ///	By default,
                ///	the losses are averaged over each loss element in the batch.<br></br>
                ///	Note that for
                ///	some losses, there multiple elements per sample.<br></br>
                ///	If the field size_average
                ///	is set to False, the losses are instead summed for each minibatch.<br></br>
                ///	Ignored
                ///	when reduce is False.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="eps">
                ///	Small value to avoid evaluation of \(\log(0)\) when
                ///	log_input`=``False`. Default: 1e-8
                /// </param>
                /// <param name="reduce">
                ///	Deprecated (see reduction).<br></br>
                ///	By default, the
                ///	losses are averaged or summed over observations for each minibatch depending
                ///	on size_average.<br></br>
                ///	When reduce is False, returns a loss per
                ///	batch element instead and ignores size_average.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="reduction">
                ///	Specifies the reduction to apply to the output:
                ///	'none' | 'mean' | 'sum'. 'none': no reduction will be applied,
                ///	'mean': the sum of the output will be divided by the number of
                ///	elements in the output, 'sum': the output will be summed.<br></br>
                ///	Note: size_average
                ///	and reduce are in the process of being deprecated, and in the meantime,
                ///	specifying either of those two args will override reduction.<br></br>
                ///	Default: 'mean'
                /// </param>
                public static void poisson_nll_loss(Tensor input, Tensor target, bool log_input = true, bool full = false, bool? size_average = null, float? eps = 1.0e-08f, bool? reduce = null, string reduction = "mean")
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        target,
                    });
                    var kwargs=new PyDict();
                    if (log_input!=true) kwargs["log_input"]=ToPython(log_input);
                    if (full!=false) kwargs["full"]=ToPython(full);
                    if (size_average!=null) kwargs["size_average"]=ToPython(size_average);
                    if (eps!=1.0e-08f) kwargs["eps"]=ToPython(eps);
                    if (reduce!=null) kwargs["reduce"]=ToPython(reduce);
                    if (reduction!="mean") kwargs["reduction"]=ToPython(reduction);
                    dynamic py = __self__.InvokeMethod("poisson_nll_loss", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	See CosineEmbeddingLoss for details.
                /// </summary>
                /// <param name="margin">
                ///	Should be a number from \(-1\) to \(1\),
                ///	\(0\) to \(0.5\) is suggested.<br></br>
                ///	If margin is missing, the
                ///	default value is \(0\).
                /// </param>
                /// <param name="size_average">
                ///	Deprecated (see reduction).<br></br>
                ///	By default,
                ///	the losses are averaged over each loss element in the batch.<br></br>
                ///	Note that for
                ///	some losses, there are multiple elements per sample.<br></br>
                ///	If the field size_average
                ///	is set to False, the losses are instead summed for each minibatch.<br></br>
                ///	Ignored
                ///	when reduce is False.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="reduce">
                ///	Deprecated (see reduction).<br></br>
                ///	By default, the
                ///	losses are averaged or summed over observations for each minibatch depending
                ///	on size_average.<br></br>
                ///	When reduce is False, returns a loss per
                ///	batch element instead and ignores size_average.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="reduction">
                ///	Specifies the reduction to apply to the output:
                ///	'none' | 'mean' | 'sum'. 'none': no reduction will be applied,
                ///	'mean': the sum of the output will be divided by the number of
                ///	elements in the output, 'sum': the output will be summed.<br></br>
                ///	Note: size_average
                ///	and reduce are in the process of being deprecated, and in the meantime,
                ///	specifying either of those two args will override reduction.<br></br>
                ///	Default: 'mean'
                /// </param>
                public static Tensor cosine_embedding_loss(Tensor input, float? margin = 0.0f, bool? size_average = null, bool? reduce = null, string reduction = "mean")
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (margin!=0.0f) kwargs["margin"]=ToPython(margin);
                    if (size_average!=null) kwargs["size_average"]=ToPython(size_average);
                    if (reduce!=null) kwargs["reduce"]=ToPython(reduce);
                    if (reduction!="mean") kwargs["reduction"]=ToPython(reduction);
                    dynamic py = __self__.InvokeMethod("cosine_embedding_loss", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	This criterion combines log_softmax and nll_loss in a single
                ///	function.<br></br>
                ///	
                ///	See CrossEntropyLoss for details.
                /// </summary>
                /// <param name="input">
                ///	\((N, C)\) where C = number of classes or \((N, C, H, W)\)
                ///	in case of 2D Loss, or \((N, C, d_1, d_2, ..., d_K)\) where \(K \geq 1\)
                ///	in the case of K-dimensional loss.
                /// </param>
                /// <param name="target">
                ///	\((N)\) where each value is \(0 \leq \text{targets}[i] \leq C-1\),
                ///	or \((N, d_1, d_2, ..., d_K)\) where \(K \geq 1\) for
                ///	K-dimensional loss.
                /// </param>
                /// <param name="weight">
                ///	a manual rescaling weight given to each
                ///	class.<br></br>
                ///	If given, has to be a Tensor of size C
                /// </param>
                /// <param name="size_average">
                ///	Deprecated (see reduction).<br></br>
                ///	By default,
                ///	the losses are averaged over each loss element in the batch.<br></br>
                ///	Note that for
                ///	some losses, there multiple elements per sample.<br></br>
                ///	If the field size_average
                ///	is set to False, the losses are instead summed for each minibatch.<br></br>
                ///	Ignored
                ///	when reduce is False.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="ignore_index">
                ///	Specifies a target value that is ignored
                ///	and does not contribute to the input gradient.<br></br>
                ///	When size_average is
                ///	True, the loss is averaged over non-ignored targets.<br></br>
                ///	Default: -100
                /// </param>
                /// <param name="reduce">
                ///	Deprecated (see reduction).<br></br>
                ///	By default, the
                ///	losses are averaged or summed over observations for each minibatch depending
                ///	on size_average.<br></br>
                ///	When reduce is False, returns a loss per
                ///	batch element instead and ignores size_average.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="reduction">
                ///	Specifies the reduction to apply to the output:
                ///	'none' | 'mean' | 'sum'. 'none': no reduction will be applied,
                ///	'mean': the sum of the output will be divided by the number of
                ///	elements in the output, 'sum': the output will be summed.<br></br>
                ///	Note: size_average
                ///	and reduce are in the process of being deprecated, and in the meantime,
                ///	specifying either of those two args will override reduction.<br></br>
                ///	Default: 'mean'
                /// </param>
                public static void cross_entropy(Tensor input, Tensor target, double? weight = null, bool? size_average = null, int? ignore_index = -100, bool? reduce = null, string reduction = "mean")
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        target,
                    });
                    var kwargs=new PyDict();
                    if (weight!=null) kwargs["weight"]=ToPython(weight);
                    if (size_average!=null) kwargs["size_average"]=ToPython(size_average);
                    if (ignore_index!=-100) kwargs["ignore_index"]=ToPython(ignore_index);
                    if (reduce!=null) kwargs["reduce"]=ToPython(reduce);
                    if (reduction!="mean") kwargs["reduction"]=ToPython(reduction);
                    dynamic py = __self__.InvokeMethod("cross_entropy", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	The Connectionist Temporal Classification loss.<br></br>
                ///	
                ///	See CTCLoss for details.<br></br>
                ///	
                ///	Note
                ///	In some circumstances when using the CUDA backend with CuDNN, this operator
                ///	may select a nondeterministic algorithm to increase performance.<br></br>
                ///	 If this is
                ///	undesirable, you can try to make the operation deterministic (potentially at
                ///	a performance cost) by setting torch.backends.cudnn.deterministic =
                ///	True.<br></br>
                ///	
                ///	Please see the notes on Reproducibility for background.<br></br>
                ///	
                ///	Note
                ///	When using the CUDA backend, this operation may induce nondeterministic
                ///	behaviour in be backward that is not easily switched off.<br></br>
                ///	
                ///	Please see the notes on Reproducibility for background.
                /// </summary>
                /// <param name="log_probs">
                ///	\((T, N, C)\) where C = number of characters in alphabet including blank,
                ///	T = input length, and N = batch size.<br></br>
                ///	
                ///	The logarithmized probabilities of the outputs
                ///	(e.g.<br></br>
                ///	obtained with torch.nn.functional.log_softmax()).
                /// </param>
                /// <param name="targets">
                ///	\((N, S)\) or (sum(target_lengths)).<br></br>
                ///	
                ///	Targets cannot be blank.<br></br>
                ///	In the second form, the targets are assumed to be concatenated.
                /// </param>
                /// <param name="input_lengths">
                ///	\((N)\).<br></br>
                ///	
                ///	Lengths of the inputs (must each be \(\leq T\))
                /// </param>
                /// <param name="target_lengths">
                ///	\((N)\).<br></br>
                ///	
                ///	Lengths of the targets
                /// </param>
                /// <param name="blank">
                ///	Blank label.<br></br>
                ///	Default \(0\).
                /// </param>
                /// <param name="reduction">
                ///	Specifies the reduction to apply to the output:
                ///	'none' | 'mean' | 'sum'. 'none': no reduction will be applied,
                ///	'mean': the output losses will be divided by the target lengths and
                ///	then the mean over the batch is taken, 'sum': the output will be
                ///	summed.<br></br>
                ///	Default: 'mean'
                /// </param>
                /// <param name="zero_infinity">
                ///	Whether to zero infinite losses and the associated gradients.<br></br>
                ///	
                ///	Default: False
                ///	Infinite losses mainly occur when the inputs are too short
                ///	to be aligned to the targets.
                /// </param>
                public static void ctc_loss(int log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int? blank = 0, string reduction = "mean", bool? zero_infinity = false)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        log_probs,
                        targets,
                        input_lengths,
                        target_lengths,
                    });
                    var kwargs=new PyDict();
                    if (blank!=0) kwargs["blank"]=ToPython(blank);
                    if (reduction!="mean") kwargs["reduction"]=ToPython(reduction);
                    if (zero_infinity!=false) kwargs["zero_infinity"]=ToPython(zero_infinity);
                    dynamic py = __self__.InvokeMethod("ctc_loss", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	See HingeEmbeddingLoss for details.
                /// </summary>
                /// <param name="margin">
                ///	Has a default value of 1.
                /// </param>
                /// <param name="size_average">
                ///	Deprecated (see reduction).<br></br>
                ///	By default,
                ///	the losses are averaged over each loss element in the batch.<br></br>
                ///	Note that for
                ///	some losses, there are multiple elements per sample.<br></br>
                ///	If the field size_average
                ///	is set to False, the losses are instead summed for each minibatch.<br></br>
                ///	Ignored
                ///	when reduce is False.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="reduce">
                ///	Deprecated (see reduction).<br></br>
                ///	By default, the
                ///	losses are averaged or summed over observations for each minibatch depending
                ///	on size_average.<br></br>
                ///	When reduce is False, returns a loss per
                ///	batch element instead and ignores size_average.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="reduction">
                ///	Specifies the reduction to apply to the output:
                ///	'none' | 'mean' | 'sum'. 'none': no reduction will be applied,
                ///	'mean': the sum of the output will be divided by the number of
                ///	elements in the output, 'sum': the output will be summed.<br></br>
                ///	Note: size_average
                ///	and reduce are in the process of being deprecated, and in the meantime,
                ///	specifying either of those two args will override reduction.<br></br>
                ///	Default: 'mean'
                /// </param>
                public static Tensor hinge_embedding_loss(Tensor input, float? margin = 1.0f, bool? size_average = null, bool? reduce = null, string reduction = "mean")
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (margin!=1.0f) kwargs["margin"]=ToPython(margin);
                    if (size_average!=null) kwargs["size_average"]=ToPython(size_average);
                    if (reduce!=null) kwargs["reduce"]=ToPython(reduce);
                    if (reduction!="mean") kwargs["reduction"]=ToPython(reduction);
                    dynamic py = __self__.InvokeMethod("hinge_embedding_loss", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	The Kullback-Leibler divergence Loss.<br></br>
                ///	
                ///	See KLDivLoss for details.
                /// </summary>
                /// <param name="input">
                ///	Tensor of arbitrary shape
                /// </param>
                /// <param name="target">
                ///	Tensor of the same shape as input
                /// </param>
                /// <param name="size_average">
                ///	Deprecated (see reduction).<br></br>
                ///	By default,
                ///	the losses are averaged over each loss element in the batch.<br></br>
                ///	Note that for
                ///	some losses, there multiple elements per sample.<br></br>
                ///	If the field size_average
                ///	is set to False, the losses are instead summed for each minibatch.<br></br>
                ///	Ignored
                ///	when reduce is False.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="reduce">
                ///	Deprecated (see reduction).<br></br>
                ///	By default, the
                ///	losses are averaged or summed over observations for each minibatch depending
                ///	on size_average.<br></br>
                ///	When reduce is False, returns a loss per
                ///	batch element instead and ignores size_average.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="reduction">
                ///	Specifies the reduction to apply to the output:
                ///	'none' | 'batchmean' | 'sum' | 'mean'.
                ///	'none': no reduction will be applied
                ///	'batchmean': the sum of the output will be divided by the batchsize
                ///	'sum': the output will be summed
                ///	'mean': the output will be divided by the number of elements in the output
                ///	Default: 'mean'
                /// </param>
                public static void kl_div(Tensor input, Tensor target, bool? size_average = null, bool? reduce = null, string reduction = "mean")
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        target,
                    });
                    var kwargs=new PyDict();
                    if (size_average!=null) kwargs["size_average"]=ToPython(size_average);
                    if (reduce!=null) kwargs["reduce"]=ToPython(reduce);
                    if (reduction!="mean") kwargs["reduction"]=ToPython(reduction);
                    dynamic py = __self__.InvokeMethod("kl_div", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Function that takes the mean element-wise absolute value difference.<br></br>
                ///	
                ///	See L1Loss for details.
                /// </summary>
                /// <param name="size_average">
                ///	Deprecated (see reduction).<br></br>
                ///	By default,
                ///	the losses are averaged over each loss element in the batch.<br></br>
                ///	Note that for
                ///	some losses, there are multiple elements per sample.<br></br>
                ///	If the field size_average
                ///	is set to False, the losses are instead summed for each minibatch.<br></br>
                ///	Ignored
                ///	when reduce is False.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="reduce">
                ///	Deprecated (see reduction).<br></br>
                ///	By default, the
                ///	losses are averaged or summed over observations for each minibatch depending
                ///	on size_average.<br></br>
                ///	When reduce is False, returns a loss per
                ///	batch element instead and ignores size_average.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="reduction">
                ///	Specifies the reduction to apply to the output:
                ///	'none' | 'mean' | 'sum'. 'none': no reduction will be applied,
                ///	'mean': the sum of the output will be divided by the number of
                ///	elements in the output, 'sum': the output will be summed.<br></br>
                ///	Note: size_average
                ///	and reduce are in the process of being deprecated, and in the meantime,
                ///	specifying either of those two args will override reduction.<br></br>
                ///	Default: 'mean'
                /// </param>
                public static Tensor l1_loss(Tensor input, bool? size_average = null, bool? reduce = null, string reduction = "mean")
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (size_average!=null) kwargs["size_average"]=ToPython(size_average);
                    if (reduce!=null) kwargs["reduce"]=ToPython(reduce);
                    if (reduction!="mean") kwargs["reduction"]=ToPython(reduction);
                    dynamic py = __self__.InvokeMethod("l1_loss", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Measures the element-wise mean squared error.<br></br>
                ///	
                ///	See MSELoss for details.
                /// </summary>
                /// <param name="size_average">
                ///	Deprecated (see reduction).<br></br>
                ///	By default,
                ///	the losses are averaged over each loss element in the batch.<br></br>
                ///	Note that for
                ///	some losses, there are multiple elements per sample.<br></br>
                ///	If the field size_average
                ///	is set to False, the losses are instead summed for each minibatch.<br></br>
                ///	Ignored
                ///	when reduce is False.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="reduce">
                ///	Deprecated (see reduction).<br></br>
                ///	By default, the
                ///	losses are averaged or summed over observations for each minibatch depending
                ///	on size_average.<br></br>
                ///	When reduce is False, returns a loss per
                ///	batch element instead and ignores size_average.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="reduction">
                ///	Specifies the reduction to apply to the output:
                ///	'none' | 'mean' | 'sum'. 'none': no reduction will be applied,
                ///	'mean': the sum of the output will be divided by the number of
                ///	elements in the output, 'sum': the output will be summed.<br></br>
                ///	Note: size_average
                ///	and reduce are in the process of being deprecated, and in the meantime,
                ///	specifying either of those two args will override reduction.<br></br>
                ///	Default: 'mean'
                /// </param>
                public static Tensor mse_loss(Tensor input, bool? size_average = null, bool? reduce = null, string reduction = "mean")
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (size_average!=null) kwargs["size_average"]=ToPython(size_average);
                    if (reduce!=null) kwargs["reduce"]=ToPython(reduce);
                    if (reduction!="mean") kwargs["reduction"]=ToPython(reduction);
                    dynamic py = __self__.InvokeMethod("mse_loss", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	See MarginRankingLoss for details.
                /// </summary>
                /// <param name="margin">
                ///	Has a default value of \(0\).
                /// </param>
                /// <param name="size_average">
                ///	Deprecated (see reduction).<br></br>
                ///	By default,
                ///	the losses are averaged over each loss element in the batch.<br></br>
                ///	Note that for
                ///	some losses, there are multiple elements per sample.<br></br>
                ///	If the field size_average
                ///	is set to False, the losses are instead summed for each minibatch.<br></br>
                ///	Ignored
                ///	when reduce is False.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="reduce">
                ///	Deprecated (see reduction).<br></br>
                ///	By default, the
                ///	losses are averaged or summed over observations for each minibatch depending
                ///	on size_average.<br></br>
                ///	When reduce is False, returns a loss per
                ///	batch element instead and ignores size_average.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="reduction">
                ///	Specifies the reduction to apply to the output:
                ///	'none' | 'mean' | 'sum'. 'none': no reduction will be applied,
                ///	'mean': the sum of the output will be divided by the number of
                ///	elements in the output, 'sum': the output will be summed.<br></br>
                ///	Note: size_average
                ///	and reduce are in the process of being deprecated, and in the meantime,
                ///	specifying either of those two args will override reduction.<br></br>
                ///	Default: 'mean'
                /// </param>
                public static Tensor margin_ranking_loss(Tensor input, float? margin = 0.0f, bool? size_average = null, bool? reduce = null, string reduction = "mean")
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (margin!=0.0f) kwargs["margin"]=ToPython(margin);
                    if (size_average!=null) kwargs["size_average"]=ToPython(size_average);
                    if (reduce!=null) kwargs["reduce"]=ToPython(reduce);
                    if (reduction!="mean") kwargs["reduction"]=ToPython(reduction);
                    dynamic py = __self__.InvokeMethod("margin_ranking_loss", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	See MultiLabelMarginLoss for details.
                /// </summary>
                /// <param name="size_average">
                ///	Deprecated (see reduction).<br></br>
                ///	By default,
                ///	the losses are averaged over each loss element in the batch.<br></br>
                ///	Note that for
                ///	some losses, there are multiple elements per sample.<br></br>
                ///	If the field size_average
                ///	is set to False, the losses are instead summed for each minibatch.<br></br>
                ///	Ignored
                ///	when reduce is False.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="reduce">
                ///	Deprecated (see reduction).<br></br>
                ///	By default, the
                ///	losses are averaged or summed over observations for each minibatch depending
                ///	on size_average.<br></br>
                ///	When reduce is False, returns a loss per
                ///	batch element instead and ignores size_average.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="reduction">
                ///	Specifies the reduction to apply to the output:
                ///	'none' | 'mean' | 'sum'. 'none': no reduction will be applied,
                ///	'mean': the sum of the output will be divided by the number of
                ///	elements in the output, 'sum': the output will be summed.<br></br>
                ///	Note: size_average
                ///	and reduce are in the process of being deprecated, and in the meantime,
                ///	specifying either of those two args will override reduction.<br></br>
                ///	Default: 'mean'
                /// </param>
                public static Tensor multilabel_margin_loss(Tensor input, bool? size_average = null, bool? reduce = null, string reduction = "mean")
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (size_average!=null) kwargs["size_average"]=ToPython(size_average);
                    if (reduce!=null) kwargs["reduce"]=ToPython(reduce);
                    if (reduction!="mean") kwargs["reduction"]=ToPython(reduction);
                    dynamic py = __self__.InvokeMethod("multilabel_margin_loss", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	See MultiLabelSoftMarginLoss for details.
                /// </summary>
                /// <param name="weight">
                ///	a manual rescaling weight given to each
                ///	class.<br></br>
                ///	If given, it has to be a Tensor of size C.<br></br>
                ///	Otherwise, it is
                ///	treated as if having all ones.
                /// </param>
                /// <param name="size_average">
                ///	Deprecated (see reduction).<br></br>
                ///	By default,
                ///	the losses are averaged over each loss element in the batch.<br></br>
                ///	Note that for
                ///	some losses, there are multiple elements per sample.<br></br>
                ///	If the field size_average
                ///	is set to False, the losses are instead summed for each minibatch.<br></br>
                ///	Ignored
                ///	when reduce is False.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="reduce">
                ///	Deprecated (see reduction).<br></br>
                ///	By default, the
                ///	losses are averaged or summed over observations for each minibatch depending
                ///	on size_average.<br></br>
                ///	When reduce is False, returns a loss per
                ///	batch element instead and ignores size_average.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="reduction">
                ///	Specifies the reduction to apply to the output:
                ///	'none' | 'mean' | 'sum'. 'none': no reduction will be applied,
                ///	'mean': the sum of the output will be divided by the number of
                ///	elements in the output, 'sum': the output will be summed.<br></br>
                ///	Note: size_average
                ///	and reduce are in the process of being deprecated, and in the meantime,
                ///	specifying either of those two args will override reduction.<br></br>
                ///	Default: 'mean'
                /// </param>
                public static Tensor multilabel_soft_margin_loss(Tensor input, double? weight = null, bool? size_average = null, bool? reduce = null, string reduction = "mean")
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (weight!=null) kwargs["weight"]=ToPython(weight);
                    if (size_average!=null) kwargs["size_average"]=ToPython(size_average);
                    if (reduce!=null) kwargs["reduce"]=ToPython(reduce);
                    if (reduction!="mean") kwargs["reduction"]=ToPython(reduction);
                    dynamic py = __self__.InvokeMethod("multilabel_soft_margin_loss", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                public static void multi_margin_loss(Tensor input, int? p = 1, float? margin = 1.0f, double? weight = null, bool? size_average = null, bool? reduce = null, string reduction = "mean")
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (p!=1) kwargs["p"]=ToPython(p);
                    if (margin!=1.0f) kwargs["margin"]=ToPython(margin);
                    if (weight!=null) kwargs["weight"]=ToPython(weight);
                    if (size_average!=null) kwargs["size_average"]=ToPython(size_average);
                    if (reduce!=null) kwargs["reduce"]=ToPython(reduce);
                    if (reduction!="mean") kwargs["reduction"]=ToPython(reduction);
                    dynamic py = __self__.InvokeMethod("multi_margin_loss", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	The negative log likelihood loss.<br></br>
                ///	
                ///	See NLLLoss for details.
                /// </summary>
                /// <param name="input">
                ///	\((N, C)\) where C = number of classes or \((N, C, H, W)\)
                ///	in case of 2D Loss, or \((N, C, d_1, d_2, ..., d_K)\) where \(K \geq 1\)
                ///	in the case of K-dimensional loss.
                /// </param>
                /// <param name="target">
                ///	\((N)\) where each value is \(0 \leq \text{targets}[i] \leq C-1\),
                ///	or \((N, d_1, d_2, ..., d_K)\) where \(K \geq 1\) for
                ///	K-dimensional loss.
                /// </param>
                /// <param name="weight">
                ///	a manual rescaling weight given to each
                ///	class.<br></br>
                ///	If given, has to be a Tensor of size C
                /// </param>
                /// <param name="size_average">
                ///	Deprecated (see reduction).<br></br>
                ///	By default,
                ///	the losses are averaged over each loss element in the batch.<br></br>
                ///	Note that for
                ///	some losses, there multiple elements per sample.<br></br>
                ///	If the field size_average
                ///	is set to False, the losses are instead summed for each minibatch.<br></br>
                ///	Ignored
                ///	when reduce is False.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="ignore_index">
                ///	Specifies a target value that is ignored
                ///	and does not contribute to the input gradient.<br></br>
                ///	When size_average is
                ///	True, the loss is averaged over non-ignored targets.<br></br>
                ///	Default: -100
                /// </param>
                /// <param name="reduce">
                ///	Deprecated (see reduction).<br></br>
                ///	By default, the
                ///	losses are averaged or summed over observations for each minibatch depending
                ///	on size_average.<br></br>
                ///	When reduce is False, returns a loss per
                ///	batch element instead and ignores size_average.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="reduction">
                ///	Specifies the reduction to apply to the output:
                ///	'none' | 'mean' | 'sum'. 'none': no reduction will be applied,
                ///	'mean': the sum of the output will be divided by the number of
                ///	elements in the output, 'sum': the output will be summed.<br></br>
                ///	Note: size_average
                ///	and reduce are in the process of being deprecated, and in the meantime,
                ///	specifying either of those two args will override reduction.<br></br>
                ///	Default: 'mean'
                /// </param>
                public static void nll_loss(int input, Tensor target, double? weight = null, bool? size_average = null, int? ignore_index = -100, bool? reduce = null, string reduction = "mean")
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        target,
                    });
                    var kwargs=new PyDict();
                    if (weight!=null) kwargs["weight"]=ToPython(weight);
                    if (size_average!=null) kwargs["size_average"]=ToPython(size_average);
                    if (ignore_index!=-100) kwargs["ignore_index"]=ToPython(ignore_index);
                    if (reduce!=null) kwargs["reduce"]=ToPython(reduce);
                    if (reduction!="mean") kwargs["reduction"]=ToPython(reduction);
                    dynamic py = __self__.InvokeMethod("nll_loss", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Function that uses a squared term if the absolute
                ///	element-wise error falls below 1 and an L1 term otherwise.<br></br>
                ///	
                ///	See SmoothL1Loss for details.
                /// </summary>
                /// <param name="size_average">
                ///	Deprecated (see reduction).<br></br>
                ///	By default,
                ///	the losses are averaged over each loss element in the batch.<br></br>
                ///	Note that for
                ///	some losses, there are multiple elements per sample.<br></br>
                ///	If the field size_average
                ///	is set to False, the losses are instead summed for each minibatch.<br></br>
                ///	Ignored
                ///	when reduce is False.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="reduce">
                ///	Deprecated (see reduction).<br></br>
                ///	By default, the
                ///	losses are averaged or summed over observations for each minibatch depending
                ///	on size_average.<br></br>
                ///	When reduce is False, returns a loss per
                ///	batch element instead and ignores size_average.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="reduction">
                ///	Specifies the reduction to apply to the output:
                ///	'none' | 'mean' | 'sum'. 'none': no reduction will be applied,
                ///	'mean': the sum of the output will be divided by the number of
                ///	elements in the output, 'sum': the output will be summed.<br></br>
                ///	Note: size_average
                ///	and reduce are in the process of being deprecated, and in the meantime,
                ///	specifying either of those two args will override reduction.<br></br>
                ///	Default: 'mean'
                /// </param>
                public static void smooth_l1_loss(Tensor input, bool? size_average = null, bool? reduce = null, string reduction = "mean")
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (size_average!=null) kwargs["size_average"]=ToPython(size_average);
                    if (reduce!=null) kwargs["reduce"]=ToPython(reduce);
                    if (reduction!="mean") kwargs["reduction"]=ToPython(reduction);
                    dynamic py = __self__.InvokeMethod("smooth_l1_loss", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	See SoftMarginLoss for details.
                /// </summary>
                /// <param name="size_average">
                ///	Deprecated (see reduction).<br></br>
                ///	By default,
                ///	the losses are averaged over each loss element in the batch.<br></br>
                ///	Note that for
                ///	some losses, there are multiple elements per sample.<br></br>
                ///	If the field size_average
                ///	is set to False, the losses are instead summed for each minibatch.<br></br>
                ///	Ignored
                ///	when reduce is False.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="reduce">
                ///	Deprecated (see reduction).<br></br>
                ///	By default, the
                ///	losses are averaged or summed over observations for each minibatch depending
                ///	on size_average.<br></br>
                ///	When reduce is False, returns a loss per
                ///	batch element instead and ignores size_average.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="reduction">
                ///	Specifies the reduction to apply to the output:
                ///	'none' | 'mean' | 'sum'. 'none': no reduction will be applied,
                ///	'mean': the sum of the output will be divided by the number of
                ///	elements in the output, 'sum': the output will be summed.<br></br>
                ///	Note: size_average
                ///	and reduce are in the process of being deprecated, and in the meantime,
                ///	specifying either of those two args will override reduction.<br></br>
                ///	Default: 'mean'
                /// </param>
                public static Tensor soft_margin_loss(Tensor input, bool? size_average = null, bool? reduce = null, string reduction = "mean")
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (size_average!=null) kwargs["size_average"]=ToPython(size_average);
                    if (reduce!=null) kwargs["reduce"]=ToPython(reduce);
                    if (reduction!="mean") kwargs["reduction"]=ToPython(reduction);
                    dynamic py = __self__.InvokeMethod("soft_margin_loss", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	See TripletMarginLoss for details
                /// </summary>
                /// <param name="margin">
                ///	Default: \(1\).
                /// </param>
                /// <param name="p">
                ///	The norm degree for pairwise distance.<br></br>
                ///	Default: \(2\).
                /// </param>
                /// <param name="swap">
                ///	The distance swap is described in detail in the paper
                ///	Learning shallow convolutional feature descriptors with triplet losses by
                ///	V.<br></br>
                ///	Balntas, E.<br></br>
                ///	Riba et al.<br></br>
                ///	Default: False.
                /// </param>
                /// <param name="size_average">
                ///	Deprecated (see reduction).<br></br>
                ///	By default,
                ///	the losses are averaged over each loss element in the batch.<br></br>
                ///	Note that for
                ///	some losses, there are multiple elements per sample.<br></br>
                ///	If the field size_average
                ///	is set to False, the losses are instead summed for each minibatch.<br></br>
                ///	Ignored
                ///	when reduce is False.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="reduce">
                ///	Deprecated (see reduction).<br></br>
                ///	By default, the
                ///	losses are averaged or summed over observations for each minibatch depending
                ///	on size_average.<br></br>
                ///	When reduce is False, returns a loss per
                ///	batch element instead and ignores size_average.<br></br>
                ///	Default: True
                /// </param>
                /// <param name="reduction">
                ///	Specifies the reduction to apply to the output:
                ///	'none' | 'mean' | 'sum'. 'none': no reduction will be applied,
                ///	'mean': the sum of the output will be divided by the number of
                ///	elements in the output, 'sum': the output will be summed.<br></br>
                ///	Note: size_average
                ///	and reduce are in the process of being deprecated, and in the meantime,
                ///	specifying either of those two args will override reduction.<br></br>
                ///	Default: 'mean'
                /// </param>
                public static void triplet_margin_loss(Tensor input, float? margin = 1.0f, int? p = 2, bool? swap = false, bool? size_average = null, bool? reduce = null, string reduction = "mean")
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (margin!=1.0f) kwargs["margin"]=ToPython(margin);
                    if (p!=2) kwargs["p"]=ToPython(p);
                    if (swap!=false) kwargs["swap"]=ToPython(swap);
                    if (size_average!=null) kwargs["size_average"]=ToPython(size_average);
                    if (reduce!=null) kwargs["reduce"]=ToPython(reduce);
                    if (reduction!="mean") kwargs["reduction"]=ToPython(reduction);
                    dynamic py = __self__.InvokeMethod("triplet_margin_loss", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Rearranges elements in a tensor of shape \((*, C \times r^2, H, W)\) to a
                ///	tensor of shape \((*, C, H \times r, W \times r)\).<br></br>
                ///	
                ///	See PixelShuffle for details.
                /// </summary>
                /// <param name="input">
                ///	the input tensor
                /// </param>
                /// <param name="upscale_factor">
                ///	factor to increase spatial resolution by
                /// </param>
                public static void pixel_shuffle(Tensor input, int upscale_factor)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        upscale_factor,
                    });
                    var kwargs=new PyDict();
                    dynamic py = __self__.InvokeMethod("pixel_shuffle", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Pads tensor.
                /// </summary>
                /// <param name="input">
                ///	N-dimensional tensor
                /// </param>
                /// <param name="pad">
                ///	m-elements tuple, where
                ///	\(\frac{m}{2} \leq\) input dimensions and \(m\) is even.
                /// </param>
                /// <param name="mode">
                ///	'constant', 'reflect', 'replicate' or 'circular'.
                ///	Default: 'constant'
                /// </param>
                /// <param name="value">
                ///	fill value for 'constant' padding.<br></br>
                ///	Default: 0
                /// </param>
                public static void pad(Tensor input, int[] pad, string mode = "constant", int @value = 0)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        pad,
                    });
                    var kwargs=new PyDict();
                    if (mode!="constant") kwargs["mode"]=ToPython(mode);
                    if (@value!=0) kwargs["value"]=ToPython(@value);
                    dynamic py = __self__.InvokeMethod("pad", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Down/up samples the input to either the given size or the given
                ///	scale_factor
                ///	
                ///	The algorithm used for interpolation is determined by mode.<br></br>
                ///	
                ///	Currently temporal, spatial and volumetric sampling are supported, i.e.<br></br>
                ///	
                ///	expected inputs are 3-D, 4-D or 5-D in shape.<br></br>
                ///	
                ///	The input dimensions are interpreted in the form:
                ///	mini-batch x channels x [optional depth] x [optional height] x width.<br></br>
                ///	
                ///	The modes available for resizing are: nearest, linear (3D-only),
                ///	bilinear, bicubic (4D-only), trilinear (5D-only), area
                /// </summary>
                /// <param name="input">
                ///	the input tensor
                /// </param>
                /// <param name="size">
                ///	output spatial size.
                /// </param>
                /// <param name="scale_factor">
                ///	multiplier for spatial size.<br></br>
                ///	Has to match input size if it is a tuple.
                /// </param>
                /// <param name="mode">
                ///	algorithm used for upsampling:
                ///	'nearest' | 'linear' | 'bilinear' | 'bicubic' |
                ///	'trilinear' | 'area'. Default: 'nearest'
                /// </param>
                /// <param name="align_corners">
                ///	Geometrically, we consider the pixels of the
                ///	input and output as squares rather than points.<br></br>
                ///	
                ///	If set to True, the input and output tensors are aligned by the
                ///	center points of their corner pixels.<br></br>
                ///	If set to False, the input and
                ///	output tensors are aligned by the corner points of their corner
                ///	pixels, and the interpolation uses edge value padding for out-of-boundary values.<br></br>
                ///	
                ///	This only has effect when mode is 'linear',
                ///	'bilinear', 'bicubic', or 'trilinear'.
                ///	Default: False
                /// </param>
                public static void interpolate(Tensor input, int[] size = null, float[] scale_factor = null, string mode = "nearest", bool? align_corners = null)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (size!=null) kwargs["size"]=ToPython(size);
                    if (scale_factor!=null) kwargs["scale_factor"]=ToPython(scale_factor);
                    if (mode!="nearest") kwargs["mode"]=ToPython(mode);
                    if (align_corners!=null) kwargs["align_corners"]=ToPython(align_corners);
                    dynamic py = __self__.InvokeMethod("interpolate", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Upsamples the input to either the given size or the given
                ///	scale_factor
                ///	
                ///	Warning
                ///	This function is deprecated in favor of torch.nn.functional.interpolate().<br></br>
                ///	
                ///	This is equivalent with nn.functional.interpolate(...).<br></br>
                ///	
                ///	Note
                ///	When using the CUDA backend, this operation may induce nondeterministic
                ///	behaviour in be backward that is not easily switched off.<br></br>
                ///	
                ///	Please see the notes on Reproducibility for background.<br></br>
                ///	
                ///	The algorithm used for upsampling is determined by mode.<br></br>
                ///	
                ///	Currently temporal, spatial and volumetric upsampling are supported, i.e.<br></br>
                ///	
                ///	expected inputs are 3-D, 4-D or 5-D in shape.<br></br>
                ///	
                ///	The input dimensions are interpreted in the form:
                ///	mini-batch x channels x [optional depth] x [optional height] x width.<br></br>
                ///	
                ///	The modes available for upsampling are: nearest, linear (3D-only),
                ///	bilinear, bicubic (4D-only), trilinear (5D-only)
                /// </summary>
                /// <param name="input">
                ///	the input tensor
                /// </param>
                /// <param name="size">
                ///	output spatial size.
                /// </param>
                /// <param name="scale_factor">
                ///	multiplier for spatial size.<br></br>
                ///	Has to be an integer.
                /// </param>
                /// <param name="mode">
                ///	algorithm used for upsampling:
                ///	'nearest' | 'linear' | 'bilinear' | 'bicubic' |
                ///	'trilinear'. Default: 'nearest'
                /// </param>
                /// <param name="align_corners">
                ///	Geometrically, we consider the pixels of the
                ///	input and output as squares rather than points.<br></br>
                ///	
                ///	If set to True, the input and output tensors are aligned by the
                ///	center points of their corner pixels.<br></br>
                ///	If set to False, the input and
                ///	output tensors are aligned by the corner points of their corner
                ///	pixels, and the interpolation uses edge value padding for out-of-boundary values.<br></br>
                ///	
                ///	This only has effect when mode is 'linear',
                ///	'bilinear', 'bicubic' or 'trilinear'.
                ///	Default: False
                /// </param>
                public static void upsample(Tensor input, int[] size = null, float[] scale_factor = null, string mode = "nearest", bool? align_corners = null)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (size!=null) kwargs["size"]=ToPython(size);
                    if (scale_factor!=null) kwargs["scale_factor"]=ToPython(scale_factor);
                    if (mode!="nearest") kwargs["mode"]=ToPython(mode);
                    if (align_corners!=null) kwargs["align_corners"]=ToPython(align_corners);
                    dynamic py = __self__.InvokeMethod("upsample", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Upsamples the input, using nearest neighbours’ pixel values.<br></br>
                ///	
                ///	Warning
                ///	This function is deprecated in favor of torch.nn.functional.interpolate().<br></br>
                ///	
                ///	This is equivalent with nn.functional.interpolate(..., mode='nearest').<br></br>
                ///	
                ///	Currently spatial and volumetric upsampling are supported (i.e.<br></br>
                ///	 expected
                ///	inputs are 4 or 5 dimensional).
                /// </summary>
                /// <param name="input">
                ///	input
                /// </param>
                /// <param name="size">
                ///	output spatia
                ///	size.
                /// </param>
                /// <param name="scale_factor">
                ///	multiplier for spatial size.<br></br>
                ///	Has to be an integer.
                /// </param>
                public static void upsample_nearest(Tensor input, int[] size = null, float[] scale_factor = null)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (size!=null) kwargs["size"]=ToPython(size);
                    if (scale_factor!=null) kwargs["scale_factor"]=ToPython(scale_factor);
                    dynamic py = __self__.InvokeMethod("upsample_nearest", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Upsamples the input, using bilinear upsampling.<br></br>
                ///	
                ///	Warning
                ///	This function is deprecated in favor of torch.nn.functional.interpolate().<br></br>
                ///	
                ///	This is equivalent with
                ///	nn.functional.interpolate(..., mode='bilinear', align_corners=True).<br></br>
                ///	
                ///	Expected inputs are spatial (4 dimensional).<br></br>
                ///	 Use upsample_trilinear fo
                ///	volumetric (5 dimensional) inputs.
                /// </summary>
                /// <param name="input">
                ///	input
                /// </param>
                /// <param name="size">
                ///	output spatial size.
                /// </param>
                /// <param name="scale_factor">
                ///	multiplier for spatial size
                /// </param>
                public static void upsample_bilinear(Tensor input, int[] size = null, float[] scale_factor = null)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                    });
                    var kwargs=new PyDict();
                    if (size!=null) kwargs["size"]=ToPython(size);
                    if (scale_factor!=null) kwargs["scale_factor"]=ToPython(scale_factor);
                    dynamic py = __self__.InvokeMethod("upsample_bilinear", pyargs, kwargs);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Given an input and a flow-field grid, computes the
                ///	output using input values and pixel locations from grid.<br></br>
                ///	
                ///	Currently, only spatial (4-D) and volumetric (5-D) input are
                ///	supported.<br></br>
                ///	
                ///	In the spatial (4-D) case, for input with shape
                ///	\((N, C, H_\text{in}, W_\text{in})\) and grid with shape
                ///	\((N, H_\text{out}, W_\text{out}, 2)\), the output will have shape
                ///	\((N, C, H_\text{out}, W_\text{out})\).<br></br>
                ///	
                ///	For each output location output[n, :, h, w], the size-2 vector
                ///	grid[n, h, w] specifies input pixel locations x and y,
                ///	which are used to interpolate the output value output[n, :, h, w].<br></br>
                ///	
                ///	In the case of 5D inputs, grid[n, d, h, w] specifies the
                ///	x, y, z pixel locations for interpolating
                ///	output[n, :, d, h, w].<br></br>
                ///	 mode argument specifies nearest or
                ///	bilinear interpolation method to sample the input pixels.<br></br>
                ///	
                ///	grid specifies the sampling pixel locations normalized by the
                ///	input spatial dimensions.<br></br>
                ///	 Therefore, it should have most values in
                ///	the range of [-1, 1].<br></br>
                ///	 For example, values x = -1, y = -1 is the
                ///	left-top pixel of input, and values  x = 1, y = 1 is the
                ///	right-bottom pixel of input.<br></br>
                ///	
                ///	If grid has values outside the range of [-1, 1], the corresponding
                ///	outputs are handled as defined by padding_mode.<br></br>
                ///	 Options are
                ///	
                ///	padding_mode=&quot;zeros&quot;: use 0 for out-of-bound grid locations,
                ///	padding_mode=&quot;border&quot;: use border values for out-of-bound grid locations,
                ///	padding_mode=&quot;reflection&quot;: use values at locations reflected by
                ///	the border for out-of-bound grid locations.<br></br>
                ///	 For location far away
                ///	from the border, it will keep being reflected until becoming in bound,
                ///	e.g., (normalized) pixel location x = -3.5 reflects by border -1
                ///	and becomes x' = 1.5, then reflects by border 1 and becomes
                ///	x'' = -0.5.
                ///	
                ///	Note
                ///	This function is often used in building Spatial Transformer Networks .
                ///	
                ///	Note
                ///	When using the CUDA backend, this operation may induce nondeterministic
                ///	behaviour in be backward that is not easily switched off.<br></br>
                ///	
                ///	Please see the notes on Reproducibility for background.
                /// </summary>
                /// <param name="input">
                ///	input of shape \((N, C, H_\text{in}, W_\text{in})\) (4-D case)
                ///	or \((N, C, D_\text{in}, H_\text{in}, W_\text{in})\) (5-D case)
                /// </param>
                /// <param name="grid">
                ///	flow-field of shape \((N, H_\text{out}, W_\text{out}, 2)\) (4-D case)
                ///	or \((N, D_\text{out}, H_\text{out}, W_\text{out}, 3)\) (5-D case)
                /// </param>
                /// <param name="mode">
                ///	interpolation mode to calculate output values
                ///	'bilinear' | 'nearest'. Default: 'bilinear'
                /// </param>
                /// <param name="padding_mode">
                ///	padding mode for outside grid values
                ///	'zeros' | 'border' | 'reflection'. Default: 'zeros'
                /// </param>
                public static Tensor grid_sample(Tensor input, Tensor grid, string mode = "bilinear", string padding_mode = "zeros")
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        input,
                        grid,
                    });
                    var kwargs=new PyDict();
                    if (mode!="bilinear") kwargs["mode"]=ToPython(mode);
                    if (padding_mode!="zeros") kwargs["padding_mode"]=ToPython(padding_mode);
                    dynamic py = __self__.InvokeMethod("grid_sample", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class functional {
                /// <summary>
                ///	Generates a 2d flow field, given a batch of affine matrices theta.<br></br>
                ///	
                ///	Generally used in conjunction with grid_sample() to
                ///	implement Spatial Transformer Networks.
                /// </summary>
                /// <param name="theta">
                ///	input batch of affine matrices (\(N \times 2 \times 3\))
                /// </param>
                /// <param name="size">
                ///	the target output image size (\(N \times C \times H \times W\)).<br></br>
                ///	
                ///	Example: torch.Size((32, 3, 24, 24))
                /// </param>
                public static Tensor affine_grid(Tensor theta, Shape size)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var functional = nn.GetAttr("functional");
                    var __self__=functional;
                    var pyargs=ToTuple(new object[]
                    {
                        theta,
                        size,
                    });
                    var kwargs=new PyDict();
                    dynamic py = __self__.InvokeMethod("affine_grid", pyargs, kwargs);
                    return ToCsharp<Tensor>(py);
                }
            }
        }
        
        public static partial class nn {
            public static partial class parallel {
                /// <summary>
                ///	Evaluates module(input) in parallel across the GPUs given in device_ids.<br></br>
                ///	
                ///	This is the functional version of the DataParallel module.
                /// </summary>
                /// <param name="module">
                ///	the module to evaluate in parallel
                /// </param>
                /// <param name="inputs">
                ///	inputs to the module
                /// </param>
                /// <param name="device_ids">
                ///	int or torch.device) : GPU ids on which to replicate module
                /// </param>
                /// <param name="output_device">
                ///	int or torch.device) : GPU location of the output  Use -1 to indicate the CPU.<br></br>
                ///	
                ///	(default: device_ids[0])
                /// </param>
                public static void data_parallel(Module module, Tensor inputs, int[] device_ids = null, int[] output_device = null)
                {
                    //auto-generated code, do not change
                    var nn = self.GetAttr("nn");
                    var parallel = nn.GetAttr("parallel");
                    var __self__=parallel;
                    var pyargs=ToTuple(new object[]
                    {
                        module,
                        inputs,
                    });
                    var kwargs=new PyDict();
                    if (device_ids!=null) kwargs["device_ids"]=ToPython(device_ids);
                    if (output_device!=null) kwargs["output_device"]=ToPython(output_device);
                    dynamic py = __self__.InvokeMethod("data_parallel", pyargs, kwargs);
                }
            }
        }
        
        
    }
}
