[
    {
        "Classes": [
            {
                "Abstract": false,
                "Args": [
                    "self"
                ],
                "DocStr": "Abstract base layer class.\n\n# Properties\n    input, output: Input/output tensor(s). Note that if the layer\n        is used more than once (shared layer), this is ill-defined\n        and will raise an exception. In such cases, use\n        `layer.get_input_at(node_index)`.\n    input_mask, output_mask: Mask tensors. Same caveats apply as\n        input, output.\n    input_shape: Shape tuple. Provided for convenience, but note\n        that there may be cases in which this attribute is\n        ill-defined (e.g. a shared layer with multiple input\n        shapes), in which case requesting `input_shape` will raise\n        an Exception. Prefer using\n        `layer.get_input_shape_at(node_index)`.\n    input_spec: List of InputSpec class instances\n        each entry describes one required input:\n            - ndim\n            - dtype\n        A layer with `n` input tensors must have\n        an `input_spec` of length `n`.\n    name: String, must be unique within a model.\n    non_trainable_weights: List of variables.\n    output_shape: Shape tuple. See `input_shape`.\n    stateful: Boolean indicating whether the layer carries\n        additional non-weight state. Used in, for instance, RNN\n        cells to carry information between batches.\n    supports_masking: Boolean indicator of whether the layer\n        supports masking, typically for unused timesteps in a\n        sequence.\n    trainable: Boolean, whether the layer weights\n        will be updated during training.\n    trainable_weights: List of variables.\n    uses_learning_phase: Whether any operation\n        of the layer uses `K.in_training_phase()`\n        or `K.in_test_phase()`.\n    weights: The concatenation of the lists trainable_weights and\n        non_trainable_weights (in this order).\n\n\n# Methods\n    call(x, mask=None): Where the layer's logic lives.\n    __call__(x, mask=None): Wrapper around the layer logic (`call`).\n        If x is a Keras tensor:\n            - Connect current layer with last layer from tensor:\n                `self._add_inbound_node(last_layer)`\n            - Add layer to tensor history\n        If layer is not built:\n            - Build from x._keras_shape\n    compute_mask(x, mask)\n    compute_output_shape(input_shape)\n    count_params()\n    get_config()\n    get_input_at(node_index)\n    get_input_mask_at(node_index)\n    get_input_shape_at(node_index)\n    get_output_at(node_index)\n    get_output_mask_at(node_index)\n    get_output_shape_at(node_index)\n    get_weights()\n    set_weights(weights)\n\n# Class Methods\n    from_config(config)\n\n# Internal methods:\n    _add_inbound_node(layer, index=0)\n    assert_input_compatibility()\n    build(input_shape)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Layer",
                "Type": "Class"
            }
        ],
        "Functions": [
            {
                "Args": [
                    "name",
                    "custom_objects"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": null,
                "Name": "deserialize"
            },
            {
                "Args": [
                    "identifier",
                    "module_objects",
                    "custom_objects",
                    "printable_module_name"
                ],
                "Defaults": [
                    "None",
                    " None",
                    " 'object'"
                ],
                "DocStr": null,
                "Name": "deserialize_keras_object"
            },
            {
                "Args": [
                    "x",
                    "alpha"
                ],
                "Defaults": [
                    "1.0"
                ],
                "DocStr": "Exponential linear unit.\n\n# Arguments\n    x: Input tensor.\n    alpha: A scalar, slope of negative section.\n\n# Returns\n    The exponential linear activation: `x` if `x > 0` and\n    `alpha * (exp(x)-1)` if `x < 0`.\n\n# References\n    - [Fast and Accurate Deep Network Learning by Exponential\n    Linear Units (ELUs)](https://arxiv.org/abs/1511.07289)",
                "Name": "elu"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Exponential (base e) activation function.\n    ",
                "Name": "exponential"
            },
            {
                "Args": [
                    "identifier"
                ],
                "DocStr": "Get the `identifier` activation function.\n\n# Arguments\n    identifier: None or str, name of the function.\n\n# Returns\n    The activation function, `linear` if `identifier` is None.\n\n# Raises\n    ValueError if unknown identifier",
                "Name": "get"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Hard sigmoid activation function.\n\nFaster to compute than sigmoid activation.\n\n# Arguments\n    x: Input tensor.\n\n# Returns\n    Hard sigmoid activation:\n\n    - `0` if `x < -2.5`\n    - `1` if `x > 2.5`\n    - `0.2 * x + 0.5` if `-2.5 <= x <= 2.5`.",
                "Name": "hard_sigmoid"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Linear (i.e. identity) activation function.\n    ",
                "Name": "linear"
            },
            {
                "Args": [
                    "x",
                    "alpha",
                    "max_value",
                    "threshold"
                ],
                "Defaults": [
                    "0.0",
                    " None",
                    " 0.0"
                ],
                "DocStr": "Rectified Linear Unit.\n\nWith default values, it returns element-wise `max(x, 0)`.\n\nOtherwise, it follows:\n`f(x) = max_value` for `x >= max_value`,\n`f(x) = x` for `threshold <= x < max_value`,\n`f(x) = alpha * (x - threshold)` otherwise.\n\n# Arguments\n    x: Input tensor.\n    alpha: float. Slope of the negative part. Defaults to zero.\n    max_value: float. Saturation threshold.\n    threshold: float. Threshold value for thresholded activation.\n\n# Returns\n    A tensor.",
                "Name": "relu"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Scaled Exponential Linear Unit (SELU).\n\nSELU is equal to: `scale * elu(x, alpha)`, where alpha and scale\nare pre-defined constants. The values of `alpha` and `scale` are\nchosen so that the mean and variance of the inputs are preserved\nbetween two consecutive layers as long as the weights are initialized\ncorrectly (see `lecun_normal` initialization) and the number of inputs\nis \"large enough\" (see references for more information).\n\n# Arguments\n    x: A tensor or variable to compute the activation function for.\n\n# Returns\n   The scaled exponential unit activation: `scale * elu(x, alpha)`.\n\n# Note\n    - To be used together with the initialization \"lecun_normal\".\n    - To be used together with the dropout variant \"AlphaDropout\".\n\n# References\n    - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)",
                "Name": "selu"
            },
            {
                "Args": [
                    "activation"
                ],
                "DocStr": null,
                "Name": "serialize"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Sigmoid activation function.\n    ",
                "Name": "sigmoid"
            },
            {
                "Args": [
                    "x",
                    "axis"
                ],
                "Defaults": [
                    "-1"
                ],
                "DocStr": "Softmax activation function.\n\n# Arguments\n    x: Input tensor.\n    axis: Integer, axis along which the softmax normalization is applied.\n\n# Returns\n    Tensor, output of softmax transformation.\n\n# Raises\n    ValueError: In case `dim(x) == 1`.",
                "Name": "softmax"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Softplus activation function.\n\n# Arguments\n    x: Input tensor.\n\n# Returns\n    The softplus activation: `log(exp(x) + 1)`.",
                "Name": "softplus"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Softsign activation function.\n\n# Arguments\n    x: Input tensor.\n\n# Returns\n    The softplus activation: `x / (abs(x) + 1)`.",
                "Name": "softsign"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Hyperbolic tangent activation function.\n    ",
                "Name": "tanh"
            }
        ],
        "Name": "activations",
        "Type": "Module"
    },
    {
        "Classes": [],
        "Functions": [
            {
                "Args": [],
                "DocStr": null,
                "Name": "DenseNet121"
            },
            {
                "Args": [],
                "DocStr": null,
                "Name": "DenseNet169"
            },
            {
                "Args": [],
                "DocStr": null,
                "Name": "DenseNet201"
            },
            {
                "Args": [],
                "DocStr": null,
                "Name": "InceptionResNetV2"
            },
            {
                "Args": [],
                "DocStr": null,
                "Name": "InceptionV3"
            },
            {
                "Args": [],
                "DocStr": null,
                "Name": "MobileNet"
            },
            {
                "Args": [],
                "DocStr": null,
                "Name": "MobileNetV2"
            },
            {
                "Args": [],
                "DocStr": null,
                "Name": "NASNetLarge"
            },
            {
                "Args": [],
                "DocStr": null,
                "Name": "NASNetMobile"
            },
            {
                "Args": [],
                "DocStr": null,
                "Name": "ResNet50"
            },
            {
                "Args": [],
                "DocStr": null,
                "Name": "VGG16"
            },
            {
                "Args": [],
                "DocStr": null,
                "Name": "VGG19"
            },
            {
                "Args": [],
                "DocStr": null,
                "Name": "Xception"
            },
            {
                "Args": [
                    "base_fun"
                ],
                "DocStr": null,
                "Name": "keras_modules_injection"
            }
        ],
        "Name": "applications",
        "Type": "Module"
    },
    {
        "Classes": [
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "inputs",
                    "outputs",
                    "updates",
                    "name"
                ],
                "Defaults": [
                    "None",
                    " None"
                ],
                "DocStr": "Runs a computation graph.\n\nIt's possible to pass arguments to `tf.Session.run()` via `session_kwargs`.\nIn particular additional operations via `fetches` argument and additional\ntensor substitutions via `feed_dict` arguments. Note that given\nsubstitutions are merged with substitutions from `inputs`. Even though\n`feed_dict` is passed once in the constructor (called in `model.compile()`)\nwe can modify the values in the dictionary. Through this feed_dict we can\nprovide additional substitutions besides Keras inputs.\n\n# Arguments\n    inputs: Feed placeholders to the computation graph.\n    outputs: Output tensors to fetch.\n    updates: Additional update ops to be run at function call.\n    name: a name to help users identify what this function does.\n    session_kwargs: arguments to `tf.Session.run()`:\n        `fetches`, `feed_dict`,\n        `options`, `run_metadata`",
                "Functions": [],
                "Name": "Function",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "vstring"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": "Version numbering for anal retentives and software idealists.\nImplements the standard interface for version number classes as\ndescribed above.  A version number consists of two or three\ndot-separated numeric components, with an optional \"pre-release\" tag\non the end.  The pre-release tag consists of the letter 'a' or 'b'\nfollowed by a number.  If the numeric components of two version\nnumbers are equal, then one with a pre-release tag will always\nbe deemed earlier (lesser) than one without.\n\nThe following are valid version numbers (shown in the order that\nwould be obtained by sorting according to the supplied cmp function):\n\n    0.4       0.4.0  (these two are equivalent)\n    0.4.1\n    0.5a1\n    0.5b3\n    0.5\n    0.9.6\n    1.0\n    1.0.4a3\n    1.0.4b1\n    1.0.4\n\nThe following are examples of invalid version numbers:\n\n    1\n    2.7.2.2\n    1.3.a4\n    1.3pl1\n    1.3c4\n\nThe rationale for this version numbering system will be explained\nin the distutils documentation.",
                "Functions": [],
                "Name": "StrictVersion",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "DocStr": "defaultdict(default_factory[, ...]) --> dict with default factory\n\nThe default factory is called without arguments to produce\na new value when a key is not present, in __getitem__ only.\nA defaultdict compares equal to a dict with the same items.\nAll remaining arguments are treated the same as if they were\npassed to the dict constructor, including keyword arguments.",
                "Functions": [],
                "Name": "defaultdict",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "name",
                    "default_name",
                    "values"
                ],
                "Defaults": [
                    "None",
                    " None"
                ],
                "DocStr": "A context manager for use when defining a Python op.\n\nThis context manager validates that the given `values` are from the\nsame graph, makes that graph the default graph, and pushes a\nname scope in that graph (see\n`tf.Graph.name_scope`\nfor more details on that).\n\nFor example, to define a new Python op called `my_op`:\n\n```python\ndef my_op(a, b, c, name=None):\n  with tf.name_scope(name, \"MyOp\", [a, b, c]) as scope:\n    a = tf.convert_to_tensor(a, name=\"a\")\n    b = tf.convert_to_tensor(b, name=\"b\")\n    c = tf.convert_to_tensor(c, name=\"c\")\n    # Define some computation that uses `a`, `b`, and `c`.\n    return foo_op(..., name=scope)\n```",
                "Functions": [],
                "Name": "name_scope",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "DocStr": "slice(stop)\nslice(start, stop[, step])\n\nCreate a slice object.  This is used for extended slicing (e.g. a[0:10:2]).",
                "Functions": [],
                "Name": "py_slice",
                "Type": "Class"
            }
        ],
        "Functions": [
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Element-wise absolute value.\n\n# Arguments\n    x: Tensor or variable.\n\n# Returns\n    A tensor.",
                "Name": "abs"
            },
            {
                "Args": [
                    "x",
                    "axis",
                    "keepdims"
                ],
                "Defaults": [
                    "None",
                    " False"
                ],
                "DocStr": "Bitwise reduction (logical AND).\n\n# Arguments\n    x: Tensor or variable.\n    axis: An integer or list of integers in [-rank(x), rank(x)),\n        the axes to compute the logical and. If `None` (default), computes\n        the logical and over all dimensions.\n    keepdims: whether the drop or broadcast the reduction axes.\n\n# Returns\n    A uint8 tensor (0s and 1s).",
                "Name": "all"
            },
            {
                "Args": [
                    "x",
                    "axis",
                    "keepdims"
                ],
                "Defaults": [
                    "None",
                    " False"
                ],
                "DocStr": "Bitwise reduction (logical OR).\n\n# Arguments\n    x: Tensor or variable.\n    axis: An integer or list of integers in [-rank(x), rank(x)),\n        the axes to compute the logical or. If `None` (default), computes\n        the logical or over all dimensions.\n    keepdims: whether the drop or broadcast the reduction axes.\n\n# Returns\n    A uint8 tensor (0s and 1s).",
                "Name": "any"
            },
            {
                "Args": [
                    "start",
                    "stop",
                    "step",
                    "dtype"
                ],
                "Defaults": [
                    "None",
                    " 1",
                    " 'int32'"
                ],
                "DocStr": "Creates a 1D tensor containing a sequence of integers.\n\nThe function arguments use the same convention as\nTheano's arange: if only one argument is provided,\nit is in fact the \"stop\" argument and \"start\" is 0.\n\nThe default type of the returned tensor is `'int32'` to\nmatch TensorFlow's default.\n\n# Arguments\n    start: Start value.\n    stop: Stop value.\n    step: Difference between two successive values.\n    dtype: Integer dtype to use.\n\n# Returns\n    An integer tensor.",
                "Name": "arange"
            },
            {
                "Args": [
                    "x",
                    "axis"
                ],
                "Defaults": [
                    "-1"
                ],
                "DocStr": "Returns the index of the maximum value along an axis.\n\n# Arguments\n    x: Tensor or variable.\n    axis: axis along which to perform the reduction.\n\n# Returns\n    A tensor.",
                "Name": "argmax"
            },
            {
                "Args": [
                    "x",
                    "axis"
                ],
                "Defaults": [
                    "-1"
                ],
                "DocStr": "Returns the index of the minimum value along an axis.\n\n# Arguments\n    x: Tensor or variable.\n    axis: axis along which to perform the reduction.\n\n# Returns\n    A tensor.",
                "Name": "argmin"
            },
            {
                "Args": [],
                "DocStr": "Publicly accessible method\nfor determining the current backend.\n\n# Returns\n    String, the name of the backend Keras is currently using.\n\n# Example\n```python\n    >>> keras.backend.backend()\n    'tensorflow'\n```",
                "Name": "backend"
            },
            {
                "Args": [
                    "x",
                    "y",
                    "axes"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": "Batchwise dot product.\n\n`batch_dot` is used to compute dot product of `x` and `y` when\n`x` and `y` are data in batch, i.e. in a shape of\n`(batch_size, :)`.\n`batch_dot` results in a tensor or variable with less dimensions\nthan the input. If the number of dimensions is reduced to 1,\nwe use `expand_dims` to make sure that ndim is at least 2.\n\n# Arguments\n    x: Keras tensor or variable with `ndim >= 2`.\n    y: Keras tensor or variable with `ndim >= 2`.\n    axes: list of (or single) int with target dimensions.\n        The lengths of `axes[0]` and `axes[1]` should be the same.\n\n# Returns\n    A tensor with shape equal to the concatenation of `x`'s shape\n    (less the dimension that was summed over) and `y`'s shape\n    (less the batch dimension and the dimension that was summed over).\n    If the final rank is 1, we reshape it to `(batch_size, 1)`.\n\n# Examples\n    Assume `x = [[1, 2], [3, 4]]` and `y = [[5, 6], [7, 8]]`\n    `batch_dot(x, y, axes=1) = [[17], [53]]` which is the main diagonal\n    of `x.dot(y.T)`, although we never have to calculate the off-diagonal\n    elements.\n\n    Shape inference:\n    Let `x`'s shape be `(100, 20)` and `y`'s shape be `(100, 30, 20)`.\n    If `axes` is (1, 2), to find the output shape of resultant tensor,\n        loop through each dimension in `x`'s shape and `y`'s shape:\n\n    * `x.shape[0]` : 100 : append to output shape\n    * `x.shape[1]` : 20 : do not append to output shape,\n        dimension 1 of `x` has been summed over. (`dot_axes[0]` = 1)\n    * `y.shape[0]` : 100 : do not append to output shape,\n        always ignore first dimension of `y`\n    * `y.shape[1]` : 30 : append to output shape\n    * `y.shape[2]` : 20 : do not append to output shape,\n        dimension 2 of `y` has been summed over. (`dot_axes[1]` = 2)\n    `output_shape` = `(100, 30)`\n\n```python\n    >>> x_batch = K.ones(shape=(32, 20, 1))\n    >>> y_batch = K.ones(shape=(32, 30, 20))\n    >>> xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=[1, 2])\n    >>> K.int_shape(xy_batch_dot)\n    (32, 1, 30)\n```",
                "Name": "batch_dot"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Turn a nD tensor into a 2D tensor with same 0th dimension.\n\nIn other words, it flattens each data samples of a batch.\n\n# Arguments\n    x: A tensor or variable.\n\n# Returns\n    A tensor.",
                "Name": "batch_flatten"
            },
            {
                "Args": [
                    "ops"
                ],
                "DocStr": "Returns the value of more than one tensor variable.\n\n# Arguments\n    ops: list of ops to run.\n\n# Returns\n    A list of Numpy arrays.",
                "Name": "batch_get_value"
            },
            {
                "Args": [
                    "x",
                    "mean",
                    "var",
                    "beta",
                    "gamma",
                    "axis",
                    "epsilon"
                ],
                "Defaults": [
                    "-1",
                    " 0.001"
                ],
                "DocStr": "Applies batch normalization on x given mean, var, beta and gamma.\n\nI.e. returns:\n`output = (x - mean) / sqrt(var + epsilon) * gamma + beta`\n\n# Arguments\n    x: Input tensor or variable.\n    mean: Mean of batch.\n    var: Variance of batch.\n    beta: Tensor with which to center the input.\n    gamma: Tensor by which to scale the input.\n    axis: Integer, the axis that should be normalized.\n        (typically the features axis).\n    epsilon: Fuzz factor.\n\n# Returns\n    A tensor.",
                "Name": "batch_normalization"
            },
            {
                "Args": [
                    "tuples"
                ],
                "DocStr": "Sets the values of many tensor variables at once.\n\n# Arguments\n    tuples: a list of tuples `(tensor, value)`.\n        `value` should be a Numpy array.",
                "Name": "batch_set_value"
            },
            {
                "Args": [
                    "x",
                    "bias",
                    "data_format"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": "Adds a bias vector to a tensor.\n\n# Arguments\n    x: Tensor or variable.\n    bias: Bias tensor to add.\n    data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n\n# Returns\n    Output tensor.\n\n# Raises\n    ValueError: In one of the two cases below:\n                1. invalid `data_format` argument.\n                2. invalid bias shape.\n                   the bias should be either a vector or\n                   a tensor with ndim(x) - 1 dimension",
                "Name": "bias_add"
            },
            {
                "Args": [
                    "target",
                    "output",
                    "from_logits"
                ],
                "Defaults": [
                    "False"
                ],
                "DocStr": "Binary crossentropy between an output tensor and a target tensor.\n\n# Arguments\n    target: A tensor with the same shape as `output`.\n    output: A tensor.\n    from_logits: Whether `output` is expected to be a logits tensor.\n        By default, we consider that `output`\n        encodes a probability distribution.\n\n# Returns\n    A tensor.",
                "Name": "binary_crossentropy"
            },
            {
                "Args": [
                    "x",
                    "dtype"
                ],
                "DocStr": "Casts a tensor to a different dtype and returns it.\n\nYou can cast a Keras variable but it still returns a Keras tensor.\n\n# Arguments\n    x: Keras tensor (or variable).\n    dtype: String, either (`'float16'`, `'float32'`, or `'float64'`).\n\n# Returns\n    Keras tensor with dtype `dtype`.\n\n# Example\n```python\n    >>> from keras import backend as K\n    >>> input = K.placeholder((2, 3), dtype='float32')\n    >>> input\n    <tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32>\n    # It doesn't work in-place as below.\n    >>> K.cast(input, dtype='float16')\n    <tf.Tensor 'Cast_1:0' shape=(2, 3) dtype=float16>\n    >>> input\n    <tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32>\n    # you need to assign it.\n    >>> input = K.cast(input, dtype='float16')\n    >>> input\n    <tf.Tensor 'Cast_2:0' shape=(2, 3) dtype=float16>\n```",
                "Name": "cast"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Cast a Numpy array to the default Keras float type.\n\n# Arguments\n    x: Numpy array.\n\n# Returns\n    The same Numpy array, cast to its new type.\n\n# Example\n```python\n    >>> from keras import backend as K\n    >>> K.floatx()\n    'float32'\n    >>> arr = numpy.array([1.0, 2.0], dtype='float64')\n    >>> arr.dtype\n    dtype('float64')\n    >>> new_arr = K.cast_to_floatx(arr)\n    >>> new_arr\n    array([ 1.,  2.], dtype=float32)\n    >>> new_arr.dtype\n    dtype('float32')\n```",
                "Name": "cast_to_floatx"
            },
            {
                "Args": [
                    "target",
                    "output",
                    "from_logits",
                    "axis"
                ],
                "Defaults": [
                    "False",
                    " -1"
                ],
                "DocStr": "Categorical crossentropy between an output tensor and a target tensor.\n\n# Arguments\n    target: A tensor of the same shape as `output`.\n    output: A tensor resulting from a softmax\n        (unless `from_logits` is True, in which\n        case `output` is expected to be the logits).\n    from_logits: Boolean, whether `output` is the\n        result of a softmax, or is a tensor of logits.\n    axis: Int specifying the channels axis. `axis=-1`\n        corresponds to data format `channels_last`,\n        and `axis=1` corresponds to data format\n        `channels_first`.\n\n# Returns\n    Output tensor.\n\n# Raises\n    ValueError: if `axis` is neither -1 nor one of\n        the axes of `output`.",
                "Name": "categorical_crossentropy"
            },
            {
                "Args": [],
                "DocStr": "Destroys the current TF graph and creates a new one.\n\nUseful to avoid clutter from old models / layers.",
                "Name": "clear_session"
            },
            {
                "Args": [
                    "x",
                    "min_value",
                    "max_value"
                ],
                "DocStr": "Element-wise value clipping.\n\n# Arguments\n    x: Tensor or variable.\n    min_value: Python float or integer.\n    max_value: Python float or integer.\n\n# Returns\n    A tensor.",
                "Name": "clip"
            },
            {
                "Args": [
                    "tensors",
                    "axis"
                ],
                "Defaults": [
                    "-1"
                ],
                "DocStr": "Concatenates a list of tensors alongside the specified axis.\n\n# Arguments\n    tensors: list of tensors to concatenate.\n    axis: concatenation axis.\n\n# Returns\n    A tensor.",
                "Name": "concatenate"
            },
            {
                "Args": [
                    "value",
                    "dtype",
                    "shape",
                    "name"
                ],
                "Defaults": [
                    "None",
                    " None",
                    " None"
                ],
                "DocStr": "Creates a constant tensor.\n\n# Arguments\n    value: A constant value (or list)\n    dtype: The type of the elements of the resulting tensor.\n    shape: Optional dimensions of resulting tensor.\n    name: Optional name for the tensor.\n\n# Returns\n    A Constant Tensor.",
                "Name": "constant"
            },
            {
                "Args": [
                    "x",
                    "kernel",
                    "strides",
                    "padding",
                    "data_format",
                    "dilation_rate"
                ],
                "Defaults": [
                    "1",
                    " 'valid'",
                    " None",
                    " 1"
                ],
                "DocStr": "1D convolution.\n\n# Arguments\n    x: Tensor or variable.\n    kernel: kernel tensor.\n    strides: stride integer.\n    padding: string, `\"same\"`, `\"causal\"` or `\"valid\"`.\n    data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n    dilation_rate: integer dilate rate.\n\n# Returns\n    A tensor, result of 1D convolution.\n\n# Raises\n    ValueError: If `data_format` is neither\n        `\"channels_last\"` nor `\"channels_first\"`.",
                "Name": "conv1d"
            },
            {
                "Args": [
                    "x",
                    "kernel",
                    "strides",
                    "padding",
                    "data_format",
                    "dilation_rate"
                ],
                "Defaults": [
                    "1",
                    " 1",
                    " 'valid'",
                    " None",
                    " 1",
                    " 1"
                ],
                "DocStr": "2D convolution.\n\n# Arguments\n    x: Tensor or variable.\n    kernel: kernel tensor.\n    strides: strides tuple.\n    padding: string, `\"same\"` or `\"valid\"`.\n    data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n        Whether to use Theano or TensorFlow/CNTK data format\n        for inputs/kernels/outputs.\n    dilation_rate: tuple of 2 integers.\n\n# Returns\n    A tensor, result of 2D convolution.\n\n# Raises\n    ValueError: If `data_format` is neither\n        `\"channels_last\"` nor `\"channels_first\"`.",
                "Name": "conv2d"
            },
            {
                "Args": [
                    "x",
                    "kernel",
                    "output_shape",
                    "strides",
                    "padding",
                    "data_format",
                    "dilation_rate"
                ],
                "Defaults": [
                    "1",
                    " 1",
                    " 'valid'",
                    " None",
                    " 1",
                    " 1"
                ],
                "DocStr": "2D deconvolution (i.e. transposed convolution).\n\n# Arguments\n    x: Tensor or variable.\n    kernel: kernel tensor.\n    output_shape: 1D int tensor for the output shape.\n    strides: strides tuple.\n    padding: string, `\"same\"` or `\"valid\"`.\n    data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n        Whether to use Theano or TensorFlow/CNTK data format\n        for inputs/kernels/outputs.\n    dilation_rate: tuple of 2 integers.\n\n# Returns\n    A tensor, result of transposed 2D convolution.\n\n# Raises\n    ValueError: If `data_format` is neither\n        `\"channels_last\"` nor `\"channels_first\"`.",
                "Name": "conv2d_transpose"
            },
            {
                "Args": [
                    "x",
                    "kernel",
                    "strides",
                    "padding",
                    "data_format",
                    "dilation_rate"
                ],
                "Defaults": [
                    "1",
                    " 1",
                    " 1",
                    " 'valid'",
                    " None",
                    " 1",
                    " 1",
                    " 1"
                ],
                "DocStr": "3D convolution.\n\n# Arguments\n    x: Tensor or variable.\n    kernel: kernel tensor.\n    strides: strides tuple.\n    padding: string, `\"same\"` or `\"valid\"`.\n    data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n        Whether to use Theano or TensorFlow/CNTK data format\n        for inputs/kernels/outputs.\n    dilation_rate: tuple of 3 integers.\n\n# Returns\n    A tensor, result of 3D convolution.\n\n# Raises\n    ValueError: If `data_format` is neither\n        `\"channels_last\"` nor `\"channels_first\"`.",
                "Name": "conv3d"
            },
            {
                "Args": [
                    "x",
                    "kernel",
                    "output_shape",
                    "strides",
                    "padding",
                    "data_format"
                ],
                "Defaults": [
                    "1",
                    " 1",
                    " 1",
                    " 'valid'",
                    " None"
                ],
                "DocStr": "3D deconvolution (i.e. transposed convolution).\n\n# Arguments\n    x: input tensor.\n    kernel: kernel tensor.\n    output_shape: 1D int tensor for the output shape.\n    strides: strides tuple.\n    padding: string, \"same\" or \"valid\".\n    data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n        Whether to use Theano or TensorFlow/CNTK data format\n        for inputs/kernels/outputs.\n\n# Returns\n    A tensor, result of transposed 3D convolution.\n\n# Raises\n    ValueError: If `data_format` is neither\n        `\"channels_last\"` nor `\"channels_first\"`.",
                "Name": "conv3d_transpose"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Computes cos of x element-wise.\n\n# Arguments\n    x: Tensor or variable.\n\n# Returns\n    A tensor.",
                "Name": "cos"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Returns the static number of elements in a Keras variable or tensor.\n\n# Arguments\n    x: Keras variable or tensor.\n\n# Returns\n    Integer, the number of elements in `x`, i.e., the product of the\n    array's static dimensions.\n\n# Example\n```python\n    >>> kvar = K.zeros((2,3))\n    >>> K.count_params(kvar)\n    6\n    >>> K.eval(kvar)\n    array([[ 0.,  0.,  0.],\n           [ 0.,  0.,  0.]], dtype=float32)\n```",
                "Name": "count_params"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred",
                    "input_length",
                    "label_length"
                ],
                "DocStr": "Runs CTC loss algorithm on each batch element.\n\n# Arguments\n    y_true: tensor `(samples, max_string_length)`\n        containing the truth labels.\n    y_pred: tensor `(samples, time_steps, num_categories)`\n        containing the prediction, or output of the softmax.\n    input_length: tensor `(samples, 1)` containing the sequence length for\n        each batch item in `y_pred`.\n    label_length: tensor `(samples, 1)` containing the sequence length for\n        each batch item in `y_true`.\n\n# Returns\n    Tensor with shape (samples,1) containing the\n        CTC loss of each element.",
                "Name": "ctc_batch_cost"
            },
            {
                "Args": [
                    "y_pred",
                    "input_length",
                    "greedy",
                    "beam_width",
                    "top_paths"
                ],
                "Defaults": [
                    "True",
                    " 100",
                    " 1"
                ],
                "DocStr": "Decodes the output of a softmax.\n\nCan use either greedy search (also known as best path)\nor a constrained dictionary search.\n\n# Arguments\n    y_pred: tensor `(samples, time_steps, num_categories)`\n        containing the prediction, or output of the softmax.\n    input_length: tensor `(samples, )` containing the sequence length for\n        each batch item in `y_pred`.\n    greedy: perform much faster best-path search if `true`.\n        This does not use a dictionary.\n    beam_width: if `greedy` is `false`: a beam search decoder will be used\n        with a beam of this width.\n    top_paths: if `greedy` is `false`,\n        how many of the most probable paths will be returned.\n\n# Returns\n    Tuple:\n        List: if `greedy` is `true`, returns a list of one element that\n            contains the decoded sequence.\n            If `false`, returns the `top_paths` most probable\n            decoded sequences.\n            Important: blank labels are returned as `-1`.\n        Tensor `(top_paths, )` that contains\n            the log probability of each decoded sequence.",
                "Name": "ctc_decode"
            },
            {
                "Args": [
                    "labels",
                    "label_lengths"
                ],
                "DocStr": "Converts CTC labels from dense to sparse.\n\n# Arguments\n    labels: dense CTC labels.\n    label_lengths: length of the labels.\n\n# Returns\n    A sparse tensor representation of the labels.",
                "Name": "ctc_label_dense_to_sparse"
            },
            {
                "Args": [
                    "x",
                    "axis"
                ],
                "Defaults": [
                    "0"
                ],
                "DocStr": "Cumulative product of the values in a tensor, alongside the specified axis.\n\n# Arguments\n    x: A tensor or variable.\n    axis: An integer, the axis to compute the product.\n\n# Returns\n    A tensor of the cumulative product of values of `x` along `axis`.",
                "Name": "cumprod"
            },
            {
                "Args": [
                    "x",
                    "axis"
                ],
                "Defaults": [
                    "0"
                ],
                "DocStr": "Cumulative sum of the values in a tensor, alongside the specified axis.\n\n# Arguments\n    x: A tensor or variable.\n    axis: An integer, the axis to compute the sum.\n\n# Returns\n    A tensor of the cumulative sum of values of `x` along `axis`.",
                "Name": "cumsum"
            },
            {
                "Args": [
                    "x",
                    "depthwise_kernel",
                    "strides",
                    "padding",
                    "data_format",
                    "dilation_rate"
                ],
                "Defaults": [
                    "1",
                    " 1",
                    " 'valid'",
                    " None",
                    " 1",
                    " 1"
                ],
                "DocStr": "2D convolution with separable filters.\n\n# Arguments\n    x: input tensor\n    depthwise_kernel: convolution kernel for the depthwise convolution.\n    strides: strides tuple (length 2).\n    padding: string, `\"same\"` or `\"valid\"`.\n    data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n    dilation_rate: tuple of integers,\n        dilation rates for the separable convolution.\n\n# Returns\n    Output tensor.\n\n# Raises\n    ValueError: If `data_format` is neither\n        `\"channels_last\"` nor `\"channels_first\"`.",
                "Name": "depthwise_conv2d"
            },
            {
                "Args": [
                    "x",
                    "y"
                ],
                "DocStr": "Multiplies 2 tensors (and/or variables) and returns a *tensor*.\n\nWhen attempting to multiply a nD tensor\nwith a nD tensor, it reproduces the Theano behavior.\n(e.g. `(2, 3) * (4, 3, 5) -> (2, 4, 5)`)\n\n# Arguments\n    x: Tensor or variable.\n    y: Tensor or variable.\n\n# Returns\n    A tensor, dot product of `x` and `y`.\n\n# Examples\n```python\n    # dot product between tensors\n    >>> x = K.placeholder(shape=(2, 3))\n    >>> y = K.placeholder(shape=(3, 4))\n    >>> xy = K.dot(x, y)\n    >>> xy\n    <tf.Tensor 'MatMul_9:0' shape=(2, 4) dtype=float32>\n```\n\n```python\n    # dot product between tensors\n    >>> x = K.placeholder(shape=(32, 28, 3))\n    >>> y = K.placeholder(shape=(3, 4))\n    >>> xy = K.dot(x, y)\n    >>> xy\n    <tf.Tensor 'MatMul_9:0' shape=(32, 28, 4) dtype=float32>\n```\n\n```python\n    # Theano-like behavior example\n    >>> x = K.random_uniform_variable(shape=(2, 3), low=0, high=1)\n    >>> y = K.ones((4, 3, 5))\n    >>> xy = K.dot(x, y)\n    >>> K.int_shape(xy)\n    (2, 4, 5)\n```",
                "Name": "dot"
            },
            {
                "Args": [
                    "x",
                    "level",
                    "noise_shape",
                    "seed"
                ],
                "Defaults": [
                    "None",
                    " None"
                ],
                "DocStr": "Sets entries in `x` to zero at random, while scaling the entire tensor.\n\n# Arguments\n    x: tensor\n    level: fraction of the entries in the tensor\n        that will be set to 0.\n    noise_shape: shape for randomly generated keep/drop flags,\n        must be broadcastable to the shape of `x`\n    seed: random seed to ensure determinism.\n\n# Returns\n    A tensor.",
                "Name": "dropout"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Returns the dtype of a Keras tensor or variable, as a string.\n\n# Arguments\n    x: Tensor or variable.\n\n# Returns\n    String, dtype of `x`.\n\n# Examples\n```python\n    >>> from keras import backend as K\n    >>> K.dtype(K.placeholder(shape=(2,4,5)))\n    'float32'\n    >>> K.dtype(K.placeholder(shape=(2,4,5), dtype='float32'))\n    'float32'\n    >>> K.dtype(K.placeholder(shape=(2,4,5), dtype='float64'))\n    'float64'\n    # Keras variable\n    >>> kvar = K.variable(np.array([[1, 2], [3, 4]]))\n    >>> K.dtype(kvar)\n    'float32_ref'\n    >>> kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')\n    >>> K.dtype(kvar)\n    'float32_ref'\n```",
                "Name": "dtype"
            },
            {
                "Args": [
                    "x",
                    "alpha"
                ],
                "Defaults": [
                    "1.0"
                ],
                "DocStr": "Exponential linear unit.\n\n# Arguments\n    x: A tensor or variable to compute the activation function for.\n    alpha: A scalar, slope of negative section.\n\n# Returns\n    A tensor.",
                "Name": "elu"
            },
            {
                "Args": [],
                "DocStr": "Returns the value of the fuzz factor used in numeric expressions.\n\n# Returns\n    A float.\n\n# Example\n```python\n    >>> keras.backend.epsilon()\n    1e-07\n```",
                "Name": "epsilon"
            },
            {
                "Args": [
                    "x",
                    "y"
                ],
                "DocStr": "Element-wise equality between two tensors.\n\n# Arguments\n    x: Tensor or variable.\n    y: Tensor or variable.\n\n# Returns\n    A bool tensor.",
                "Name": "equal"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Evaluates the value of a variable.\n\n# Arguments\n    x: A variable.\n\n# Returns\n    A Numpy array.\n\n# Examples\n```python\n    >>> from keras import backend as K\n    >>> kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')\n    >>> K.eval(kvar)\n    array([[ 1.,  2.],\n           [ 3.,  4.]], dtype=float32)\n```",
                "Name": "eval"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Element-wise exponential.\n\n# Arguments\n    x: Tensor or variable.\n\n# Returns\n    A tensor.",
                "Name": "exp"
            },
            {
                "Args": [
                    "x",
                    "axis"
                ],
                "Defaults": [
                    "-1"
                ],
                "DocStr": "Adds a 1-sized dimension at index \"axis\".\n\n# Arguments\n    x: A tensor or variable.\n    axis: Position where to add a new axis.\n\n# Returns\n    A tensor with expanded dimensions.",
                "Name": "expand_dims"
            },
            {
                "Args": [
                    "size",
                    "dtype",
                    "name"
                ],
                "Defaults": [
                    "None",
                    " None"
                ],
                "DocStr": "Instantiate an identity matrix and returns it.\n\n# Arguments\n    size: Integer, number of rows/columns.\n    dtype: String, data type of returned Keras variable.\n    name: String, name of returned Keras variable.\n\n# Returns\n    A Keras variable, an identity matrix.\n\n# Example\n```python\n    >>> from keras import backend as K\n    >>> kvar = K.eye(3)\n    >>> K.eval(kvar)\n    array([[ 1.,  0.,  0.],\n           [ 0.,  1.,  0.],\n           [ 0.,  0.,  1.]], dtype=float32)\n```",
                "Name": "eye"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Flatten a tensor.\n\n# Arguments\n    x: A tensor or variable.\n\n# Returns\n    A tensor, reshaped into 1-D",
                "Name": "flatten"
            },
            {
                "Args": [],
                "DocStr": "Returns the default float type, as a string.\n(e.g. 'float16', 'float32', 'float64').\n\n# Returns\n    String, the current default float type.\n\n# Example\n```python\n    >>> keras.backend.floatx()\n    'float32'\n```",
                "Name": "floatx"
            },
            {
                "Args": [
                    "fn",
                    "elems",
                    "initializer",
                    "name"
                ],
                "Defaults": [
                    "None",
                    " None"
                ],
                "DocStr": "Reduce elems using fn to combine them from left to right.\n\n# Arguments\n    fn: Callable that will be called upon each element in elems and an\n        accumulator, for instance `lambda acc, x: acc + x`\n    elems: tensor\n    initializer: The first value used (`elems[0]` in case of None)\n    name: A string name for the foldl node in the graph\n\n# Returns\n    Tensor with same type and shape as `initializer`.",
                "Name": "foldl"
            },
            {
                "Args": [
                    "fn",
                    "elems",
                    "initializer",
                    "name"
                ],
                "Defaults": [
                    "None",
                    " None"
                ],
                "DocStr": "Reduce elems using fn to combine them from right to left.\n\n# Arguments\n    fn: Callable that will be called upon each element in elems and an\n        accumulator, for instance `lambda acc, x: acc + x`\n    elems: tensor\n    initializer: The first value used (`elems[-1]` in case of None)\n    name: A string name for the foldr node in the graph\n\n# Returns\n    Tensor with same type and shape as `initializer`.",
                "Name": "foldr"
            },
            {
                "Args": [
                    "inputs",
                    "outputs",
                    "updates"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": "Instantiates a Keras function.\n\n# Arguments\n    inputs: List of placeholder tensors.\n    outputs: List of output tensors.\n    updates: List of update ops.\n    **kwargs: Passed to `tf.Session.run`.\n\n# Returns\n    Output values as Numpy arrays.\n\n# Raises\n    ValueError: if invalid kwargs are passed in.",
                "Name": "function"
            },
            {
                "Args": [
                    "reference",
                    "indices"
                ],
                "DocStr": "Retrieves the elements of indices `indices` in the tensor `reference`.\n\n# Arguments\n    reference: A tensor.\n    indices: An integer tensor of indices.\n\n# Returns\n    A tensor of same type as `reference`.",
                "Name": "gather"
            },
            {
                "Args": [],
                "DocStr": "Returns the TF session to be used by the backend.\n\nIf a default TensorFlow session is available, we will return it.\n\nElse, we will return the global Keras session.\n\nIf no global Keras session exists at this point:\nwe will create a new global session.\n\nNote that you can manually set the global session\nvia `K.set_session(sess)`.\n\n# Returns\n    A TensorFlow session.",
                "Name": "get_session"
            },
            {
                "Args": [
                    "prefix"
                ],
                "Defaults": [
                    "''"
                ],
                "DocStr": "Get the uid for the default graph.\n\n# Arguments\n    prefix: An optional prefix of the graph.\n\n# Returns\n    A unique identifier for the graph.",
                "Name": "get_uid"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Returns the value of a variable.\n\n# Arguments\n    x: input variable.\n\n# Returns\n    A Numpy array.",
                "Name": "get_value"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Returns the shape of a variable.\n\n# Arguments\n    x: A variable.\n\n# Returns\n    A tuple of integers.",
                "Name": "get_variable_shape"
            },
            {
                "Args": [
                    "loss",
                    "variables"
                ],
                "DocStr": "Returns the gradients of `loss` w.r.t. `variables`.\n\n# Arguments\n    loss: Scalar tensor to minimize.\n    variables: List of variables.\n\n# Returns\n    A gradients tensor.",
                "Name": "gradients"
            },
            {
                "Args": [
                    "x",
                    "y"
                ],
                "DocStr": "Element-wise truth value of (x > y).\n\n# Arguments\n    x: Tensor or variable.\n    y: Tensor or variable.\n\n# Returns\n    A bool tensor.",
                "Name": "greater"
            },
            {
                "Args": [
                    "x",
                    "y"
                ],
                "DocStr": "Element-wise truth value of (x >= y).\n\n# Arguments\n    x: Tensor or variable.\n    y: Tensor or variable.\n\n# Returns\n    A bool tensor.",
                "Name": "greater_equal"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Segment-wise linear approximation of sigmoid.\n\nFaster than sigmoid.\nReturns `0.` if `x < -2.5`, `1.` if `x > 2.5`.\nIn `-2.5 <= x <= 2.5`, returns `0.2 * x + 0.5`.\n\n# Arguments\n    x: A tensor or variable.\n\n# Returns\n    A tensor.",
                "Name": "hard_sigmoid"
            },
            {
                "Args": [
                    "fn",
                    "name",
                    "accept_all"
                ],
                "Defaults": [
                    "False"
                ],
                "DocStr": "Checks if a callable accepts a given keyword argument.\n\nFor Python 2, checks if there is an argument with the given name.\n\nFor Python 3, checks if there is an argument with the given name, and\nalso whether this argument can be called with a keyword (i.e. if it is\nnot a positional-only argument).\n\n# Arguments\n    fn: Callable to inspect.\n    name: Check if `fn` can be called with `name` as a keyword argument.\n    accept_all: What to return if there is no parameter called `name`\n                but the function accepts a `**kwargs` argument.\n\n# Returns\n    bool, whether `fn` accepts a `name` keyword argument.",
                "Name": "has_arg"
            },
            {
                "Args": [
                    "x",
                    "name"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": "Returns a tensor with the same content as the input tensor.\n\n# Arguments\n    x: The input tensor.\n    name: String, name for the variable to create.\n\n# Returns\n    A tensor of the same shape, type and content.",
                "Name": "identity"
            },
            {
                "Args": [],
                "DocStr": "Returns the default image data format convention ('channels_first' or 'channels_last').\n\n# Returns\n    A string, either `'channels_first'` or `'channels_last'`\n\n# Example\n```python\n    >>> keras.backend.image_data_format()\n    'channels_first'\n```",
                "Name": "image_data_format"
            },
            {
                "Args": [],
                "DocStr": "Legacy getter for `image_data_format`.\n\n# Returns\n    string, one of `'th'`, `'tf'`",
                "Name": "image_dim_ordering"
            },
            {
                "Args": [
                    "x",
                    "alt",
                    "training"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": "Selects `x` in test phase, and `alt` otherwise.\n\nNote that `alt` should have the *same shape* as `x`.\n\n# Arguments\n    x: What to return in test phase\n        (tensor or callable that returns a tensor).\n    alt: What to return otherwise\n        (tensor or callable that returns a tensor).\n    training: Optional scalar tensor\n        (or Python boolean, or Python integer)\n        specifying the learning phase.\n\n# Returns\n    Either `x` or `alt` based on `K.learning_phase`.",
                "Name": "in_test_phase"
            },
            {
                "Args": [
                    "predictions",
                    "targets",
                    "k"
                ],
                "DocStr": "Returns whether the `targets` are in the top `k` `predictions`.\n\n# Arguments\n    predictions: A tensor of shape `(batch_size, classes)` and type `float32`.\n    targets: A 1D tensor of length `batch_size` and type `int32` or `int64`.\n    k: An `int`, number of top elements to consider.\n\n# Returns\n    A 1D tensor of length `batch_size` and type `bool`.\n    `output[i]` is `True` if `predictions[i, targets[i]]` is within top-`k`\n    values of `predictions[i]`.",
                "Name": "in_top_k"
            },
            {
                "Args": [
                    "x",
                    "alt",
                    "training"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": "Selects `x` in train phase, and `alt` otherwise.\n\nNote that `alt` should have the *same shape* as `x`.\n\n# Arguments\n    x: What to return in train phase\n        (tensor or callable that returns a tensor).\n    alt: What to return otherwise\n        (tensor or callable that returns a tensor).\n    training: Optional scalar tensor\n        (or Python boolean, or Python integer)\n        specifying the learning phase.\n\n# Returns\n    Either `x` or `alt` based on the `training` flag.\n    the `training` flag defaults to `K.learning_phase()`.",
                "Name": "in_train_phase"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Returns the shape of tensor or variable as a tuple of int or None entries.\n\n# Arguments\n    x: Tensor or variable.\n\n# Returns\n    A tuple of integers (or None entries).\n\n# Examples\n```python\n    >>> from keras import backend as K\n    >>> inputs = K.placeholder(shape=(2, 4, 5))\n    >>> K.int_shape(inputs)\n    (2, 4, 5)\n    >>> val = np.array([[1, 2], [3, 4]])\n    >>> kvar = K.variable(value=val)\n    >>> K.int_shape(kvar)\n    (2, 2)\n```",
                "Name": "int_shape"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Returns whether `x` is a Keras tensor.\n\nA \"Keras tensor\" is a tensor that was returned by a Keras layer,\n(`Layer` class) or by `Input`.\n\n# Arguments\n    x: A candidate tensor.\n\n# Returns\n    A boolean: Whether the argument is a Keras tensor.\n\n# Raises\n    ValueError: In case `x` is not a symbolic tensor.\n\n# Examples\n```python\n    >>> from keras import backend as K\n    >>> from keras.layers import Input, Dense\n    >>> np_var = numpy.array([1, 2])\n    >>> K.is_keras_tensor(np_var) # A numpy array is not a symbolic tensor.\n    ValueError\n    >>> k_var = tf.placeholder('float32', shape=(1,1))\n    >>> K.is_keras_tensor(k_var) # A variable indirectly created outside of keras is not a Keras tensor.\n    False\n    >>> keras_var = K.variable(np_var)\n    >>> K.is_keras_tensor(keras_var)  # A variable created with the keras backend is not a Keras tensor.\n    False\n    >>> keras_placeholder = K.placeholder(shape=(2, 4, 5))\n    >>> K.is_keras_tensor(keras_placeholder)  # A placeholder is not a Keras tensor.\n    False\n    >>> keras_input = Input([10])\n    >>> K.is_keras_tensor(keras_input) # An Input is a Keras tensor.\n    True\n    >>> keras_layer_output = Dense(10)(keras_input)\n    >>> K.is_keras_tensor(keras_layer_output) # Any Keras layer output is a Keras tensor.\n    True\n```",
                "Name": "is_keras_tensor"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Returns whether `x` is a placeholder.\n\n# Arguments\n    x: A candidate placeholder.\n\n# Returns\n    Boolean.",
                "Name": "is_placeholder"
            },
            {
                "Args": [
                    "tensor"
                ],
                "DocStr": "Returns whether a tensor is a sparse tensor.\n\n# Arguments\n    tensor: A tensor instance.\n\n# Returns\n    A boolean.\n\n# Example\n```python\n    >>> from keras import backend as K\n    >>> a = K.placeholder((2, 2), sparse=False)\n    >>> print(K.is_sparse(a))\n    False\n    >>> b = K.placeholder((2, 2), sparse=True)\n    >>> print(K.is_sparse(b))\n    True\n```",
                "Name": "is_sparse"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": null,
                "Name": "is_tensor"
            },
            {
                "Args": [
                    "x",
                    "axis"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": "Normalizes a tensor wrt the L2 norm alongside the specified axis.\n\n# Arguments\n    x: Tensor or variable.\n    axis: axis along which to perform normalization.\n\n# Returns\n    A tensor.",
                "Name": "l2_normalize"
            },
            {
                "Args": [],
                "DocStr": "Returns the learning phase flag.\n\nThe learning phase flag is a bool tensor (0 = test, 1 = train)\nto be passed as input to any Keras function\nthat uses a different behavior at train time and test time.\n\n# Returns\n    Learning phase (scalar integer tensor or Python integer).",
                "Name": "learning_phase"
            },
            {
                "Args": [
                    "x",
                    "y"
                ],
                "DocStr": "Element-wise truth value of (x < y).\n\n# Arguments\n    x: Tensor or variable.\n    y: Tensor or variable.\n\n# Returns\n    A bool tensor.",
                "Name": "less"
            },
            {
                "Args": [
                    "x",
                    "y"
                ],
                "DocStr": "Element-wise truth value of (x <= y).\n\n# Arguments\n    x: Tensor or variable.\n    y: Tensor or variable.\n\n# Returns\n    A bool tensor.",
                "Name": "less_equal"
            },
            {
                "Args": [
                    "inputs",
                    "kernel",
                    "kernel_size",
                    "strides",
                    "data_format"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": "Apply 1D conv with un-shared weights.\n\n# Arguments\n    inputs: 3D tensor with shape: (batch_size, steps, input_dim)\n    kernel: the unshared weight for convolution,\n            with shape (output_length, feature_dim, filters)\n    kernel_size: a tuple of a single integer,\n                 specifying the length of the 1D convolution window\n    strides: a tuple of a single integer,\n             specifying the stride length of the convolution\n    data_format: the data format, channels_first or channels_last\n\n# Returns\n    the tensor after 1d conv with un-shared weights, with shape (batch_size, output_length, filters)\n\n# Raises\n    ValueError: If `data_format` is neither\n        `\"channels_last\"` nor `\"channels_first\"`.",
                "Name": "local_conv1d"
            },
            {
                "Args": [
                    "inputs",
                    "kernel",
                    "kernel_size",
                    "strides",
                    "output_shape",
                    "data_format"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": "Apply 2D conv with un-shared weights.\n\n# Arguments\n    inputs: 4D tensor with shape:\n            (batch_size, filters, new_rows, new_cols)\n            if data_format='channels_first'\n            or 4D tensor with shape:\n            (batch_size, new_rows, new_cols, filters)\n            if data_format='channels_last'.\n    kernel: the unshared weight for convolution,\n            with shape (output_items, feature_dim, filters)\n    kernel_size: a tuple of 2 integers, specifying the\n                 width and height of the 2D convolution window.\n    strides: a tuple of 2 integers, specifying the strides\n             of the convolution along the width and height.\n    output_shape: a tuple with (output_row, output_col)\n    data_format: the data format, channels_first or channels_last\n\n# Returns\n    A 4d tensor with shape:\n    (batch_size, filters, new_rows, new_cols)\n    if data_format='channels_first'\n    or 4D tensor with shape:\n    (batch_size, new_rows, new_cols, filters)\n    if data_format='channels_last'.\n\n# Raises\n    ValueError: if `data_format` is neither\n                `channels_last` or `channels_first`.",
                "Name": "local_conv2d"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Element-wise log.\n\n# Arguments\n    x: Tensor or variable.\n\n# Returns\n    A tensor.",
                "Name": "log"
            },
            {
                "Args": [
                    "x",
                    "axis",
                    "keepdims"
                ],
                "Defaults": [
                    "None",
                    " False"
                ],
                "DocStr": "Computes log(sum(exp(elements across dimensions of a tensor))).\n\nThis function is more numerically stable than log(sum(exp(x))).\nIt avoids overflows caused by taking the exp of large inputs and\nunderflows caused by taking the log of small inputs.\n\n# Arguments\n    x: A tensor or variable.\n    axis: axis: An integer or list of integers in [-rank(x), rank(x)),\n        the axes to compute the logsumexp. If `None` (default), computes\n        the logsumexp over all dimensions.\n    keepdims: A boolean, whether to keep the dimensions or not.\n        If `keepdims` is `False`, the rank of the tensor is reduced\n        by 1. If `keepdims` is `True`, the reduced dimension is\n        retained with length 1.\n\n# Returns\n    The reduced tensor.",
                "Name": "logsumexp"
            },
            {
                "Args": [
                    "value"
                ],
                "DocStr": "Sets the manual variable initialization flag.\n\nThis boolean flag determines whether\nvariables should be initialized\nas they are instantiated (default), or if\nthe user should handle the initialization\n(e.g. via `tf.initialize_all_variables()`).\n\n# Arguments\n    value: Python boolean.",
                "Name": "manual_variable_initialization"
            },
            {
                "Args": [
                    "fn",
                    "elems",
                    "name",
                    "dtype"
                ],
                "Defaults": [
                    "None",
                    " None"
                ],
                "DocStr": "Map the function fn over the elements elems and return the outputs.\n\n# Arguments\n    fn: Callable that will be called upon each element in elems\n    elems: tensor\n    name: A string name for the map node in the graph\n    dtype: Output data type.\n\n# Returns\n    Tensor with dtype `dtype`.",
                "Name": "map_fn"
            },
            {
                "Args": [
                    "x",
                    "axis",
                    "keepdims"
                ],
                "Defaults": [
                    "None",
                    " False"
                ],
                "DocStr": "Maximum value in a tensor.\n\n# Arguments\n    x: A tensor or variable.\n    axis: An integer or list of integers in [-rank(x), rank(x)),\n        the axes to find maximum values. If `None` (default), finds the\n        maximum over all dimensions.\n    keepdims: A boolean, whether to keep the dimensions or not.\n        If `keepdims` is `False`, the rank of the tensor is reduced\n        by 1. If `keepdims` is `True`,\n        the reduced dimension is retained with length 1.\n\n# Returns\n    A tensor with maximum values of `x`.",
                "Name": "max"
            },
            {
                "Args": [
                    "x",
                    "y"
                ],
                "DocStr": "Element-wise maximum of two tensors.\n\n# Arguments\n    x: Tensor or variable.\n    y: Tensor or variable.\n\n# Returns\n    A tensor.",
                "Name": "maximum"
            },
            {
                "Args": [
                    "x",
                    "axis",
                    "keepdims"
                ],
                "Defaults": [
                    "None",
                    " False"
                ],
                "DocStr": "Mean of a tensor, alongside the specified axis.\n\n# Arguments\n    x: A tensor or variable.\n    axis: An integer or list of integers in [-rank(x), rank(x)),\n        the axes to compute the mean. If `None` (default), computes\n        the mean over all dimensions.\n    keepdims: A boolean, whether to keep the dimensions or not.\n        If `keepdims` is `False`, the rank of the tensor is reduced\n        by 1 for each entry in `axis`. If `keepdims` is `True`,\n        the reduced dimensions are retained with length 1.\n\n# Returns\n    A tensor with the mean of elements of `x`.",
                "Name": "mean"
            },
            {
                "Args": [
                    "x",
                    "axis",
                    "keepdims"
                ],
                "Defaults": [
                    "None",
                    " False"
                ],
                "DocStr": "Minimum value in a tensor.\n\n# Arguments\n    x: A tensor or variable.\n    axis: An integer or list of integers in [-rank(x), rank(x)),\n        the axes to find minimum values. If `None` (default), finds the\n        minimum over all dimensions.\n    keepdims: A boolean, whether to keep the dimensions or not.\n        If `keepdims` is `False`, the rank of the tensor is reduced\n        by 1. If `keepdims` is `True`,\n        the reduced dimension is retained with length 1.\n\n# Returns\n    A tensor with miminum values of `x`.",
                "Name": "min"
            },
            {
                "Args": [
                    "x",
                    "y"
                ],
                "DocStr": "Element-wise minimum of two tensors.\n\n# Arguments\n    x: Tensor or variable.\n    y: Tensor or variable.\n\n# Returns\n    A tensor.",
                "Name": "minimum"
            },
            {
                "Args": [
                    "x",
                    "value",
                    "momentum"
                ],
                "DocStr": "Compute the moving average of a variable.\n\n# Arguments\n    x: A `Variable`.\n    value: A tensor with the same shape as `x`.\n    momentum: The moving average momentum.\n\n# Returns\n    An operation to update the variable.",
                "Name": "moving_average_update"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Returns the number of axes in a tensor, as an integer.\n\n# Arguments\n    x: Tensor or variable.\n\n# Returns\n    Integer (scalar), number of axes.\n\n# Examples\n```python\n    >>> from keras import backend as K\n    >>> inputs = K.placeholder(shape=(2, 4, 5))\n    >>> val = np.array([[1, 2], [3, 4]])\n    >>> kvar = K.variable(value=val)\n    >>> K.ndim(inputs)\n    3\n    >>> K.ndim(kvar)\n    2\n```",
                "Name": "ndim"
            },
            {
                "Args": [
                    "x",
                    "gamma",
                    "beta",
                    "reduction_axes",
                    "epsilon"
                ],
                "Defaults": [
                    "0.001"
                ],
                "DocStr": "Computes mean and std for batch then apply batch_normalization on batch.\n\n# Arguments\n    x: Input tensor or variable.\n    gamma: Tensor by which to scale the input.\n    beta: Tensor with which to center the input.\n    reduction_axes: iterable of integers,\n        axes over which to normalize.\n    epsilon: Fuzz factor.\n\n# Returns\n    A tuple length of 3, `(normalized_tensor, mean, variance)`.",
                "Name": "normalize_batch_in_training"
            },
            {
                "Args": [
                    "value"
                ],
                "DocStr": "Checks that the value correspond to a valid data format.\n\n# Arguments\n    value: String or None. `'channels_first'` or `'channels_last'`.\n\n# Returns\n    A string, either `'channels_first'` or `'channels_last'`\n\n# Example\n```python\n    >>> from keras import backend as K\n    >>> K.normalize_data_format(None)\n    'channels_first'\n    >>> K.normalize_data_format('channels_last')\n    'channels_last'\n```\n\n# Raises\n    ValueError: if `value` or the global `data_format` invalid.",
                "Name": "normalize_data_format"
            },
            {
                "Args": [
                    "x",
                    "y"
                ],
                "DocStr": "Element-wise inequality between two tensors.\n\n# Arguments\n    x: Tensor or variable.\n    y: Tensor or variable.\n\n# Returns\n    A bool tensor.",
                "Name": "not_equal"
            },
            {
                "Args": [
                    "indices",
                    "num_classes"
                ],
                "DocStr": "Computes the one-hot representation of an integer tensor.\n\n# Arguments\n    indices: nD integer tensor of shape\n        `(batch_size, dim1, dim2, ... dim(n-1))`\n    num_classes: Integer, number of classes to consider.\n\n# Returns\n    (n + 1)D one hot representation of the input\n    with shape `(batch_size, dim1, dim2, ... dim(n-1), num_classes)`",
                "Name": "one_hot"
            },
            {
                "Args": [
                    "shape",
                    "dtype",
                    "name"
                ],
                "Defaults": [
                    "None",
                    " None"
                ],
                "DocStr": "Instantiates an all-ones variable and returns it.\n\n# Arguments\n    shape: Tuple of integers, shape of returned Keras variable.\n    dtype: String, data type of returned Keras variable.\n    name: String, name of returned Keras variable.\n\n# Returns\n    A Keras variable, filled with `1.0`.\n    Note that if `shape` was symbolic, we cannot return a variable,\n    and will return a dynamically-shaped tensor instead.\n\n# Example\n```python\n    >>> from keras import backend as K\n    >>> kvar = K.ones((3,4))\n    >>> K.eval(kvar)\n    array([[ 1.,  1.,  1.,  1.],\n           [ 1.,  1.,  1.,  1.],\n           [ 1.,  1.,  1.,  1.]], dtype=float32)\n```",
                "Name": "ones"
            },
            {
                "Args": [
                    "x",
                    "dtype",
                    "name"
                ],
                "Defaults": [
                    "None",
                    " None"
                ],
                "DocStr": "Instantiates an all-ones variable of the same shape as another tensor.\n\n# Arguments\n    x: Keras variable or tensor.\n    dtype: String, dtype of returned Keras variable.\n         None uses the dtype of x.\n    name: String, name for the variable to create.\n\n# Returns\n    A Keras variable with the shape of x filled with ones.\n\n# Example\n```python\n    >>> from keras import backend as K\n    >>> kvar = K.variable(np.random.random((2,3)))\n    >>> kvar_ones = K.ones_like(kvar)\n    >>> K.eval(kvar_ones)\n    array([[ 1.,  1.,  1.],\n           [ 1.,  1.,  1.]], dtype=float32)\n```",
                "Name": "ones_like"
            },
            {
                "Args": [
                    "x",
                    "pattern"
                ],
                "DocStr": "Permutes axes in a tensor.\n\n# Arguments\n    x: Tensor or variable.\n    pattern: A tuple of\n        dimension indices, e.g. `(0, 2, 1)`.\n\n# Returns\n    A tensor.",
                "Name": "permute_dimensions"
            },
            {
                "Args": [
                    "shape",
                    "ndim",
                    "dtype",
                    "sparse",
                    "name"
                ],
                "Defaults": [
                    "None",
                    " None",
                    " None",
                    " False",
                    " None"
                ],
                "DocStr": "Instantiates a placeholder tensor and returns it.\n\n# Arguments\n    shape: Shape of the placeholder\n        (integer tuple, may include `None` entries).\n    ndim: Number of axes of the tensor.\n        At least one of {`shape`, `ndim`} must be specified.\n        If both are specified, `shape` is used.\n    dtype: Placeholder type.\n    sparse: Boolean, whether the placeholder should have a sparse type.\n    name: Optional name string for the placeholder.\n\n# Returns\n    Tensor instance (with Keras metadata included).\n\n# Examples\n```python\n    >>> from keras import backend as K\n    >>> input_ph = K.placeholder(shape=(2, 4, 5))\n    >>> input_ph._keras_shape\n    (2, 4, 5)\n    >>> input_ph\n    <tf.Tensor 'Placeholder_4:0' shape=(2, 4, 5) dtype=float32>\n```",
                "Name": "placeholder"
            },
            {
                "Args": [
                    "x",
                    "pool_size",
                    "strides",
                    "padding",
                    "data_format",
                    "pool_mode"
                ],
                "Defaults": [
                    "1",
                    " 1",
                    " 'valid'",
                    " None",
                    " 'max'"
                ],
                "DocStr": "2D Pooling.\n\n# Arguments\n    x: Tensor or variable.\n    pool_size: tuple of 2 integers.\n    strides: tuple of 2 integers.\n    padding: string, `\"same\"` or `\"valid\"`.\n    data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n    pool_mode: string, `\"max\"` or `\"avg\"`.\n\n# Returns\n    A tensor, result of 2D pooling.\n\n# Raises\n    ValueError: if `data_format` is neither `\"channels_last\"` or `\"channels_first\"`.\n    ValueError: if `pool_mode` is neither `\"max\"` or `\"avg\"`.",
                "Name": "pool2d"
            },
            {
                "Args": [
                    "x",
                    "pool_size",
                    "strides",
                    "padding",
                    "data_format",
                    "pool_mode"
                ],
                "Defaults": [
                    "1",
                    " 1",
                    " 1",
                    " 'valid'",
                    " None",
                    " 'max'"
                ],
                "DocStr": "3D Pooling.\n\n# Arguments\n    x: Tensor or variable.\n    pool_size: tuple of 3 integers.\n    strides: tuple of 3 integers.\n    padding: string, `\"same\"` or `\"valid\"`.\n    data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n    pool_mode: string, `\"max\"` or `\"avg\"`.\n\n# Returns\n    A tensor, result of 3D pooling.\n\n# Raises\n    ValueError: if `data_format` is neither `\"channels_last\"` or `\"channels_first\"`.\n    ValueError: if `pool_mode` is neither `\"max\"` or `\"avg\"`.",
                "Name": "pool3d"
            },
            {
                "Args": [
                    "x",
                    "a"
                ],
                "DocStr": "Element-wise exponentiation.\n\n# Arguments\n    x: Tensor or variable.\n    a: Python integer.\n\n# Returns\n    A tensor.",
                "Name": "pow"
            },
            {
                "Args": [
                    "x",
                    "message"
                ],
                "Defaults": [
                    "''"
                ],
                "DocStr": "Prints `message` and the tensor value when evaluated.\n\n Note that `print_tensor` returns a new tensor identical to `x`\n which should be used in the following code. Otherwise the\n print operation is not taken into account during evaluation.\n\n # Example\n ```python\n     >>> x = K.print_tensor(x, message=\"x is: \")\n ```\n\n# Arguments\n    x: Tensor to print.\n    message: Message to print jointly with the tensor.\n\n# Returns\n    The same tensor `x`, unchanged.",
                "Name": "print_tensor"
            },
            {
                "Args": [
                    "x",
                    "axis",
                    "keepdims"
                ],
                "Defaults": [
                    "None",
                    " False"
                ],
                "DocStr": "Multiplies the values in a tensor, alongside the specified axis.\n\n# Arguments\n    x: A tensor or variable.\n    axis: An integer or list of integers in [-rank(x), rank(x)),\n        the axes to compute the product. If `None` (default), computes\n        the product over all dimensions.\n    keepdims: A boolean, whether to keep the dimensions or not.\n        If `keepdims` is `False`, the rank of the tensor is reduced\n        by 1. If `keepdims` is `True`,\n        the reduced dimension is retained with length 1.\n\n# Returns\n    A tensor with the product of elements of `x`.",
                "Name": "prod"
            },
            {
                "Args": [
                    "shape",
                    "p",
                    "dtype",
                    "seed"
                ],
                "Defaults": [
                    "0.0",
                    " None",
                    " None"
                ],
                "DocStr": "Returns a tensor with random binomial distribution of values.\n\n# Arguments\n    shape: A tuple of integers, the shape of tensor to create.\n    p: A float, `0. <= p <= 1`, probability of binomial distribution.\n    dtype: String, dtype of returned tensor.\n    seed: Integer, random seed.\n\n# Returns\n    A tensor.",
                "Name": "random_binomial"
            },
            {
                "Args": [
                    "shape",
                    "mean",
                    "stddev",
                    "dtype",
                    "seed"
                ],
                "Defaults": [
                    "0.0",
                    " 1.0",
                    " None",
                    " None"
                ],
                "DocStr": "Returns a tensor with normal distribution of values.\n\n# Arguments\n    shape: A tuple of integers, the shape of tensor to create.\n    mean: A float, mean of the normal distribution to draw samples.\n    stddev: A float, standard deviation of the normal distribution\n        to draw samples.\n    dtype: String, dtype of returned tensor.\n    seed: Integer, random seed.\n\n# Returns\n    A tensor.",
                "Name": "random_normal"
            },
            {
                "Args": [
                    "shape",
                    "mean",
                    "scale",
                    "dtype",
                    "name",
                    "seed"
                ],
                "Defaults": [
                    "None",
                    " None",
                    " None"
                ],
                "DocStr": "Instantiates a variable with values drawn from a normal distribution.\n\n# Arguments\n    shape: Tuple of integers, shape of returned Keras variable.\n    mean: Float, mean of the normal distribution.\n    scale: Float, standard deviation of the normal distribution.\n    dtype: String, dtype of returned Keras variable.\n    name: String, name of returned Keras variable.\n    seed: Integer, random seed.\n\n# Returns\n    A Keras variable, filled with drawn samples.\n\n# Example\n```python\n    # TensorFlow example\n    >>> kvar = K.random_normal_variable((2,3), 0, 1)\n    >>> kvar\n    <tensorflow.python.ops.variables.Variable object at 0x10ab12dd0>\n    >>> K.eval(kvar)\n    array([[ 1.19591331,  0.68685907, -0.63814116],\n           [ 0.92629528,  0.28055015,  1.70484698]], dtype=float32)\n```",
                "Name": "random_normal_variable"
            },
            {
                "Args": [
                    "shape",
                    "minval",
                    "maxval",
                    "dtype",
                    "seed"
                ],
                "Defaults": [
                    "0.0",
                    " 1.0",
                    " None",
                    " None"
                ],
                "DocStr": "Returns a tensor with uniform distribution of values.\n\n# Arguments\n    shape: A tuple of integers, the shape of tensor to create.\n    minval: A float, lower boundary of the uniform distribution\n        to draw samples.\n    maxval: A float, upper boundary of the uniform distribution\n        to draw samples.\n    dtype: String, dtype of returned tensor.\n    seed: Integer, random seed.\n\n# Returns\n    A tensor.",
                "Name": "random_uniform"
            },
            {
                "Args": [
                    "shape",
                    "low",
                    "high",
                    "dtype",
                    "name",
                    "seed"
                ],
                "Defaults": [
                    "None",
                    " None",
                    " None"
                ],
                "DocStr": "Instantiates a variable with values drawn from a uniform distribution.\n\n# Arguments\n    shape: Tuple of integers, shape of returned Keras variable.\n    low: Float, lower boundary of the output interval.\n    high: Float, upper boundary of the output interval.\n    dtype: String, dtype of returned Keras variable.\n    name: String, name of returned Keras variable.\n    seed: Integer, random seed.\n\n# Returns\n    A Keras variable, filled with drawn samples.\n\n# Example\n```python\n    # TensorFlow example\n    >>> kvar = K.random_uniform_variable((2,3), 0, 1)\n    >>> kvar\n    <tensorflow.python.ops.variables.Variable object at 0x10ab40b10>\n    >>> K.eval(kvar)\n    array([[ 0.10940075,  0.10047495,  0.476143  ],\n           [ 0.66137183,  0.00869417,  0.89220798]], dtype=float32)\n```",
                "Name": "random_uniform_variable"
            },
            {
                "Args": [
                    "x",
                    "alpha",
                    "max_value",
                    "threshold"
                ],
                "Defaults": [
                    "0.0",
                    " None",
                    " 0.0"
                ],
                "DocStr": "Rectified linear unit.\n\nWith default values, it returns element-wise `max(x, 0)`.\n\nOtherwise, it follows:\n`f(x) = max_value` for `x >= max_value`,\n`f(x) = x` for `threshold <= x < max_value`,\n`f(x) = alpha * (x - threshold)` otherwise.\n\n# Arguments\n    x: A tensor or variable.\n    alpha: A scalar, slope of negative section (default=`0.`).\n    max_value: float. Saturation threshold.\n    threshold: float. Threshold value for thresholded activation.\n\n# Returns\n    A tensor.",
                "Name": "relu"
            },
            {
                "Args": [
                    "x",
                    "n"
                ],
                "DocStr": "Repeats a 2D tensor.\n\nif `x` has shape (samples, dim) and `n` is `2`,\nthe output will have shape `(samples, 2, dim)`.\n\n# Arguments\n    x: Tensor or variable.\n    n: Python integer, number of times to repeat.\n\n# Returns\n    A tensor.",
                "Name": "repeat"
            },
            {
                "Args": [
                    "x",
                    "rep",
                    "axis"
                ],
                "DocStr": "Repeats the elements of a tensor along an axis, like `np.repeat`.\n\nIf `x` has shape `(s1, s2, s3)` and `axis` is `1`, the output\nwill have shape `(s1, s2 * rep, s3)`.\n\n# Arguments\n    x: Tensor or variable.\n    rep: Python integer, number of times to repeat.\n    axis: Axis along which to repeat.\n\n# Returns\n    A tensor.",
                "Name": "repeat_elements"
            },
            {
                "Args": [],
                "DocStr": "Resets graph identifiers.\n    ",
                "Name": "reset_uids"
            },
            {
                "Args": [
                    "x",
                    "shape"
                ],
                "DocStr": "Reshapes a tensor to the specified shape.\n\n# Arguments\n    x: Tensor or variable.\n    shape: Target shape tuple.\n\n# Returns\n    A tensor.",
                "Name": "reshape"
            },
            {
                "Args": [
                    "x",
                    "height_factor",
                    "width_factor",
                    "data_format",
                    "interpolation"
                ],
                "Defaults": [
                    "'nearest'"
                ],
                "DocStr": "Resizes the images contained in a 4D tensor.\n\n# Arguments\n    x: Tensor or variable to resize.\n    height_factor: Positive integer.\n    width_factor: Positive integer.\n    data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n    interpolation: A string, one of `nearest` or `bilinear`.\n\n# Returns\n    A tensor.\n\n# Raises\n    ValueError: if `data_format` is neither `\"channels_last\"` or `\"channels_first\"`.",
                "Name": "resize_images"
            },
            {
                "Args": [
                    "x",
                    "depth_factor",
                    "height_factor",
                    "width_factor",
                    "data_format"
                ],
                "DocStr": "Resizes the volume contained in a 5D tensor.\n\n# Arguments\n    x: Tensor or variable to resize.\n    depth_factor: Positive integer.\n    height_factor: Positive integer.\n    width_factor: Positive integer.\n    data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n\n# Returns\n    A tensor.\n\n# Raises\n    ValueError: if `data_format` is neither `\"channels_last\"` or `\"channels_first\"`.",
                "Name": "resize_volumes"
            },
            {
                "Args": [
                    "x",
                    "axes"
                ],
                "DocStr": "Reverses a tensor along the specified axes.\n\n# Arguments\n    x: Tensor to reverse.\n    axes: Integer or iterable of integers.\n        Axes to reverse.\n\n# Returns\n    A tensor.",
                "Name": "reverse"
            },
            {
                "Args": [
                    "step_function",
                    "inputs",
                    "initial_states",
                    "go_backwards",
                    "mask",
                    "constants",
                    "unroll",
                    "input_length"
                ],
                "Defaults": [
                    "False",
                    " None",
                    " None",
                    " False",
                    " None"
                ],
                "DocStr": "Iterates over the time dimension of a tensor.\n\n# Arguments\n    step_function:\n        Parameters:\n            inputs: Tensor with shape (samples, ...) (no time dimension),\n                representing input for the batch of samples at a certain\n                time step.\n            states: List of tensors.\n        Returns:\n            outputs: Tensor with shape (samples, ...) (no time dimension),\n            new_states: List of tensors, same length and shapes\n                as 'states'.\n    inputs: Tensor of temporal data of shape (samples, time, ...)\n        (at least 3D).\n    initial_states: Tensor with shape (samples, ...) (no time dimension),\n        containing the initial values for the states used in\n        the step function.\n    go_backwards: Boolean. If True, do the iteration over the time\n        dimension in reverse order and return the reversed sequence.\n    mask: Binary tensor with shape (samples, time),\n        with a zero for every element that is masked.\n    constants: A list of constant values passed at each step.\n    unroll: Whether to unroll the RNN or to use a symbolic loop\n        (`while_loop` or `scan` depending on backend).\n    input_length: Static number of timesteps in the input.\n\n# Returns\n    A tuple, `(last_output, outputs, new_states)`.\n\n    last_output: The latest output of the rnn, of shape `(samples, ...)`\n    outputs: Tensor with shape `(samples, time, ...)` where each\n        entry `outputs[s, t]` is the output of the step function\n        at time `t` for sample `s`.\n    new_states: List of tensors, latest states returned by\n        the step function, of shape `(samples, ...)`.\n\n# Raises\n    ValueError: If input dimension is less than 3.\n    ValueError: If `unroll` is `True`\n        but input timestep is not a fixed number.\n    ValueError: If `mask` is provided (not `None`)\n        but states is not provided (`len(states)` == 0).",
                "Name": "rnn"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Element-wise rounding to the closest integer.\n\nIn case of tie, the rounding mode used is \"half to even\".\n\n# Arguments\n    x: Tensor or variable.\n\n# Returns\n    A tensor.",
                "Name": "round"
            },
            {
                "Args": [
                    "x",
                    "depthwise_kernel",
                    "pointwise_kernel",
                    "strides",
                    "padding",
                    "data_format",
                    "dilation_rate"
                ],
                "Defaults": [
                    "1",
                    " 'valid'",
                    " None",
                    " 1"
                ],
                "DocStr": "1D convolution with separable filters.\n\n# Arguments\n    x: input tensor\n    depthwise_kernel: convolution kernel for the depthwise convolution.\n    pointwise_kernel: kernel for the 1x1 convolution.\n    strides: stride integer.\n    padding: string, `\"same\"` or `\"valid\"`.\n    data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n    dilation_rate: integer dilation rate.\n\n# Returns\n    Output tensor.\n\n# Raises\n    ValueError: If `data_format` is neither\n        `\"channels_last\"` nor `\"channels_first\"`.",
                "Name": "separable_conv1d"
            },
            {
                "Args": [
                    "x",
                    "depthwise_kernel",
                    "pointwise_kernel",
                    "strides",
                    "padding",
                    "data_format",
                    "dilation_rate"
                ],
                "Defaults": [
                    "1",
                    " 1",
                    " 'valid'",
                    " None",
                    " 1",
                    " 1"
                ],
                "DocStr": "2D convolution with separable filters.\n\n# Arguments\n    x: input tensor\n    depthwise_kernel: convolution kernel for the depthwise convolution.\n    pointwise_kernel: kernel for the 1x1 convolution.\n    strides: strides tuple (length 2).\n    padding: string, `\"same\"` or `\"valid\"`.\n    data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n    dilation_rate: tuple of integers,\n        dilation rates for the separable convolution.\n\n# Returns\n    Output tensor.\n\n# Raises\n    ValueError: If `data_format` is neither\n        `\"channels_last\"` nor `\"channels_first\"`.",
                "Name": "separable_conv2d"
            },
            {
                "Args": [
                    "e"
                ],
                "DocStr": "Sets the value of the fuzz factor used in numeric expressions.\n\n# Arguments\n    e: float. New value of epsilon.\n\n# Example\n```python\n    >>> from keras import backend as K\n    >>> K.epsilon()\n    1e-07\n    >>> K.set_epsilon(1e-05)\n    >>> K.epsilon()\n    1e-05\n```",
                "Name": "set_epsilon"
            },
            {
                "Args": [
                    "floatx"
                ],
                "DocStr": "Sets the default float type.\n\n# Arguments\n    floatx: String, 'float16', 'float32', or 'float64'.\n\n# Example\n```python\n    >>> from keras import backend as K\n    >>> K.floatx()\n    'float32'\n    >>> K.set_floatx('float16')\n    >>> K.floatx()\n    'float16'\n```",
                "Name": "set_floatx"
            },
            {
                "Args": [
                    "data_format"
                ],
                "DocStr": "Sets the value of the data format convention.\n\n# Arguments\n    data_format: string. `'channels_first'` or `'channels_last'`.\n\n# Example\n```python\n    >>> from keras import backend as K\n    >>> K.image_data_format()\n    'channels_first'\n    >>> K.set_image_data_format('channels_last')\n    >>> K.image_data_format()\n    'channels_last'\n```",
                "Name": "set_image_data_format"
            },
            {
                "Args": [
                    "dim_ordering"
                ],
                "DocStr": "Legacy setter for `image_data_format`.\n\n# Arguments\n    dim_ordering: string. `tf` or `th`.\n\n# Example\n```python\n    >>> from keras import backend as K\n    >>> K.image_data_format()\n    'channels_first'\n    >>> K.set_image_data_format('channels_last')\n    >>> K.image_data_format()\n    'channels_last'\n```\n\n# Raises\n    ValueError: if `dim_ordering` is invalid.",
                "Name": "set_image_dim_ordering"
            },
            {
                "Args": [
                    "value"
                ],
                "DocStr": "Sets the learning phase to a fixed value.\n\n# Arguments\n    value: Learning phase value, either 0 or 1 (integers).\n\n# Raises\n    ValueError: if `value` is neither `0` nor `1`.",
                "Name": "set_learning_phase"
            },
            {
                "Args": [
                    "session"
                ],
                "DocStr": "Sets the global TensorFlow session.\n\n# Arguments\n    session: A TF Session.",
                "Name": "set_session"
            },
            {
                "Args": [
                    "x",
                    "value"
                ],
                "DocStr": "Sets the value of a variable, from a Numpy array.\n\n# Arguments\n    x: Tensor to set to a new value.\n    value: Value to set the tensor to, as a Numpy array\n        (of the same shape).",
                "Name": "set_value"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Returns the symbolic shape of a tensor or variable.\n\n# Arguments\n    x: A tensor or variable.\n\n# Returns\n    A symbolic shape (which is itself a tensor).\n\n# Examples\n```python\n    # TensorFlow example\n    >>> from keras import backend as K\n    >>> tf_session = K.get_session()\n    >>> val = np.array([[1, 2], [3, 4]])\n    >>> kvar = K.variable(value=val)\n    >>> inputs = keras.backend.placeholder(shape=(2, 4, 5))\n    >>> K.shape(kvar)\n    <tf.Tensor 'Shape_8:0' shape=(2,) dtype=int32>\n    >>> K.shape(inputs)\n    <tf.Tensor 'Shape_9:0' shape=(3,) dtype=int32>\n    # To get integer shape (Instead, you can use K.int_shape(x))\n    >>> K.shape(kvar).eval(session=tf_session)\n    array([2, 2], dtype=int32)\n    >>> K.shape(inputs).eval(session=tf_session)\n    array([2, 4, 5], dtype=int32)\n```",
                "Name": "shape"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Element-wise sigmoid.\n\n# Arguments\n    x: A tensor or variable.\n\n# Returns\n    A tensor.",
                "Name": "sigmoid"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Element-wise sign.\n\n# Arguments\n    x: Tensor or variable.\n\n# Returns\n    A tensor.",
                "Name": "sign"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Computes sin of x element-wise.\n\n# Arguments\n    x: Tensor or variable.\n\n# Returns\n    A tensor.",
                "Name": "sin"
            },
            {
                "Args": [
                    "x",
                    "start",
                    "size"
                ],
                "DocStr": "Extracts a slice from a tensor.\n\n# Arguments\n    x: Input tensor.\n    start: Integer list/tuple or tensor\n        indicating the start indices of the slice\n        along each axis.\n    size: Integer list/tuple or tensor\n        indicating how many dimensions to slice\n        along each axis.\n\n# Returns\n    Tensor `x[start[0]: start[0] + size[0],\n              ...,\n              start[-1]: start[-1] + size[-1]]`",
                "Name": "slice"
            },
            {
                "Args": [
                    "x",
                    "axis"
                ],
                "Defaults": [
                    "-1"
                ],
                "DocStr": "Softmax of a tensor.\n\n# Arguments\n    x: A tensor or variable.\n    axis: The dimension softmax would be performed on.\n        The default is -1 which indicates the last dimension.\n\n# Returns\n    A tensor.",
                "Name": "softmax"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Softplus of a tensor.\n\n# Arguments\n    x: A tensor or variable.\n\n# Returns\n    A tensor.",
                "Name": "softplus"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Softsign of a tensor.\n\n# Arguments\n    x: A tensor or variable.\n\n# Returns\n    A tensor.",
                "Name": "softsign"
            },
            {
                "Args": [
                    "target",
                    "output",
                    "from_logits",
                    "axis"
                ],
                "Defaults": [
                    "False",
                    " -1"
                ],
                "DocStr": "Categorical crossentropy with integer targets.\n\n# Arguments\n    target: An integer tensor.\n    output: A tensor resulting from a softmax\n        (unless `from_logits` is True, in which\n        case `output` is expected to be the logits).\n    from_logits: Boolean, whether `output` is the\n        result of a softmax, or is a tensor of logits.\n    axis: Int specifying the channels axis. `axis=-1`\n        corresponds to data format `channels_last`,\n        and `axis=1` corresponds to data format\n        `channels_first`.\n\n# Returns\n    Output tensor.\n\n# Raises\n    ValueError: if `axis` is neither -1 nor one of\n        the axes of `output`.",
                "Name": "sparse_categorical_crossentropy"
            },
            {
                "Args": [
                    "x",
                    "padding",
                    "data_format"
                ],
                "Defaults": [
                    "1",
                    " 1",
                    " 1",
                    " 1",
                    " None"
                ],
                "DocStr": "Pads the 2nd and 3rd dimensions of a 4D tensor.\n\n# Arguments\n    x: Tensor or variable.\n    padding: Tuple of 2 tuples, padding pattern.\n    data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n\n# Returns\n    A padded 4D tensor.\n\n# Raises\n    ValueError: if `data_format` is neither `\"channels_last\"` or `\"channels_first\"`.",
                "Name": "spatial_2d_padding"
            },
            {
                "Args": [
                    "x",
                    "padding",
                    "data_format"
                ],
                "Defaults": [
                    "1",
                    " 1",
                    " 1",
                    " 1",
                    " 1",
                    " 1",
                    " None"
                ],
                "DocStr": "Pads 5D tensor with zeros along the depth, height, width dimensions.\n\nPads these dimensions with respectively\n\"padding[0]\", \"padding[1]\" and \"padding[2]\" zeros left and right.\n\nFor 'channels_last' data_format,\nthe 2nd, 3rd and 4th dimension will be padded.\nFor 'channels_first' data_format,\nthe 3rd, 4th and 5th dimension will be padded.\n\n# Arguments\n    x: Tensor or variable.\n    padding: Tuple of 3 tuples, padding pattern.\n    data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n\n# Returns\n    A padded 5D tensor.\n\n# Raises\n    ValueError: if `data_format` is neither `\"channels_last\"` or `\"channels_first\"`.",
                "Name": "spatial_3d_padding"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Element-wise square root.\n\n# Arguments\n    x: Tensor or variable.\n\n# Returns\n    A tensor.",
                "Name": "sqrt"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Element-wise square.\n\n# Arguments\n    x: Tensor or variable.\n\n# Returns\n    A tensor.",
                "Name": "square"
            },
            {
                "Args": [
                    "x",
                    "axis"
                ],
                "DocStr": "Removes a 1-dimension from the tensor at index \"axis\".\n\n# Arguments\n    x: A tensor or variable.\n    axis: Axis to drop.\n\n# Returns\n    A tensor with the same data as `x` but reduced dimensions.",
                "Name": "squeeze"
            },
            {
                "Args": [
                    "x",
                    "axis"
                ],
                "Defaults": [
                    "0"
                ],
                "DocStr": "Stacks a list of rank `R` tensors into a rank `R+1` tensor.\n\n# Arguments\n    x: List of tensors.\n    axis: Axis along which to perform stacking.\n\n# Returns\n    A tensor.",
                "Name": "stack"
            },
            {
                "Args": [
                    "x",
                    "axis",
                    "keepdims"
                ],
                "Defaults": [
                    "None",
                    " False"
                ],
                "DocStr": "Standard deviation of a tensor, alongside the specified axis.\n\n# Arguments\n    x: A tensor or variable.\n    axis: An integer or list of integers in [-rank(x), rank(x)),\n        the axes to compute the standard deviation. If `None` (default),\n        computes the standard deviation over all dimensions.\n    keepdims: A boolean, whether to keep the dimensions or not.\n        If `keepdims` is `False`, the rank of the tensor is reduced\n        by 1. If `keepdims` is `True`,\n        the reduced dimension is retained with length 1.\n\n# Returns\n    A tensor with the standard deviation of elements of `x`.",
                "Name": "std"
            },
            {
                "Args": [
                    "variables"
                ],
                "DocStr": "Returns `variables` but with zero gradient w.r.t. every other variable.\n\n# Arguments\n    variables: tensor or list of tensors to consider constant with respect\n        to any other variable.\n\n# Returns\n    A single tensor or a list of tensors (depending on the passed argument)\n        that has constant gradient with respect to any other variable.",
                "Name": "stop_gradient"
            },
            {
                "Args": [
                    "x",
                    "axis",
                    "keepdims"
                ],
                "Defaults": [
                    "None",
                    " False"
                ],
                "DocStr": "Sum of the values in a tensor, alongside the specified axis.\n\n# Arguments\n    x: A tensor or variable.\n    axis: An integer or list of integers in [-rank(x), rank(x)),\n        the axes to sum over. If `None` (default), sums over all\n        dimensions.\n    keepdims: A boolean, whether to keep the dimensions or not.\n        If `keepdims` is `False`, the rank of the tensor is reduced\n        by 1. If `keepdims` is `True`,\n        the reduced dimension is retained with length 1.\n\n# Returns\n    A tensor with sum of `x`.",
                "Name": "sum"
            },
            {
                "Args": [
                    "condition",
                    "then_expression",
                    "else_expression"
                ],
                "DocStr": "Switches between two operations depending on a scalar value.\n\nNote that both `then_expression` and `else_expression`\nshould be symbolic tensors of the *same shape*.\n\n# Arguments\n    condition: tensor (`int` or `bool`).\n    then_expression: either a tensor, or a callable that returns a tensor.\n    else_expression: either a tensor, or a callable that returns a tensor.\n\n# Returns\n    The selected tensor.\n\n# Raises\n    ValueError: If rank of `condition` is greater than rank of expressions.",
                "Name": "switch"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Element-wise tanh.\n\n# Arguments\n    x: A tensor or variable.\n\n# Returns\n    A tensor.",
                "Name": "tanh"
            },
            {
                "Args": [
                    "x",
                    "padding"
                ],
                "Defaults": [
                    "1",
                    " 1"
                ],
                "DocStr": "Pads the middle dimension of a 3D tensor.\n\n# Arguments\n    x: Tensor or variable.\n    padding: Tuple of 2 integers, how many zeros to\n        add at the start and end of dim 1.\n\n# Returns\n    A padded 3D tensor.",
                "Name": "temporal_padding"
            },
            {
                "Args": [
                    "x",
                    "n"
                ],
                "DocStr": "Creates a tensor by tiling `x` by `n`.\n\n# Arguments\n    x: A tensor or variable\n    n: A list of integer. The length must be the same as the number of\n        dimensions in `x`.\n\n# Returns\n    A tiled tensor.",
                "Name": "tile"
            },
            {
                "Args": [
                    "tensor"
                ],
                "DocStr": "Converts a sparse tensor into a dense tensor and returns it.\n\n# Arguments\n    tensor: A tensor instance (potentially sparse).\n\n# Returns\n    A dense tensor.\n\n# Examples\n```python\n    >>> from keras import backend as K\n    >>> b = K.placeholder((2, 2), sparse=True)\n    >>> print(K.is_sparse(b))\n    True\n    >>> c = K.to_dense(b)\n    >>> print(K.is_sparse(c))\n    False\n```",
                "Name": "to_dense"
            },
            {
                "Args": [
                    "x"
                ],
                "DocStr": "Transposes a tensor and returns it.\n\n# Arguments\n    x: Tensor or variable.\n\n# Returns\n    A tensor.\n\n# Examples\n```python\n    >>> var = K.variable([[1, 2, 3], [4, 5, 6]])\n    >>> K.eval(var)\n    array([[ 1.,  2.,  3.],\n           [ 4.,  5.,  6.]], dtype=float32)\n    >>> var_transposed = K.transpose(var)\n    >>> K.eval(var_transposed)\n    array([[ 1.,  4.],\n           [ 2.,  5.],\n           [ 3.,  6.]], dtype=float32)\n```\n\n```python\n    >>> inputs = K.placeholder((2, 3))\n    >>> inputs\n    <tf.Tensor 'Placeholder_11:0' shape=(2, 3) dtype=float32>\n    >>> input_transposed = K.transpose(inputs)\n    >>> input_transposed\n    <tf.Tensor 'transpose_4:0' shape=(3, 2) dtype=float32>\n\n```",
                "Name": "transpose"
            },
            {
                "Args": [
                    "shape",
                    "target_format",
                    "spatial_axes"
                ],
                "DocStr": "Converts a tuple or a list to the correct `data_format`.\n\nIt does so by switching the positions of its elements.\n\n# Arguments\n    shape: Tuple or list, often representing shape,\n        corresponding to `'channels_last'`.\n    target_format: A string, either `'channels_first'` or `'channels_last'`.\n    spatial_axes: A tuple of integers.\n        Correspond to the indexes of the spatial axes.\n        For example, if you pass a shape\n        representing (batch_size, timesteps, rows, cols, channels),\n        then `spatial_axes=(2, 3)`.\n\n# Returns\n    A tuple or list, with the elements permuted according\n    to `target_format`.\n\n# Example\n```python\n    >>> from keras.utils.generic_utils import transpose_shape\n    >>> transpose_shape((16, 128, 128, 32),'channels_first', spatial_axes=(1, 2))\n    (16, 32, 128, 128)\n    >>> transpose_shape((16, 128, 128, 32), 'channels_last', spatial_axes=(1, 2))\n    (16, 128, 128, 32)\n    >>> transpose_shape((128, 128, 32), 'channels_first', spatial_axes=(0, 1))\n    (32, 128, 128)\n```\n\n# Raises\n    ValueError: if `value` or the global `data_format` invalid.",
                "Name": "transpose_shape"
            },
            {
                "Args": [
                    "shape",
                    "mean",
                    "stddev",
                    "dtype",
                    "seed"
                ],
                "Defaults": [
                    "0.0",
                    " 1.0",
                    " None",
                    " None"
                ],
                "DocStr": "Returns a tensor with truncated random normal distribution of values.\n\nThe generated values follow a normal distribution\nwith specified mean and standard deviation,\nexcept that values whose magnitude is more than\ntwo standard deviations from the mean are dropped and re-picked.\n\n# Arguments\n    shape: A tuple of integers, the shape of tensor to create.\n    mean: Mean of the values.\n    stddev: Standard deviation of the values.\n    dtype: String, dtype of returned tensor.\n    seed: Integer, random seed.\n\n# Returns\n    A tensor.",
                "Name": "truncated_normal"
            },
            {
                "Args": [
                    "x",
                    "new_x"
                ],
                "DocStr": "Update the value of `x` to `new_x`.\n\n# Arguments\n    x: A `Variable`.\n    new_x: A tensor of same shape as `x`.\n\n# Returns\n    The variable `x` updated.",
                "Name": "update"
            },
            {
                "Args": [
                    "x",
                    "increment"
                ],
                "DocStr": "Update the value of `x` by adding `increment`.\n\n# Arguments\n    x: A `Variable`.\n    increment: A tensor of same shape as `x`.\n\n# Returns\n    The variable `x` updated.",
                "Name": "update_add"
            },
            {
                "Args": [
                    "x",
                    "decrement"
                ],
                "DocStr": "Update the value of `x` by subtracting `decrement`.\n\n# Arguments\n    x: A `Variable`.\n    decrement: A tensor of same shape as `x`.\n\n# Returns\n    The variable `x` updated.",
                "Name": "update_sub"
            },
            {
                "Args": [
                    "x",
                    "axis",
                    "keepdims"
                ],
                "Defaults": [
                    "None",
                    " False"
                ],
                "DocStr": "Variance of a tensor, alongside the specified axis.\n\n# Arguments\n    x: A tensor or variable.\n    axis: An integer or list of integers in [-rank(x), rank(x)),\n        the axes to compute the variance. If `None` (default), computes\n        the variance over all dimensions.\n    keepdims: A boolean, whether to keep the dimensions or not.\n        If `keepdims` is `False`, the rank of the tensor is reduced\n        by 1. If `keepdims` is `True`,\n        the reduced dimension is retained with length 1.\n\n# Returns\n    A tensor with the variance of elements of `x`.",
                "Name": "var"
            },
            {
                "Args": [
                    "value",
                    "dtype",
                    "name",
                    "constraint"
                ],
                "Defaults": [
                    "None",
                    " None",
                    " None"
                ],
                "DocStr": "Instantiates a variable and returns it.\n\n# Arguments\n    value: Numpy array, initial value of the tensor.\n    dtype: Tensor type.\n    name: Optional name string for the tensor.\n    constraint: Optional projection function to be\n        applied to the variable after an optimizer update.\n\n# Returns\n    A variable instance (with Keras metadata included).\n\n# Examples\n```python\n    >>> from keras import backend as K\n    >>> val = np.array([[1, 2], [3, 4]])\n    >>> kvar = K.variable(value=val, dtype='float64', name='example_var')\n    >>> K.dtype(kvar)\n    'float64'\n    >>> print(kvar)\n    example_var\n    >>> K.eval(kvar)\n    array([[ 1.,  2.],\n           [ 3.,  4.]])\n```",
                "Name": "variable"
            },
            {
                "Args": [
                    "shape",
                    "dtype",
                    "name"
                ],
                "Defaults": [
                    "None",
                    " None"
                ],
                "DocStr": "Instantiates an all-zeros variable and returns it.\n\n# Arguments\n    shape: Tuple of integers, shape of returned Keras variable\n    dtype: String, data type of returned Keras variable\n    name: String, name of returned Keras variable\n\n# Returns\n    A variable (including Keras metadata), filled with `0.0`.\n    Note that if `shape` was symbolic, we cannot return a variable,\n    and will return a dynamically-shaped tensor instead.\n\n# Example\n```python\n    >>> from keras import backend as K\n    >>> kvar = K.zeros((3,4))\n    >>> K.eval(kvar)\n    array([[ 0.,  0.,  0.,  0.],\n           [ 0.,  0.,  0.,  0.],\n           [ 0.,  0.,  0.,  0.]], dtype=float32)\n```",
                "Name": "zeros"
            },
            {
                "Args": [
                    "x",
                    "dtype",
                    "name"
                ],
                "Defaults": [
                    "None",
                    " None"
                ],
                "DocStr": "Instantiates an all-zeros variable of the same shape as another tensor.\n\n# Arguments\n    x: Keras variable or Keras tensor.\n    dtype: String, dtype of returned Keras variable.\n         None uses the dtype of x.\n    name: String, name for the variable to create.\n\n# Returns\n    A Keras variable with the shape of x filled with zeros.\n\n# Example\n```python\n    >>> from keras import backend as K\n    >>> kvar = K.variable(np.random.random((2,3)))\n    >>> kvar_zeros = K.zeros_like(kvar)\n    >>> K.eval(kvar_zeros)\n    array([[ 0.,  0.,  0.],\n           [ 0.,  0.,  0.]], dtype=float32)\n```",
                "Name": "zeros_like"
            }
        ],
        "Name": "backend",
        "Type": "Module"
    },
    {
        "Classes": [
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "stateful_metrics"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": "Callback that accumulates epoch averages of metrics.\n\nThis callback is automatically applied to every Keras model.\n\n# Arguments\n    stateful_metrics: Iterable of string names of metrics that\n        should *not* be averaged over an epoch.\n        Metrics in this list will be logged as-is in `on_epoch_end`.\n        All others will be averaged in `on_epoch_end`.",
                "Functions": [],
                "Name": "BaseLogger",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "filename",
                    "separator",
                    "append"
                ],
                "Defaults": [
                    "'",
                    "'",
                    " False"
                ],
                "DocStr": "Callback that streams epoch results to a csv file.\n\nSupports all values that can be represented as a string,\nincluding 1D iterables such as np.ndarray.\n\n# Example\n\n```python\ncsv_logger = CSVLogger('training.log')\nmodel.fit(X_train, Y_train, callbacks=[csv_logger])\n```\n\n# Arguments\n    filename: filename of the csv file, e.g. 'run/log.csv'.\n    separator: string used to separate elements in the csv file.\n    append: True: append if file exists (useful for continuing\n        training). False: overwrite existing file,",
                "Functions": [],
                "Name": "CSVLogger",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self"
                ],
                "DocStr": "Abstract base class used to build new callbacks.\n\n# Properties\n    params: dict. Training parameters\n        (eg. verbosity, batch size, number of epochs...).\n    model: instance of `keras.models.Model`.\n        Reference of the model being trained.\n\nThe `logs` dictionary that callback methods\ntake as argument will contain keys for quantities relevant to\nthe current batch or epoch.\n\nCurrently, the `.fit()` method of the `Sequential` model class\nwill include the following quantities in the `logs` that\nit passes to its callbacks:\n\n    on_epoch_end: logs include `acc` and `loss`, and\n        optionally include `val_loss`\n        (if validation is enabled in `fit`), and `val_acc`\n        (if validation and accuracy monitoring are enabled).\n    on_batch_begin: logs include `size`,\n        the number of samples in the current batch.\n    on_batch_end: logs include `loss`, and optionally `acc`\n        (if accuracy monitoring is enabled).",
                "Functions": [],
                "Name": "Callback",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "callbacks",
                    "queue_length"
                ],
                "Defaults": [
                    "None",
                    " 10"
                ],
                "DocStr": "Container abstracting a list of callbacks.\n\n# Arguments\n    callbacks: List of `Callback` instances.\n    queue_length: Queue length for keeping\n        running statistics over callback execution time.",
                "Functions": [],
                "Name": "CallbackList",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "monitor",
                    "min_delta",
                    "patience",
                    "verbose",
                    "mode",
                    "baseline",
                    "restore_best_weights"
                ],
                "Defaults": [
                    "'val_loss'",
                    " 0",
                    " 0",
                    " 0",
                    " 'auto'",
                    " None",
                    " False"
                ],
                "DocStr": "Stop training when a monitored quantity has stopped improving.\n\n# Arguments\n    monitor: quantity to be monitored.\n    min_delta: minimum change in the monitored quantity\n        to qualify as an improvement, i.e. an absolute\n        change of less than min_delta, will count as no\n        improvement.\n    patience: number of epochs with no improvement\n        after which training will be stopped.\n    verbose: verbosity mode.\n    mode: one of {auto, min, max}. In `min` mode,\n        training will stop when the quantity\n        monitored has stopped decreasing; in `max`\n        mode it will stop when the quantity\n        monitored has stopped increasing; in `auto`\n        mode, the direction is automatically inferred\n        from the name of the monitored quantity.\n    baseline: Baseline value for the monitored quantity to reach.\n        Training will stop if the model doesn't show improvement\n        over the baseline.\n    restore_best_weights: whether to restore model weights from\n        the epoch with the best value of the monitored quantity.\n        If False, the model weights obtained at the last step of\n        training are used.",
                "Functions": [],
                "Name": "EarlyStopping",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self"
                ],
                "DocStr": "Callback that records events into a `History` object.\n\nThis callback is automatically applied to\nevery Keras model. The `History` object\ngets returned by the `fit` method of models.",
                "Functions": [],
                "Name": "History",
                "Type": "Class"
            },
            {
                "Abstract": true,
                "Args": [],
                "DocStr": null,
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "C"
                        ],
                        "DocStr": "Abstract classes can override this to customize issubclass().\n\nThis is invoked early on by abc.ABCMeta.__subclasscheck__().\nIt should return True, False or NotImplemented.  If it returns\nNotImplemented, the normal algorithm is used.  Otherwise, it\noverrides the normal algorithm (and the outcome is cached).",
                        "Name": "__subclasshook__"
                    }
                ],
                "Name": "Iterable",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "on_epoch_begin",
                    "on_epoch_end",
                    "on_batch_begin",
                    "on_batch_end",
                    "on_train_begin",
                    "on_train_end"
                ],
                "Defaults": [
                    "None",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None"
                ],
                "DocStr": "Callback for creating simple, custom callbacks on-the-fly.\n\nThis callback is constructed with anonymous functions that will be called\nat the appropriate time. Note that the callbacks expects positional\narguments, as:\n\n - `on_epoch_begin` and `on_epoch_end` expect two positional arguments:\n    `epoch`, `logs`\n - `on_batch_begin` and `on_batch_end` expect two positional arguments:\n    `batch`, `logs`\n - `on_train_begin` and `on_train_end` expect one positional argument:\n    `logs`\n\n# Arguments\n    on_epoch_begin: called at the beginning of every epoch.\n    on_epoch_end: called at the end of every epoch.\n    on_batch_begin: called at the beginning of every batch.\n    on_batch_end: called at the end of every batch.\n    on_train_begin: called at the beginning of model training.\n    on_train_end: called at the end of model training.\n\n# Example\n\n```python\n# Print the batch number at the beginning of every batch.\nbatch_print_callback = LambdaCallback(\n    on_batch_begin=lambda batch,logs: print(batch))\n\n# Stream the epoch loss to a file in JSON format. The file content\n# is not well-formed JSON but rather has a JSON object per line.\nimport json\njson_log = open('loss_log.json', mode='wt', buffering=1)\njson_logging_callback = LambdaCallback(\n    on_epoch_end=lambda epoch, logs: json_log.write(\n        json.dumps({'epoch': epoch, 'loss': logs['loss']}) + '\\n'),\n    on_train_end=lambda logs: json_log.close()\n)\n\n# Terminate some processes after having finished model training.\nprocesses = ...\ncleanup_callback = LambdaCallback(\n    on_train_end=lambda logs: [\n        p.terminate() for p in processes if p.is_alive()])\n\nmodel.fit(...,\n          callbacks=[batch_print_callback,\n                     json_logging_callback,\n                     cleanup_callback])\n```",
                "Functions": [],
                "Name": "LambdaCallback",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "schedule",
                    "verbose"
                ],
                "Defaults": [
                    "0"
                ],
                "DocStr": "Learning rate scheduler.\n\n# Arguments\n    schedule: a function that takes an epoch index as input\n        (integer, indexed from 0) and current learning rate\n        and returns a new learning rate as output (float).\n    verbose: int. 0: quiet, 1: update messages.",
                "Functions": [],
                "Name": "LearningRateScheduler",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "filepath",
                    "monitor",
                    "verbose",
                    "save_best_only",
                    "save_weights_only",
                    "mode",
                    "period"
                ],
                "Defaults": [
                    "'val_loss'",
                    " 0",
                    " False",
                    " False",
                    " 'auto'",
                    " 1"
                ],
                "DocStr": "Save the model after every epoch.\n\n`filepath` can contain named formatting options,\nwhich will be filled the value of `epoch` and\nkeys in `logs` (passed in `on_epoch_end`).\n\nFor example: if `filepath` is `weights.{epoch:02d}-{val_loss:.2f}.hdf5`,\nthen the model checkpoints will be saved with the epoch number and\nthe validation loss in the filename.\n\n# Arguments\n    filepath: string, path to save the model file.\n    monitor: quantity to monitor.\n    verbose: verbosity mode, 0 or 1.\n    save_best_only: if `save_best_only=True`,\n        the latest best model according to\n        the quantity monitored will not be overwritten.\n    mode: one of {auto, min, max}.\n        If `save_best_only=True`, the decision\n        to overwrite the current save file is made\n        based on either the maximization or the\n        minimization of the monitored quantity. For `val_acc`,\n        this should be `max`, for `val_loss` this should\n        be `min`, etc. In `auto` mode, the direction is\n        automatically inferred from the name of the monitored quantity.\n    save_weights_only: if True, then only the model's weights will be\n        saved (`model.save_weights(filepath)`), else the full model\n        is saved (`model.save(filepath)`).\n    period: Interval (number of epochs) between checkpoints.",
                "Functions": [],
                "Name": "ModelCheckpoint",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "DocStr": "Dictionary that remembers insertion order",
                "Functions": [],
                "Name": "OrderedDict",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "target",
                    "width",
                    "verbose",
                    "interval",
                    "stateful_metrics"
                ],
                "Defaults": [
                    "30",
                    " 1",
                    " 0.05",
                    " None"
                ],
                "DocStr": "Displays a progress bar.\n\n# Arguments\n    target: Total number of steps expected, None if unknown.\n    width: Progress bar width on screen.\n    verbose: Verbosity mode, 0 (silent), 1 (verbose), 2 (semi-verbose)\n    stateful_metrics: Iterable of string names of metrics that\n        should *not* be averaged over time. Metrics in this list\n        will be displayed as-is. All others will be averaged\n        by the progbar before display.\n    interval: Minimum visual progress update interval (in seconds).",
                "Functions": [],
                "Name": "Progbar",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "count_mode",
                    "stateful_metrics"
                ],
                "Defaults": [
                    "'samples'",
                    " None"
                ],
                "DocStr": "Callback that prints metrics to stdout.\n\n# Arguments\n    count_mode: One of \"steps\" or \"samples\".\n        Whether the progress bar should\n        count samples seen or steps (batches) seen.\n    stateful_metrics: Iterable of string names of metrics that\n        should *not* be averaged over an epoch.\n        Metrics in this list will be logged as-is.\n        All others will be averaged over time (e.g. loss, etc).\n\n# Raises\n    ValueError: In case of invalid `count_mode`.",
                "Functions": [],
                "Name": "ProgbarLogger",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "monitor",
                    "factor",
                    "patience",
                    "verbose",
                    "mode",
                    "min_delta",
                    "cooldown",
                    "min_lr"
                ],
                "Defaults": [
                    "'val_loss'",
                    " 0.1",
                    " 10",
                    " 0",
                    " 'auto'",
                    " 0.0001",
                    " 0",
                    " 0"
                ],
                "DocStr": "Reduce learning rate when a metric has stopped improving.\n\nModels often benefit from reducing the learning rate by a factor\nof 2-10 once learning stagnates. This callback monitors a\nquantity and if no improvement is seen for a 'patience' number\nof epochs, the learning rate is reduced.\n\n# Example\n\n```python\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=5, min_lr=0.001)\nmodel.fit(X_train, Y_train, callbacks=[reduce_lr])\n```\n\n# Arguments\n    monitor: quantity to be monitored.\n    factor: factor by which the learning rate will\n        be reduced. new_lr = lr * factor\n    patience: number of epochs with no improvement\n        after which learning rate will be reduced.\n    verbose: int. 0: quiet, 1: update messages.\n    mode: one of {auto, min, max}. In `min` mode,\n        lr will be reduced when the quantity\n        monitored has stopped decreasing; in `max`\n        mode it will be reduced when the quantity\n        monitored has stopped increasing; in `auto`\n        mode, the direction is automatically inferred\n        from the name of the monitored quantity.\n    min_delta: threshold for measuring the new optimum,\n        to only focus on significant changes.\n    cooldown: number of epochs to wait before resuming\n        normal operation after lr has been reduced.\n    min_lr: lower bound on the learning rate.",
                "Functions": [],
                "Name": "ReduceLROnPlateau",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "root",
                    "path",
                    "field",
                    "headers",
                    "send_as_json"
                ],
                "Defaults": [
                    "'http://localhost:9000'",
                    " '/publish/epoch/end/'",
                    " 'data'",
                    " None",
                    " False"
                ],
                "DocStr": "Callback used to stream events to a server.\n\nRequires the `requests` library.\nEvents are sent to `root + '/publish/epoch/end/'` by default. Calls are\nHTTP POST, with a `data` argument which is a\nJSON-encoded dictionary of event data.\nIf send_as_json is set to True, the content type of the request will be\napplication/json. Otherwise the serialized JSON will be send within a form\n\n# Arguments\n    root: String; root url of the target server.\n    path: String; path relative to `root` to which the events will be sent.\n    field: String; JSON field under which the data will be stored.\n        The field is used only if the payload is sent within a form\n        (i.e. send_as_json is set to False).\n    headers: Dictionary; optional custom HTTP headers.\n    send_as_json: Boolean; whether the request should be send as\n        application/json.",
                "Functions": [],
                "Name": "RemoteMonitor",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "log_dir",
                    "histogram_freq",
                    "batch_size",
                    "write_graph",
                    "write_grads",
                    "write_images",
                    "embeddings_freq",
                    "embeddings_layer_names",
                    "embeddings_metadata",
                    "embeddings_data",
                    "update_freq"
                ],
                "Defaults": [
                    "'./logs'",
                    " 0",
                    " 32",
                    " True",
                    " False",
                    " False",
                    " 0",
                    " None",
                    " None",
                    " None",
                    " 'epoch'"
                ],
                "DocStr": "TensorBoard basic visualizations.\n\n[TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard)\nis a visualization tool provided with TensorFlow.\n\nThis callback writes a log for TensorBoard, which allows\nyou to visualize dynamic graphs of your training and test\nmetrics, as well as activation histograms for the different\nlayers in your model.\n\nIf you have installed TensorFlow with pip, you should be able\nto launch TensorBoard from the command line:\n```sh\ntensorboard --logdir=/full_path_to_your_logs\n```\n\nWhen using a backend other than TensorFlow, TensorBoard will still work\n(if you have TensorFlow installed), but the only feature available will\nbe the display of the losses and metrics plots.\n\n# Arguments\n    log_dir: the path of the directory where to save the log\n        files to be parsed by TensorBoard.\n    histogram_freq: frequency (in epochs) at which to compute activation\n        and weight histograms for the layers of the model. If set to 0,\n        histograms won't be computed. Validation data (or split) must be\n        specified for histogram visualizations.\n    write_graph: whether to visualize the graph in TensorBoard.\n        The log file can become quite large when\n        write_graph is set to True.\n    write_grads: whether to visualize gradient histograms in TensorBoard.\n        `histogram_freq` must be greater than 0.\n    batch_size: size of batch of inputs to feed to the network\n        for histograms computation.\n    write_images: whether to write model weights to visualize as\n        image in TensorBoard.\n    embeddings_freq: frequency (in epochs) at which selected embedding\n        layers will be saved. If set to 0, embeddings won't be computed.\n        Data to be visualized in TensorBoard's Embedding tab must be passed\n        as `embeddings_data`.\n    embeddings_layer_names: a list of names of layers to keep eye on. If\n        None or empty list all the embedding layer will be watched.\n    embeddings_metadata: a dictionary which maps layer name to a file name\n        in which metadata for this embedding layer is saved. See the\n        [details](https://www.tensorflow.org/how_tos/embedding_viz/#metadata_optional)\n        about metadata files format. In case if the same metadata file is\n        used for all embedding layers, string can be passed.\n    embeddings_data: data to be embedded at layers specified in\n        `embeddings_layer_names`. Numpy array (if the model has a single\n        input) or list of Numpy arrays (if the model has multiple inputs).\n        Learn [more about embeddings]\n        (https://www.tensorflow.org/programmers_guide/embedding).\n    update_freq: `'batch'` or `'epoch'` or integer. When using `'batch'`, writes\n        the losses and metrics to TensorBoard after each batch. The same\n        applies for `'epoch'`. If using an integer, let's say `10000`,\n        the callback will write the metrics and losses to TensorBoard every\n        10000 samples. Note that writing too frequently to TensorBoard\n        can slow down your training.",
                "Functions": [],
                "Name": "TensorBoard",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self"
                ],
                "DocStr": "Callback that terminates training when a NaN loss is encountered.\n    ",
                "Functions": [],
                "Name": "TerminateOnNaN",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "DocStr": "deque([iterable[, maxlen]]) --> deque object\n\nA list-like sequence optimized for data accesses near its endpoints.",
                "Functions": [],
                "Name": "deque",
                "Type": "Class"
            }
        ],
        "Functions": [
            {
                "Args": [
                    "data",
                    "names",
                    "shapes",
                    "check_batch_axis",
                    "exception_prefix"
                ],
                "Defaults": [
                    "None",
                    " True",
                    " ''"
                ],
                "DocStr": "Normalizes inputs and targets provided by users.\n\nUsers may pass data as a list of arrays, dictionary of arrays,\nor as a single array. We normalize this to an ordered list of\narrays (same order as `names`), while checking that the provided\narrays have shapes that match the network's expectations.\n\n# Arguments\n    data: User-provided input data (polymorphic).\n    names: List of expected array names.\n    shapes: Optional list of expected array shapes.\n    check_batch_axis: Boolean; whether to check that\n        the batch axis of the arrays matches the expected\n        value found in `shapes`.\n    exception_prefix: String prefix used for exception formatting.\n\n# Returns\n    List of standardized input arrays (one array per model input).\n\n# Raises\n    ValueError: in case of improperly formatted user-provided data.",
                "Name": "standardize_input_data"
            }
        ],
        "Name": "callbacks",
        "Type": "Module"
    },
    {
        "Classes": [
            {
                "Abstract": false,
                "Args": [],
                "DocStr": null,
                "Functions": [],
                "Name": "Constraint",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "max_value",
                    "axis"
                ],
                "Defaults": [
                    "2",
                    " 0"
                ],
                "DocStr": "MaxNorm weight constraint.\n\nConstrains the weights incident to each hidden unit\nto have a norm less than or equal to a desired value.\n\n# Arguments\n    m: the maximum norm for the incoming weights.\n    axis: integer, axis along which to calculate weight norms.\n        For instance, in a `Dense` layer the weight matrix\n        has shape `(input_dim, output_dim)`,\n        set `axis` to `0` to constrain each weight vector\n        of length `(input_dim,)`.\n        In a `Conv2D` layer with `data_format=\"channels_last\"`,\n        the weight tensor has shape\n        `(rows, cols, input_depth, output_depth)`,\n        set `axis` to `[0, 1, 2]`\n        to constrain the weights of each filter tensor of size\n        `(rows, cols, input_depth)`.\n\n# References\n    - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting]\n      (http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)",
                "Functions": [],
                "Name": "MaxNorm",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "min_value",
                    "max_value",
                    "rate",
                    "axis"
                ],
                "Defaults": [
                    "0.0",
                    " 1.0",
                    " 1.0",
                    " 0"
                ],
                "DocStr": "MinMaxNorm weight constraint.\n\nConstrains the weights incident to each hidden unit\nto have the norm between a lower bound and an upper bound.\n\n# Arguments\n    min_value: the minimum norm for the incoming weights.\n    max_value: the maximum norm for the incoming weights.\n    rate: rate for enforcing the constraint: weights will be\n        rescaled to yield\n        `(1 - rate) * norm + rate * norm.clip(min_value, max_value)`.\n        Effectively, this means that rate=1.0 stands for strict\n        enforcement of the constraint, while rate<1.0 means that\n        weights will be rescaled at each step to slowly move\n        towards a value inside the desired interval.\n    axis: integer, axis along which to calculate weight norms.\n        For instance, in a `Dense` layer the weight matrix\n        has shape `(input_dim, output_dim)`,\n        set `axis` to `0` to constrain each weight vector\n        of length `(input_dim,)`.\n        In a `Conv2D` layer with `data_format=\"channels_last\"`,\n        the weight tensor has shape\n        `(rows, cols, input_depth, output_depth)`,\n        set `axis` to `[0, 1, 2]`\n        to constrain the weights of each filter tensor of size\n        `(rows, cols, input_depth)`.",
                "Functions": [],
                "Name": "MinMaxNorm",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Constrains the weights to be non-negative.\n    ",
                "Functions": [],
                "Name": "NonNeg",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "axis"
                ],
                "Defaults": [
                    "0"
                ],
                "DocStr": "Constrains the weights incident to each hidden unit to have unit norm.\n\n# Arguments\n    axis: integer, axis along which to calculate weight norms.\n        For instance, in a `Dense` layer the weight matrix\n        has shape `(input_dim, output_dim)`,\n        set `axis` to `0` to constrain each weight vector\n        of length `(input_dim,)`.\n        In a `Conv2D` layer with `data_format=\"channels_last\"`,\n        the weight tensor has shape\n        `(rows, cols, input_depth, output_depth)`,\n        set `axis` to `[0, 1, 2]`\n        to constrain the weights of each filter tensor of size\n        `(rows, cols, input_depth)`.",
                "Functions": [],
                "Name": "UnitNorm",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "max_value",
                    "axis"
                ],
                "Defaults": [
                    "2",
                    " 0"
                ],
                "DocStr": "MaxNorm weight constraint.\n\nConstrains the weights incident to each hidden unit\nto have a norm less than or equal to a desired value.\n\n# Arguments\n    m: the maximum norm for the incoming weights.\n    axis: integer, axis along which to calculate weight norms.\n        For instance, in a `Dense` layer the weight matrix\n        has shape `(input_dim, output_dim)`,\n        set `axis` to `0` to constrain each weight vector\n        of length `(input_dim,)`.\n        In a `Conv2D` layer with `data_format=\"channels_last\"`,\n        the weight tensor has shape\n        `(rows, cols, input_depth, output_depth)`,\n        set `axis` to `[0, 1, 2]`\n        to constrain the weights of each filter tensor of size\n        `(rows, cols, input_depth)`.\n\n# References\n    - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting]\n      (http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)",
                "Functions": [],
                "Name": "max_norm",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "max_value",
                    "axis"
                ],
                "Defaults": [
                    "2",
                    " 0"
                ],
                "DocStr": "MaxNorm weight constraint.\n\nConstrains the weights incident to each hidden unit\nto have a norm less than or equal to a desired value.\n\n# Arguments\n    m: the maximum norm for the incoming weights.\n    axis: integer, axis along which to calculate weight norms.\n        For instance, in a `Dense` layer the weight matrix\n        has shape `(input_dim, output_dim)`,\n        set `axis` to `0` to constrain each weight vector\n        of length `(input_dim,)`.\n        In a `Conv2D` layer with `data_format=\"channels_last\"`,\n        the weight tensor has shape\n        `(rows, cols, input_depth, output_depth)`,\n        set `axis` to `[0, 1, 2]`\n        to constrain the weights of each filter tensor of size\n        `(rows, cols, input_depth)`.\n\n# References\n    - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting]\n      (http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)",
                "Functions": [],
                "Name": "maxnorm",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "min_value",
                    "max_value",
                    "rate",
                    "axis"
                ],
                "Defaults": [
                    "0.0",
                    " 1.0",
                    " 1.0",
                    " 0"
                ],
                "DocStr": "MinMaxNorm weight constraint.\n\nConstrains the weights incident to each hidden unit\nto have the norm between a lower bound and an upper bound.\n\n# Arguments\n    min_value: the minimum norm for the incoming weights.\n    max_value: the maximum norm for the incoming weights.\n    rate: rate for enforcing the constraint: weights will be\n        rescaled to yield\n        `(1 - rate) * norm + rate * norm.clip(min_value, max_value)`.\n        Effectively, this means that rate=1.0 stands for strict\n        enforcement of the constraint, while rate<1.0 means that\n        weights will be rescaled at each step to slowly move\n        towards a value inside the desired interval.\n    axis: integer, axis along which to calculate weight norms.\n        For instance, in a `Dense` layer the weight matrix\n        has shape `(input_dim, output_dim)`,\n        set `axis` to `0` to constrain each weight vector\n        of length `(input_dim,)`.\n        In a `Conv2D` layer with `data_format=\"channels_last\"`,\n        the weight tensor has shape\n        `(rows, cols, input_depth, output_depth)`,\n        set `axis` to `[0, 1, 2]`\n        to constrain the weights of each filter tensor of size\n        `(rows, cols, input_depth)`.",
                "Functions": [],
                "Name": "min_max_norm",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Constrains the weights to be non-negative.\n    ",
                "Functions": [],
                "Name": "non_neg",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Constrains the weights to be non-negative.\n    ",
                "Functions": [],
                "Name": "nonneg",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "axis"
                ],
                "Defaults": [
                    "0"
                ],
                "DocStr": "Constrains the weights incident to each hidden unit to have unit norm.\n\n# Arguments\n    axis: integer, axis along which to calculate weight norms.\n        For instance, in a `Dense` layer the weight matrix\n        has shape `(input_dim, output_dim)`,\n        set `axis` to `0` to constrain each weight vector\n        of length `(input_dim,)`.\n        In a `Conv2D` layer with `data_format=\"channels_last\"`,\n        the weight tensor has shape\n        `(rows, cols, input_depth, output_depth)`,\n        set `axis` to `[0, 1, 2]`\n        to constrain the weights of each filter tensor of size\n        `(rows, cols, input_depth)`.",
                "Functions": [],
                "Name": "unit_norm",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "axis"
                ],
                "Defaults": [
                    "0"
                ],
                "DocStr": "Constrains the weights incident to each hidden unit to have unit norm.\n\n# Arguments\n    axis: integer, axis along which to calculate weight norms.\n        For instance, in a `Dense` layer the weight matrix\n        has shape `(input_dim, output_dim)`,\n        set `axis` to `0` to constrain each weight vector\n        of length `(input_dim,)`.\n        In a `Conv2D` layer with `data_format=\"channels_last\"`,\n        the weight tensor has shape\n        `(rows, cols, input_depth, output_depth)`,\n        set `axis` to `[0, 1, 2]`\n        to constrain the weights of each filter tensor of size\n        `(rows, cols, input_depth)`.",
                "Functions": [],
                "Name": "unitnorm",
                "Type": "Class"
            }
        ],
        "Functions": [
            {
                "Args": [
                    "config",
                    "custom_objects"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": null,
                "Name": "deserialize"
            },
            {
                "Args": [
                    "identifier",
                    "module_objects",
                    "custom_objects",
                    "printable_module_name"
                ],
                "Defaults": [
                    "None",
                    " None",
                    " 'object'"
                ],
                "DocStr": null,
                "Name": "deserialize_keras_object"
            },
            {
                "Args": [
                    "identifier"
                ],
                "DocStr": null,
                "Name": "get"
            },
            {
                "Args": [
                    "constraint"
                ],
                "DocStr": null,
                "Name": "serialize"
            },
            {
                "Args": [
                    "instance"
                ],
                "DocStr": null,
                "Name": "serialize_keras_object"
            }
        ],
        "Name": "constraints",
        "Type": "Module"
    },
    {
        "Classes": [],
        "Functions": [],
        "Name": "datasets",
        "Type": "Module"
    },
    {
        "Classes": [
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Layer to be used as an entry point into a model.\n\nIt can either wrap an existing tensor (pass an `input_tensor` argument)\nor create its a placeholder tensor (pass arguments `input_shape`\nor `batch_input_shape` as well as `dtype`).\n\n# Arguments\n    input_shape: Shape tuple, not including the batch axis.\n    batch_size: Optional input batch size (integer or None).\n    batch_input_shape: Shape tuple, including the batch axis.\n    dtype: Datatype of the input.\n    input_tensor: Optional tensor to use as layer input\n        instead of creating a placeholder.\n    sparse: Boolean, whether the placeholder created\n        is meant to be sparse.\n    name: Name of the layer (string).",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "InputLayer",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "dtype",
                    "shape",
                    "ndim",
                    "max_ndim",
                    "min_ndim",
                    "axes"
                ],
                "Defaults": [
                    "None",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None"
                ],
                "DocStr": "Specifies the ndim, dtype and shape of every input to a layer.\n\nEvery layer should expose (if appropriate) an `input_spec` attribute:\na list of instances of InputSpec (one per input tensor).\n\nA None entry in a shape is compatible with any dimension,\na None shape is compatible with any shape.\n\n# Arguments\n    dtype: Expected datatype of the input.\n    shape: Shape tuple, expected shape of the input\n        (may include None for unchecked axes).\n    ndim: Integer, expected rank of the input.\n    max_ndim: Integer, maximum rank of the input.\n    min_ndim: Integer, minimum rank of the input.\n    axes: Dictionary mapping integer axes to\n        a specific dimension value.",
                "Functions": [],
                "Name": "InputSpec",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self"
                ],
                "DocStr": "Abstract base layer class.\n\n# Properties\n    input, output: Input/output tensor(s). Note that if the layer\n        is used more than once (shared layer), this is ill-defined\n        and will raise an exception. In such cases, use\n        `layer.get_input_at(node_index)`.\n    input_mask, output_mask: Mask tensors. Same caveats apply as\n        input, output.\n    input_shape: Shape tuple. Provided for convenience, but note\n        that there may be cases in which this attribute is\n        ill-defined (e.g. a shared layer with multiple input\n        shapes), in which case requesting `input_shape` will raise\n        an Exception. Prefer using\n        `layer.get_input_shape_at(node_index)`.\n    input_spec: List of InputSpec class instances\n        each entry describes one required input:\n            - ndim\n            - dtype\n        A layer with `n` input tensors must have\n        an `input_spec` of length `n`.\n    name: String, must be unique within a model.\n    non_trainable_weights: List of variables.\n    output_shape: Shape tuple. See `input_shape`.\n    stateful: Boolean indicating whether the layer carries\n        additional non-weight state. Used in, for instance, RNN\n        cells to carry information between batches.\n    supports_masking: Boolean indicator of whether the layer\n        supports masking, typically for unused timesteps in a\n        sequence.\n    trainable: Boolean, whether the layer weights\n        will be updated during training.\n    trainable_weights: List of variables.\n    uses_learning_phase: Whether any operation\n        of the layer uses `K.in_training_phase()`\n        or `K.in_test_phase()`.\n    weights: The concatenation of the lists trainable_weights and\n        non_trainable_weights (in this order).\n\n\n# Methods\n    call(x, mask=None): Where the layer's logic lives.\n    __call__(x, mask=None): Wrapper around the layer logic (`call`).\n        If x is a Keras tensor:\n            - Connect current layer with last layer from tensor:\n                `self._add_inbound_node(last_layer)`\n            - Add layer to tensor history\n        If layer is not built:\n            - Build from x._keras_shape\n    compute_mask(x, mask)\n    compute_output_shape(input_shape)\n    count_params()\n    get_config()\n    get_input_at(node_index)\n    get_input_mask_at(node_index)\n    get_input_shape_at(node_index)\n    get_output_at(node_index)\n    get_output_mask_at(node_index)\n    get_output_shape_at(node_index)\n    get_weights()\n    set_weights(weights)\n\n# Class Methods\n    from_config(config)\n\n# Internal methods:\n    _add_inbound_node(layer, index=0)\n    assert_input_compatibility()\n    build(input_shape)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Layer",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "The `Model` class adds training & evaluation routines to a `Network`.\n    ",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config",
                            "custom_objects"
                        ],
                        "Defaults": [
                            "None"
                        ],
                        "DocStr": "Instantiates a Model from its config (output of `get_config()`).\n\n# Arguments\n    config: Model config dictionary.\n    custom_objects: Optional dictionary mapping names\n        (strings) to custom classes or functions to be\n        considered during deserialization.\n\n# Returns\n    A model instance.\n\n# Raises\n    ValueError: In case of improperly formatted config dict.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Model",
                "Type": "Class"
            }
        ],
        "Functions": [
            {
                "Args": [
                    "shape",
                    "batch_shape",
                    "name",
                    "dtype",
                    "sparse",
                    "tensor"
                ],
                "Defaults": [
                    "None",
                    " None",
                    " None",
                    " None",
                    " False",
                    " None"
                ],
                "DocStr": "`Input()` is used to instantiate a Keras tensor.\n\nA Keras tensor is a tensor object from the underlying backend\n(Theano, TensorFlow or CNTK), which we augment with certain\nattributes that allow us to build a Keras model\njust by knowing the inputs and outputs of the model.\n\nFor instance, if a, b and c are Keras tensors,\nit becomes possible to do:\n`model = Model(input=[a, b], output=c)`\n\nThe added Keras attributes are:\n    `_keras_shape`: Integer shape tuple propagated\n        via Keras-side shape inference.\n    `_keras_history`: Last layer applied to the tensor.\n        the entire layer graph is retrievable from that layer,\n        recursively.\n\n# Arguments\n    shape: A shape tuple (integer), not including the batch size.\n        For instance, `shape=(32,)` indicates that the expected input\n        will be batches of 32-dimensional vectors.\n    batch_shape: A shape tuple (integer), including the batch size.\n        For instance, `batch_shape=(10, 32)` indicates that\n        the expected input will be batches of 10 32-dimensional vectors.\n        `batch_shape=(None, 32)` indicates batches of an arbitrary number\n        of 32-dimensional vectors.\n    name: An optional name string for the layer.\n        Should be unique in a model (do not reuse the same name twice).\n        It will be autogenerated if it isn't provided.\n    dtype: The data type expected by the input, as a string\n        (`float32`, `float64`, `int32`...)\n    sparse: A boolean specifying whether the placeholder\n        to be created is sparse.\n    tensor: Optional existing tensor to wrap into the `Input` layer.\n        If set, the layer will not create a placeholder tensor.\n\n# Returns\n    A tensor.\n\n# Example\n\n```python\n# this is a logistic regression in Keras\nx = Input(shape=(32,))\ny = Dense(16, activation='softmax')(x)\nmodel = Model(x, y)\n```",
                "Name": "Input"
            },
            {
                "Args": [
                    "tensor",
                    "layer",
                    "node_index"
                ],
                "Defaults": [
                    "None",
                    " None"
                ],
                "DocStr": "Returns the list of input tensors necessary to compute `tensor`.\n\nOutput will always be a list of tensors\n(potentially with 1 element).\n\n# Arguments\n    tensor: The tensor to start from.\n    layer: Origin layer of the tensor. Will be\n        determined via tensor._keras_history if not provided.\n    node_index: Origin node index of the tensor.\n\n# Returns\n    List of input tensors.",
                "Name": "get_source_inputs"
            }
        ],
        "Name": "engine",
        "Type": "Module"
    },
    {
        "Classes": [
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "value"
                ],
                "Defaults": [
                    "0"
                ],
                "DocStr": "Initializer that generates tensors initialized to a constant value.\n\n# Arguments\n    value: float; the value of the generator tensors.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "Constant",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "gain"
                ],
                "Defaults": [
                    "1.0"
                ],
                "DocStr": "Initializer that generates the identity matrix.\n\nOnly use for 2D matrices.\nIf the long side of the matrix is a multiple of the short side,\nmultiple identity matrices are concatenated along the long side.\n\n# Arguments\n    gain: Multiplicative factor to apply to the identity matrix.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "Identity",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Initializer base class: all initializers inherit from this class.\n    ",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "Initializer",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Initializer that generates tensors initialized to 1.\n    ",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "Ones",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "gain",
                    "seed"
                ],
                "Defaults": [
                    "1.0",
                    " None"
                ],
                "DocStr": "Initializer that generates a random orthogonal matrix.\n\n# Arguments\n    gain: Multiplicative factor to apply to the orthogonal matrix.\n    seed: A Python integer. Used to seed the random generator.\n\n# References\n    Saxe et al., http://arxiv.org/abs/1312.6120",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "Orthogonal",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "mean",
                    "stddev",
                    "seed"
                ],
                "Defaults": [
                    "0.0",
                    " 0.05",
                    " None"
                ],
                "DocStr": "Initializer that generates tensors with a normal distribution.\n\n# Arguments\n    mean: a python scalar or a scalar tensor. Mean of the random values\n      to generate.\n    stddev: a python scalar or a scalar tensor. Standard deviation of the\n      random values to generate.\n    seed: A Python integer. Used to seed the random generator.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "RandomNormal",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "minval",
                    "maxval",
                    "seed"
                ],
                "Defaults": [
                    "-0.05",
                    " 0.05",
                    " None"
                ],
                "DocStr": "Initializer that generates tensors with a uniform distribution.\n\n# Arguments\n    minval: A python scalar or a scalar tensor. Lower bound of the range\n      of random values to generate.\n    maxval: A python scalar or a scalar tensor. Upper bound of the range\n      of random values to generate.  Defaults to 1 for float types.\n    seed: A Python integer. Used to seed the random generator.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "RandomUniform",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "mean",
                    "stddev",
                    "seed"
                ],
                "Defaults": [
                    "0.0",
                    " 0.05",
                    " None"
                ],
                "DocStr": "Initializer that generates a truncated normal distribution.\n\nThese values are similar to values from a `RandomNormal`\nexcept that values more than two standard deviations from the mean\nare discarded and re-drawn. This is the recommended initializer for\nneural network weights and filters.\n\n# Arguments\n    mean: a python scalar or a scalar tensor. Mean of the random values\n      to generate.\n    stddev: a python scalar or a scalar tensor. Standard deviation of the\n      random values to generate.\n    seed: A Python integer. Used to seed the random generator.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "TruncatedNormal",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "scale",
                    "mode",
                    "distribution",
                    "seed"
                ],
                "Defaults": [
                    "1.0",
                    " 'fan_in'",
                    " 'normal'",
                    " None"
                ],
                "DocStr": "Initializer capable of adapting its scale to the shape of weights.\n\nWith `distribution=\"normal\"`, samples are drawn from a truncated normal\ndistribution centered on zero, with `stddev = sqrt(scale / n)` where n is:\n\n    - number of input units in the weight tensor, if mode = \"fan_in\"\n    - number of output units, if mode = \"fan_out\"\n    - average of the numbers of input and output units, if mode = \"fan_avg\"\n\nWith `distribution=\"uniform\"`,\nsamples are drawn from a uniform distribution\nwithin [-limit, limit], with `limit = sqrt(3 * scale / n)`.\n\n# Arguments\n    scale: Scaling factor (positive float).\n    mode: One of \"fan_in\", \"fan_out\", \"fan_avg\".\n    distribution: Random distribution to use. One of \"normal\", \"uniform\".\n    seed: A Python integer. Used to seed the random generator.\n\n# Raises\n    ValueError: In case of an invalid value for the \"scale\", mode\" or\n      \"distribution\" arguments.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "VarianceScaling",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Initializer that generates tensors initialized to 0.\n    ",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "Zeros",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "value"
                ],
                "Defaults": [
                    "0"
                ],
                "DocStr": "Initializer that generates tensors initialized to a constant value.\n\n# Arguments\n    value: float; the value of the generator tensors.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "constant",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "gain"
                ],
                "Defaults": [
                    "1.0"
                ],
                "DocStr": "Initializer that generates the identity matrix.\n\nOnly use for 2D matrices.\nIf the long side of the matrix is a multiple of the short side,\nmultiple identity matrices are concatenated along the long side.\n\n# Arguments\n    gain: Multiplicative factor to apply to the identity matrix.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "identity",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "mean",
                    "stddev",
                    "seed"
                ],
                "Defaults": [
                    "0.0",
                    " 0.05",
                    " None"
                ],
                "DocStr": "Initializer that generates tensors with a normal distribution.\n\n# Arguments\n    mean: a python scalar or a scalar tensor. Mean of the random values\n      to generate.\n    stddev: a python scalar or a scalar tensor. Standard deviation of the\n      random values to generate.\n    seed: A Python integer. Used to seed the random generator.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "normal",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Initializer that generates tensors initialized to 1.\n    ",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "one",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Initializer that generates tensors initialized to 1.\n    ",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "ones",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "gain",
                    "seed"
                ],
                "Defaults": [
                    "1.0",
                    " None"
                ],
                "DocStr": "Initializer that generates a random orthogonal matrix.\n\n# Arguments\n    gain: Multiplicative factor to apply to the orthogonal matrix.\n    seed: A Python integer. Used to seed the random generator.\n\n# References\n    Saxe et al., http://arxiv.org/abs/1312.6120",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "orthogonal",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "mean",
                    "stddev",
                    "seed"
                ],
                "Defaults": [
                    "0.0",
                    " 0.05",
                    " None"
                ],
                "DocStr": "Initializer that generates tensors with a normal distribution.\n\n# Arguments\n    mean: a python scalar or a scalar tensor. Mean of the random values\n      to generate.\n    stddev: a python scalar or a scalar tensor. Standard deviation of the\n      random values to generate.\n    seed: A Python integer. Used to seed the random generator.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "random_normal",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "minval",
                    "maxval",
                    "seed"
                ],
                "Defaults": [
                    "-0.05",
                    " 0.05",
                    " None"
                ],
                "DocStr": "Initializer that generates tensors with a uniform distribution.\n\n# Arguments\n    minval: A python scalar or a scalar tensor. Lower bound of the range\n      of random values to generate.\n    maxval: A python scalar or a scalar tensor. Upper bound of the range\n      of random values to generate.  Defaults to 1 for float types.\n    seed: A Python integer. Used to seed the random generator.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "random_uniform",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "mean",
                    "stddev",
                    "seed"
                ],
                "Defaults": [
                    "0.0",
                    " 0.05",
                    " None"
                ],
                "DocStr": "Initializer that generates a truncated normal distribution.\n\nThese values are similar to values from a `RandomNormal`\nexcept that values more than two standard deviations from the mean\nare discarded and re-drawn. This is the recommended initializer for\nneural network weights and filters.\n\n# Arguments\n    mean: a python scalar or a scalar tensor. Mean of the random values\n      to generate.\n    stddev: a python scalar or a scalar tensor. Standard deviation of the\n      random values to generate.\n    seed: A Python integer. Used to seed the random generator.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "truncated_normal",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "minval",
                    "maxval",
                    "seed"
                ],
                "Defaults": [
                    "-0.05",
                    " 0.05",
                    " None"
                ],
                "DocStr": "Initializer that generates tensors with a uniform distribution.\n\n# Arguments\n    minval: A python scalar or a scalar tensor. Lower bound of the range\n      of random values to generate.\n    maxval: A python scalar or a scalar tensor. Upper bound of the range\n      of random values to generate.  Defaults to 1 for float types.\n    seed: A Python integer. Used to seed the random generator.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "uniform",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Initializer that generates tensors initialized to 0.\n    ",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "zero",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Initializer that generates tensors initialized to 0.\n    ",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "zeros",
                "Type": "Class"
            }
        ],
        "Functions": [
            {
                "Args": [
                    "shape",
                    "data_format"
                ],
                "Defaults": [
                    "'channels_last'"
                ],
                "DocStr": "Computes the number of input and output units for a weight shape.\n\n# Arguments\n    shape: Integer shape tuple.\n    data_format: Image data format to use for convolution kernels.\n        Note that all kernels in Keras are standardized on the\n        `channels_last` ordering (even when inputs are set\n        to `channels_first`).\n\n# Returns\n    A tuple of scalars, `(fan_in, fan_out)`.\n\n# Raises\n    ValueError: in case of invalid `data_format` argument.",
                "Name": "_compute_fans"
            },
            {
                "Args": [
                    "config",
                    "custom_objects"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": null,
                "Name": "deserialize"
            },
            {
                "Args": [
                    "identifier",
                    "module_objects",
                    "custom_objects",
                    "printable_module_name"
                ],
                "Defaults": [
                    "None",
                    " None",
                    " 'object'"
                ],
                "DocStr": null,
                "Name": "deserialize_keras_object"
            },
            {
                "Args": [
                    "identifier"
                ],
                "DocStr": null,
                "Name": "get"
            },
            {
                "Args": [
                    "seed"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": "Glorot normal initializer, also called Xavier normal initializer.\n\nIt draws samples from a truncated normal distribution centered on 0\nwith `stddev = sqrt(2 / (fan_in + fan_out))`\nwhere `fan_in` is the number of input units in the weight tensor\nand `fan_out` is the number of output units in the weight tensor.\n\n# Arguments\n    seed: A Python integer. Used to seed the random generator.\n\n# Returns\n    An initializer.\n\n# References\n    Glorot & Bengio, AISTATS 2010\n    http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf",
                "Name": "glorot_normal"
            },
            {
                "Args": [
                    "seed"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": "Glorot uniform initializer, also called Xavier uniform initializer.\n\nIt draws samples from a uniform distribution within [-limit, limit]\nwhere `limit` is `sqrt(6 / (fan_in + fan_out))`\nwhere `fan_in` is the number of input units in the weight tensor\nand `fan_out` is the number of output units in the weight tensor.\n\n# Arguments\n    seed: A Python integer. Used to seed the random generator.\n\n# Returns\n    An initializer.\n\n# References\n    Glorot & Bengio, AISTATS 2010\n    http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf",
                "Name": "glorot_uniform"
            },
            {
                "Args": [
                    "seed"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": "He normal initializer.\n\nIt draws samples from a truncated normal distribution centered on 0\nwith `stddev = sqrt(2 / fan_in)`\nwhere `fan_in` is the number of input units in the weight tensor.\n\n# Arguments\n    seed: A Python integer. Used to seed the random generator.\n\n# Returns\n    An initializer.\n\n# References\n    He et al., http://arxiv.org/abs/1502.01852",
                "Name": "he_normal"
            },
            {
                "Args": [
                    "seed"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": "He uniform variance scaling initializer.\n\nIt draws samples from a uniform distribution within [-limit, limit]\nwhere `limit` is `sqrt(6 / fan_in)`\nwhere `fan_in` is the number of input units in the weight tensor.\n\n# Arguments\n    seed: A Python integer. Used to seed the random generator.\n\n# Returns\n    An initializer.\n\n# References\n    He et al., http://arxiv.org/abs/1502.01852",
                "Name": "he_uniform"
            },
            {
                "Args": [
                    "seed"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": "LeCun normal initializer.\n\nIt draws samples from a truncated normal distribution centered on 0\nwith `stddev = sqrt(1 / fan_in)`\nwhere `fan_in` is the number of input units in the weight tensor.\n\n# Arguments\n    seed: A Python integer. Used to seed the random generator.\n\n# Returns\n    An initializer.\n\n# References\n    - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n    - [Efficient Backprop](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)",
                "Name": "lecun_normal"
            },
            {
                "Args": [
                    "seed"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": "LeCun uniform initializer.\n\nIt draws samples from a uniform distribution within [-limit, limit]\nwhere `limit` is `sqrt(3 / fan_in)`\nwhere `fan_in` is the number of input units in the weight tensor.\n\n# Arguments\n    seed: A Python integer. Used to seed the random generator.\n\n# Returns\n    An initializer.\n\n# References\n    LeCun 98, Efficient Backprop,\n    http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf",
                "Name": "lecun_uniform"
            },
            {
                "Args": [
                    "initializer"
                ],
                "DocStr": null,
                "Name": "serialize"
            },
            {
                "Args": [
                    "instance"
                ],
                "DocStr": null,
                "Name": "serialize_keras_object"
            }
        ],
        "Name": "initializers",
        "Type": "Module"
    },
    {
        "Classes": [
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "activation"
                ],
                "DocStr": "Applies an activation function to an output.\n\n# Arguments\n    activation: name of activation function to use\n        (see: [activations](../activations.md)),\n        or alternatively, a Theano or TensorFlow operation.\n\n# Input shape\n    Arbitrary. Use the keyword argument `input_shape`\n    (tuple of integers, does not include the samples axis)\n    when using this layer as the first layer in a model.\n\n# Output shape\n    Same shape as input.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Activation",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "l1",
                    "l2"
                ],
                "Defaults": [
                    "0.0",
                    " 0.0"
                ],
                "DocStr": "Layer that applies an update to the cost function based input activity.\n\n# Arguments\n    l1: L1 regularization factor (positive float).\n    l2: L2 regularization factor (positive float).\n\n# Input shape\n    Arbitrary. Use the keyword argument `input_shape`\n    (tuple of integers, does not include the samples axis)\n    when using this layer as the first layer in a model.\n\n# Output shape\n    Same shape as input.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "ActivityRegularization",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self"
                ],
                "DocStr": "Layer that adds a list of inputs.\n\nIt takes as input a list of tensors,\nall of the same shape, and returns\na single tensor (also of the same shape).\n\n# Examples\n\n```python\n    import keras\n\n    input1 = keras.layers.Input(shape=(16,))\n    x1 = keras.layers.Dense(8, activation='relu')(input1)\n    input2 = keras.layers.Input(shape=(32,))\n    x2 = keras.layers.Dense(8, activation='relu')(input2)\n    # equivalent to added = keras.layers.add([x1, x2])\n    added = keras.layers.Add()([x1, x2])\n\n    out = keras.layers.Dense(4)(added)\n    model = keras.models.Model(inputs=[input1, input2], outputs=out)\n```",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Add",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "rate",
                    "noise_shape",
                    "seed"
                ],
                "Defaults": [
                    "None",
                    " None"
                ],
                "DocStr": "Applies Alpha Dropout to the input.\n\nAlpha Dropout is a `Dropout` that keeps mean and variance of inputs\nto their original values, in order to ensure the self-normalizing property\neven after this dropout.\nAlpha Dropout fits well to Scaled Exponential Linear Units\nby randomly setting activations to the negative saturation value.\n\n# Arguments\n    rate: float, drop probability (as with `Dropout`).\n        The multiplicative noise will have\n        standard deviation `sqrt(rate / (1 - rate))`.\n    seed: A Python integer to use as random seed.\n\n# Input shape\n    Arbitrary. Use the keyword argument `input_shape`\n    (tuple of integers, does not include the samples axis)\n    when using this layer as the first layer in a model.\n\n# Output shape\n    Same shape as input.\n\n# References\n    - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "AlphaDropout",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self"
                ],
                "DocStr": "Layer that averages a list of inputs.\n\nIt takes as input a list of tensors,\nall of the same shape, and returns\na single tensor (also of the same shape).",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Average",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Average pooling for temporal data.\n\n# Arguments\n    pool_size: Integer, size of the average pooling windows.\n    strides: Integer, or None. Factor by which to downscale.\n        E.g. 2 will halve the input.\n        If None, it will default to `pool_size`.\n    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, steps, features)` while `channels_first`\n        corresponds to inputs with shape\n        `(batch, features, steps)`.\n\n# Input shape\n    - If `data_format='channels_last'`:\n        3D tensor with shape:\n        `(batch_size, steps, features)`\n    - If `data_format='channels_first'`:\n        3D tensor with shape:\n        `(batch_size, features, steps)`\n\n# Output shape\n    - If `data_format='channels_last'`:\n        3D tensor with shape:\n        `(batch_size, downsampled_steps, features)`\n    - If `data_format='channels_first'`:\n        3D tensor with shape:\n        `(batch_size, features, downsampled_steps)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "AveragePooling1D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Average pooling operation for spatial data.\n\n# Arguments\n    pool_size: integer or tuple of 2 integers,\n        factors by which to downscale (vertical, horizontal).\n        (2, 2) will halve the input in both spatial dimension.\n        If only one integer is specified, the same window length\n        will be used for both dimensions.\n    strides: Integer, tuple of 2 integers, or None.\n        Strides values.\n        If None, it will default to `pool_size`.\n    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, height, width, channels)` while `channels_first`\n        corresponds to inputs with shape\n        `(batch, channels, height, width)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n\n# Input shape\n    - If `data_format='channels_last'`:\n        4D tensor with shape:\n        `(batch_size, rows, cols, channels)`\n    - If `data_format='channels_first'`:\n        4D tensor with shape:\n        `(batch_size, channels, rows, cols)`\n\n# Output shape\n    - If `data_format='channels_last'`:\n        4D tensor with shape:\n        `(batch_size, pooled_rows, pooled_cols, channels)`\n    - If `data_format='channels_first'`:\n        4D tensor with shape:\n        `(batch_size, channels, pooled_rows, pooled_cols)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "AveragePooling2D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Average pooling operation for 3D data (spatial or spatio-temporal).\n\n# Arguments\n    pool_size: tuple of 3 integers,\n        factors by which to downscale (dim1, dim2, dim3).\n        (2, 2, 2) will halve the size of the 3D input in each dimension.\n    strides: tuple of 3 integers, or None. Strides values.\n    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n        while `channels_first` corresponds to inputs with shape\n        `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n\n# Input shape\n    - If `data_format='channels_last'`:\n        5D tensor with shape:\n        `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n    - If `data_format='channels_first'`:\n        5D tensor with shape:\n        `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`\n\n# Output shape\n    - If `data_format='channels_last'`:\n        5D tensor with shape:\n        `(batch_size, pooled_dim1, pooled_dim2, pooled_dim3, channels)`\n    - If `data_format='channels_first'`:\n        5D tensor with shape:\n        `(batch_size, channels, pooled_dim1, pooled_dim2, pooled_dim3)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "AveragePooling3D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Average pooling for temporal data.\n\n# Arguments\n    pool_size: Integer, size of the average pooling windows.\n    strides: Integer, or None. Factor by which to downscale.\n        E.g. 2 will halve the input.\n        If None, it will default to `pool_size`.\n    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, steps, features)` while `channels_first`\n        corresponds to inputs with shape\n        `(batch, features, steps)`.\n\n# Input shape\n    - If `data_format='channels_last'`:\n        3D tensor with shape:\n        `(batch_size, steps, features)`\n    - If `data_format='channels_first'`:\n        3D tensor with shape:\n        `(batch_size, features, steps)`\n\n# Output shape\n    - If `data_format='channels_last'`:\n        3D tensor with shape:\n        `(batch_size, downsampled_steps, features)`\n    - If `data_format='channels_first'`:\n        3D tensor with shape:\n        `(batch_size, features, downsampled_steps)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "AvgPool1D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Average pooling operation for spatial data.\n\n# Arguments\n    pool_size: integer or tuple of 2 integers,\n        factors by which to downscale (vertical, horizontal).\n        (2, 2) will halve the input in both spatial dimension.\n        If only one integer is specified, the same window length\n        will be used for both dimensions.\n    strides: Integer, tuple of 2 integers, or None.\n        Strides values.\n        If None, it will default to `pool_size`.\n    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, height, width, channels)` while `channels_first`\n        corresponds to inputs with shape\n        `(batch, channels, height, width)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n\n# Input shape\n    - If `data_format='channels_last'`:\n        4D tensor with shape:\n        `(batch_size, rows, cols, channels)`\n    - If `data_format='channels_first'`:\n        4D tensor with shape:\n        `(batch_size, channels, rows, cols)`\n\n# Output shape\n    - If `data_format='channels_last'`:\n        4D tensor with shape:\n        `(batch_size, pooled_rows, pooled_cols, channels)`\n    - If `data_format='channels_first'`:\n        4D tensor with shape:\n        `(batch_size, channels, pooled_rows, pooled_cols)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "AvgPool2D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Average pooling operation for 3D data (spatial or spatio-temporal).\n\n# Arguments\n    pool_size: tuple of 3 integers,\n        factors by which to downscale (dim1, dim2, dim3).\n        (2, 2, 2) will halve the size of the 3D input in each dimension.\n    strides: tuple of 3 integers, or None. Strides values.\n    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n        while `channels_first` corresponds to inputs with shape\n        `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n\n# Input shape\n    - If `data_format='channels_last'`:\n        5D tensor with shape:\n        `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n    - If `data_format='channels_first'`:\n        5D tensor with shape:\n        `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`\n\n# Output shape\n    - If `data_format='channels_last'`:\n        5D tensor with shape:\n        `(batch_size, pooled_dim1, pooled_dim2, pooled_dim3, channels)`\n    - If `data_format='channels_first'`:\n        5D tensor with shape:\n        `(batch_size, channels, pooled_dim1, pooled_dim2, pooled_dim3)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "AvgPool3D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Batch normalization layer (Ioffe and Szegedy, 2014).\n\nNormalize the activations of the previous layer at each batch,\ni.e. applies a transformation that maintains the mean activation\nclose to 0 and the activation standard deviation close to 1.\n\n# Arguments\n    axis: Integer, the axis that should be normalized\n        (typically the features axis).\n        For instance, after a `Conv2D` layer with\n        `data_format=\"channels_first\"`,\n        set `axis=1` in `BatchNormalization`.\n    momentum: Momentum for the moving mean and the moving variance.\n    epsilon: Small float added to variance to avoid dividing by zero.\n    center: If True, add offset of `beta` to normalized tensor.\n        If False, `beta` is ignored.\n    scale: If True, multiply by `gamma`.\n        If False, `gamma` is not used.\n        When the next layer is linear (also e.g. `nn.relu`),\n        this can be disabled since the scaling\n        will be done by the next layer.\n    beta_initializer: Initializer for the beta weight.\n    gamma_initializer: Initializer for the gamma weight.\n    moving_mean_initializer: Initializer for the moving mean.\n    moving_variance_initializer: Initializer for the moving variance.\n    beta_regularizer: Optional regularizer for the beta weight.\n    gamma_regularizer: Optional regularizer for the gamma weight.\n    beta_constraint: Optional constraint for the beta weight.\n    gamma_constraint: Optional constraint for the gamma weight.\n\n# Input shape\n    Arbitrary. Use the keyword argument `input_shape`\n    (tuple of integers, does not include the samples axis)\n    when using this layer as the first layer in a model.\n\n# Output shape\n    Same shape as input.\n\n# References\n    - [Batch Normalization: Accelerating Deep Network Training by\n       Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "BatchNormalization",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "layer",
                    "merge_mode",
                    "weights"
                ],
                "Defaults": [
                    "'concat'",
                    " None"
                ],
                "DocStr": "Bidirectional wrapper for RNNs.\n\n# Arguments\n    layer: `Recurrent` instance.\n    merge_mode: Mode by which outputs of the\n        forward and backward RNNs will be combined.\n        One of {'sum', 'mul', 'concat', 'ave', None}.\n        If None, the outputs will not be combined,\n        they will be returned as a list.\n\n# Raises\n    ValueError: In case of invalid `merge_mode` argument.\n\n# Examples\n\n```python\n    model = Sequential()\n    model.add(Bidirectional(LSTM(10, return_sequences=True),\n                            input_shape=(5, 10)))\n    model.add(Bidirectional(LSTM(10)))\n    model.add(Dense(5))\n    model.add(Activation('softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n```",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config",
                            "custom_objects"
                        ],
                        "Defaults": [
                            "None"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Bidirectional",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "axis"
                ],
                "Defaults": [
                    "-1"
                ],
                "DocStr": "Layer that concatenates a list of inputs.\n\nIt takes as input a list of tensors,\nall of the same shape except for the concatenation axis,\nand returns a single tensor, the concatenation of all inputs.\n\n# Arguments\n    axis: Axis along which to concatenate.\n    **kwargs: standard layer keyword arguments.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Concatenate",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "1D convolution layer (e.g. temporal convolution).\n\nThis layer creates a convolution kernel that is convolved\nwith the layer input over a single spatial (or temporal) dimension\nto produce a tensor of outputs.\nIf `use_bias` is True, a bias vector is created and added to the outputs.\nFinally, if `activation` is not `None`,\nit is applied to the outputs as well.\n\nWhen using this layer as the first layer in a model,\nprovide an `input_shape` argument\n(tuple of integers or `None`, e.g.\n`(10, 128)` for sequences of 10 vectors of 128-dimensional vectors,\nor `(None, 128)` for variable-length sequences of 128-dimensional vectors.\n\n# Arguments\n    filters: Integer, the dimensionality of the output space\n        (i.e. the number of output filters in the convolution).\n    kernel_size: An integer or tuple/list of a single integer,\n        specifying the length of the 1D convolution window.\n    strides: An integer or tuple/list of a single integer,\n        specifying the stride length of the convolution.\n        Specifying any stride value != 1 is incompatible with specifying\n        any `dilation_rate` value != 1.\n    padding: One of `\"valid\"`, `\"causal\"` or `\"same\"` (case-insensitive).\n        `\"valid\"` means \"no padding\".\n        `\"same\"` results in padding the input such that\n        the output has the same length as the original input.\n        `\"causal\"` results in causal (dilated) convolutions,\n        e.g. `output[t]` does not depend on `input[t + 1:]`.\n        A zero padding is used such that\n        the output has the same length as the original input.\n        Useful when modeling temporal data where the model\n        should not violate the temporal order. See\n        [WaveNet: A Generative Model for Raw Audio, section 2.1]\n        (https://arxiv.org/abs/1609.03499).\n    data_format: A string,\n        one of `\"channels_last\"` (default) or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs.\n        `\"channels_last\"` corresponds to inputs with shape\n        `(batch, steps, channels)`\n        (default format for temporal data in Keras)\n        while `\"channels_first\"` corresponds to inputs\n        with shape `(batch, channels, steps)`.\n    dilation_rate: an integer or tuple/list of a single integer, specifying\n        the dilation rate to use for dilated convolution.\n        Currently, specifying any `dilation_rate` value != 1 is\n        incompatible with specifying any `strides` value != 1.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    activity_regularizer: Regularizer function applied to\n        the output of the layer (its \"activation\").\n        (see [regularizer](../regularizers.md)).\n    kernel_constraint: Constraint function applied to the kernel matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n\n# Input shape\n    3D tensor with shape: `(batch, steps, channels)`\n\n# Output shape\n    3D tensor with shape: `(batch, new_steps, filters)`\n    `steps` value might have changed due to padding or strides.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Conv1D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "2D convolution layer (e.g. spatial convolution over images).\n\nThis layer creates a convolution kernel that is convolved\nwith the layer input to produce a tensor of\noutputs. If `use_bias` is True,\na bias vector is created and added to the outputs. Finally, if\n`activation` is not `None`, it is applied to the outputs as well.\n\nWhen using this layer as the first layer in a model,\nprovide the keyword argument `input_shape`\n(tuple of integers, does not include the sample axis),\ne.g. `input_shape=(128, 128, 3)` for 128x128 RGB pictures\nin `data_format=\"channels_last\"`.\n\n# Arguments\n    filters: Integer, the dimensionality of the output space\n        (i.e. the number of output filters in the convolution).\n    kernel_size: An integer or tuple/list of 2 integers, specifying the\n        height and width of the 2D convolution window.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n    strides: An integer or tuple/list of 2 integers,\n        specifying the strides of the convolution\n        along the height and width.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n        Specifying any stride value != 1 is incompatible with specifying\n        any `dilation_rate` value != 1.\n    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n        Note that `\"same\"` is slightly inconsistent across backends with\n        `strides` != 1, as described\n        [here](https://github.com/keras-team/keras/pull/9473#issuecomment-372166860)\n    data_format: A string,\n        one of `\"channels_last\"` or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs.\n        `\"channels_last\"` corresponds to inputs with shape\n        `(batch, height, width, channels)` while `\"channels_first\"`\n        corresponds to inputs with shape\n        `(batch, channels, height, width)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n    dilation_rate: an integer or tuple/list of 2 integers, specifying\n        the dilation rate to use for dilated convolution.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n        Currently, specifying any `dilation_rate` value != 1 is\n        incompatible with specifying any stride value != 1.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    activity_regularizer: Regularizer function applied to\n        the output of the layer (its \"activation\").\n        (see [regularizer](../regularizers.md)).\n    kernel_constraint: Constraint function applied to the kernel matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n\n# Input shape\n    4D tensor with shape:\n    `(batch, channels, rows, cols)`\n    if `data_format` is `\"channels_first\"`\n    or 4D tensor with shape:\n    `(batch, rows, cols, channels)`\n    if `data_format` is `\"channels_last\"`.\n\n# Output shape\n    4D tensor with shape:\n    `(batch, filters, new_rows, new_cols)`\n    if `data_format` is `\"channels_first\"`\n    or 4D tensor with shape:\n    `(batch, new_rows, new_cols, filters)`\n    if `data_format` is `\"channels_last\"`.\n    `rows` and `cols` values might have changed due to padding.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Conv2D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Transposed convolution layer (sometimes called Deconvolution).\n\nThe need for transposed convolutions generally arises\nfrom the desire to use a transformation going in the opposite direction\nof a normal convolution, i.e., from something that has the shape of the\noutput of some convolution to something that has the shape of its input\nwhile maintaining a connectivity pattern that is compatible with\nsaid convolution.\n\nWhen using this layer as the first layer in a model,\nprovide the keyword argument `input_shape`\n(tuple of integers, does not include the sample axis),\ne.g. `input_shape=(128, 128, 3)` for 128x128 RGB pictures\nin `data_format=\"channels_last\"`.\n\n# Arguments\n    filters: Integer, the dimensionality of the output space\n        (i.e. the number of output filters in the convolution).\n    kernel_size: An integer or tuple/list of 2 integers, specifying the\n        height and width of the 2D convolution window.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n    strides: An integer or tuple/list of 2 integers,\n        specifying the strides of the convolution\n        along the height and width.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n        Specifying any stride value != 1 is incompatible with specifying\n        any `dilation_rate` value != 1.\n    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n    output_padding: An integer or tuple/list of 2 integers,\n        specifying the amount of padding along the height and width\n        of the output tensor.\n        Can be a single integer to specify the same value for all\n        spatial dimensions.\n        The amount of output padding along a given dimension must be\n        lower than the stride along that same dimension.\n        If set to `None` (default), the output shape is inferred.\n    data_format: A string,\n        one of `\"channels_last\"` or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs.\n        `\"channels_last\"` corresponds to inputs with shape\n        `(batch, height, width, channels)` while `\"channels_first\"`\n        corresponds to inputs with shape\n        `(batch, channels, height, width)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n    dilation_rate: an integer or tuple/list of 2 integers, specifying\n        the dilation rate to use for dilated convolution.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n        Currently, specifying any `dilation_rate` value != 1 is\n        incompatible with specifying any stride value != 1.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    activity_regularizer: Regularizer function applied to\n        the output of the layer (its \"activation\").\n        (see [regularizer](../regularizers.md)).\n    kernel_constraint: Constraint function applied to the kernel matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n\n# Input shape\n    4D tensor with shape:\n    `(batch, channels, rows, cols)`\n    if `data_format` is `\"channels_first\"`\n    or 4D tensor with shape:\n    `(batch, rows, cols, channels)`\n    if `data_format` is `\"channels_last\"`.\n\n# Output shape\n    4D tensor with shape:\n    `(batch, filters, new_rows, new_cols)`\n    if `data_format` is `\"channels_first\"`\n    or 4D tensor with shape:\n    `(batch, new_rows, new_cols, filters)`\n    if `data_format` is `\"channels_last\"`.\n    `rows` and `cols` values might have changed due to padding.\n    If `output_padding` is specified:\n\n    ```\n    new_rows = ((rows - 1) * strides[0] + kernel_size[0]\n                - 2 * padding[0] + output_padding[0])\n    new_cols = ((cols - 1) * strides[1] + kernel_size[1]\n                - 2 * padding[1] + output_padding[1])\n    ```\n\n# References\n    - [A guide to convolution arithmetic for deep learning]\n      (https://arxiv.org/abs/1603.07285v1)\n    - [Deconvolutional Networks]\n      (http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Conv2DTranspose",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "3D convolution layer (e.g. spatial convolution over volumes).\n\nThis layer creates a convolution kernel that is convolved\nwith the layer input to produce a tensor of\noutputs. If `use_bias` is True,\na bias vector is created and added to the outputs. Finally, if\n`activation` is not `None`, it is applied to the outputs as well.\n\nWhen using this layer as the first layer in a model,\nprovide the keyword argument `input_shape`\n(tuple of integers, does not include the sample axis),\ne.g. `input_shape=(128, 128, 128, 1)` for 128x128x128 volumes\nwith a single channel,\nin `data_format=\"channels_last\"`.\n\n# Arguments\n    filters: Integer, the dimensionality of the output space\n        (i.e. the number of output filters in the convolution).\n    kernel_size: An integer or tuple/list of 3 integers, specifying the\n        depth, height and width of the 3D convolution window.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n    strides: An integer or tuple/list of 3 integers,\n        specifying the strides of the convolution along each spatial dimension.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n        Specifying any stride value != 1 is incompatible with specifying\n        any `dilation_rate` value != 1.\n    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n    data_format: A string,\n        one of `\"channels_last\"` or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs.\n        `\"channels_last\"` corresponds to inputs with shape\n        `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n        while `\"channels_first\"` corresponds to inputs with shape\n        `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n    dilation_rate: an integer or tuple/list of 3 integers, specifying\n        the dilation rate to use for dilated convolution.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n        Currently, specifying any `dilation_rate` value != 1 is\n        incompatible with specifying any stride value != 1.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    activity_regularizer: Regularizer function applied to\n        the output of the layer (its \"activation\").\n        (see [regularizer](../regularizers.md)).\n    kernel_constraint: Constraint function applied to the kernel matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n\n# Input shape\n    5D tensor with shape:\n    `(batch, channels, conv_dim1, conv_dim2, conv_dim3)`\n    if `data_format` is `\"channels_first\"`\n    or 5D tensor with shape:\n    `(batch, conv_dim1, conv_dim2, conv_dim3, channels)`\n    if `data_format` is `\"channels_last\"`.\n\n# Output shape\n    5D tensor with shape:\n    `(batch, filters, new_conv_dim1, new_conv_dim2, new_conv_dim3)`\n    if `data_format` is `\"channels_first\"`\n    or 5D tensor with shape:\n    `(batch, new_conv_dim1, new_conv_dim2, new_conv_dim3, filters)`\n    if `data_format` is `\"channels_last\"`.\n    `new_conv_dim1`, `new_conv_dim2` and `new_conv_dim3` values might have\n    changed due to padding.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Conv3D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "filters",
                    "kernel_size",
                    "strides",
                    "padding",
                    "output_padding",
                    "data_format",
                    "activation",
                    "use_bias",
                    "kernel_initializer",
                    "bias_initializer",
                    "kernel_regularizer",
                    "bias_regularizer",
                    "activity_regularizer",
                    "kernel_constraint",
                    "bias_constraint"
                ],
                "Defaults": [
                    "1",
                    " 1",
                    " 1",
                    " 'valid'",
                    " None",
                    " None",
                    " None",
                    " True",
                    " 'glorot_uniform'",
                    " 'zeros'",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None"
                ],
                "DocStr": "Transposed convolution layer (sometimes called Deconvolution).\n\nThe need for transposed convolutions generally arises\nfrom the desire to use a transformation going in the opposite direction\nof a normal convolution, i.e., from something that has the shape of the\noutput of some convolution to something that has the shape of its input\nwhile maintaining a connectivity pattern that is compatible with\nsaid convolution.\n\nWhen using this layer as the first layer in a model,\nprovide the keyword argument `input_shape`\n(tuple of integers, does not include the sample axis),\ne.g. `input_shape=(128, 128, 128, 3)` for a 128x128x128 volume with 3 channels\nif `data_format=\"channels_last\"`.\n\n# Arguments\n    filters: Integer, the dimensionality of the output space\n        (i.e. the number of output filters in the convolution).\n    kernel_size: An integer or tuple/list of 3 integers, specifying the\n        depth, height and width of the 3D convolution window.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n    strides: An integer or tuple/list of 3 integers,\n        specifying the strides of the convolution\n        along the depth, height and width.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n        Specifying any stride value != 1 is incompatible with specifying\n        any `dilation_rate` value != 1.\n    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n    output_padding: An integer or tuple/list of 3 integers,\n        specifying the amount of padding along the depth, height, and\n        width.\n        Can be a single integer to specify the same value for all\n        spatial dimensions.\n        The amount of output padding along a given dimension must be\n        lower than the stride along that same dimension.\n        If set to `None` (default), the output shape is inferred.\n    data_format: A string,\n        one of `\"channels_last\"` or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs.\n        `\"channels_last\"` corresponds to inputs with shape\n        `(batch, depth, height, width, channels)` while `\"channels_first\"`\n        corresponds to inputs with shape\n        `(batch, channels, depth, height, width)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n    dilation_rate: an integer or tuple/list of 3 integers, specifying\n        the dilation rate to use for dilated convolution.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n        Currently, specifying any `dilation_rate` value != 1 is\n        incompatible with specifying any stride value != 1.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    activity_regularizer: Regularizer function applied to\n        the output of the layer (its \"activation\").\n        (see [regularizer](../regularizers.md)).\n    kernel_constraint: Constraint function applied to the kernel matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n\n# Input shape\n    5D tensor with shape:\n    `(batch, channels, depth, rows, cols)`\n    if `data_format` is `\"channels_first\"`\n    or 5D tensor with shape:\n    `(batch, depth, rows, cols, channels)`\n    if `data_format` is `\"channels_last\"`.\n\n# Output shape\n    5D tensor with shape:\n    `(batch, filters, new_depth, new_rows, new_cols)`\n    if `data_format` is `\"channels_first\"`\n    or 5D tensor with shape:\n    `(batch, new_depth, new_rows, new_cols, filters)`\n    if `data_format` is `\"channels_last\"`.\n    `depth` and `rows` and `cols` values might have changed due to padding.\n    If `output_padding` is specified::\n\n    ```\n    new_depth = ((depth - 1) * strides[0] + kernel_size[0]\n                 - 2 * padding[0] + output_padding[0])\n    new_rows = ((rows - 1) * strides[1] + kernel_size[1]\n                - 2 * padding[1] + output_padding[1])\n    new_cols = ((cols - 1) * strides[2] + kernel_size[2]\n                - 2 * padding[2] + output_padding[2])\n    ```\n\n# References\n    - [A guide to convolution arithmetic for deep learning]\n      (https://arxiv.org/abs/1603.07285v1)\n    - [Deconvolutional Networks]\n      (http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Conv3DTranspose",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Convolutional LSTM.\n\nIt is similar to an LSTM layer, but the input transformations\nand recurrent transformations are both convolutional.\n\n# Arguments\n    filters: Integer, the dimensionality of the output space\n        (i.e. the number output of filters in the convolution).\n    kernel_size: An integer or tuple/list of n integers, specifying the\n        dimensions of the convolution window.\n    strides: An integer or tuple/list of n integers,\n        specifying the strides of the convolution.\n        Specifying any stride value != 1 is incompatible with specifying\n        any `dilation_rate` value != 1.\n    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n    data_format: A string,\n        one of `\"channels_last\"` (default) or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs.\n        `\"channels_last\"` corresponds to inputs with shape\n        `(batch, time, ..., channels)`\n        while `\"channels_first\"` corresponds to\n        inputs with shape `(batch, time, channels, ...)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be `\"channels_last\"`.\n    dilation_rate: An integer or tuple/list of n integers, specifying\n        the dilation rate to use for dilated convolution.\n        Currently, specifying any `dilation_rate` value != 1 is\n        incompatible with specifying any `strides` value != 1.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    recurrent_activation: Activation function to use\n        for the recurrent step\n        (see [activations](../activations.md)).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix,\n        used for the linear transformation of the inputs.\n        (see [initializers](../initializers.md)).\n    recurrent_initializer: Initializer for the `recurrent_kernel`\n        weights matrix,\n        used for the linear transformation of the recurrent state.\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    unit_forget_bias: Boolean.\n        If True, add 1 to the bias of the forget gate at initialization.\n        Use in combination with `bias_initializer=\"zeros\"`.\n        This is recommended in [Jozefowicz et al.]\n        (http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf).\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    recurrent_regularizer: Regularizer function applied to\n        the `recurrent_kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    activity_regularizer: Regularizer function applied to\n        the output of the layer (its \"activation\").\n        (see [regularizer](../regularizers.md)).\n    kernel_constraint: Constraint function applied to\n        the `kernel` weights matrix\n        (see [constraints](../constraints.md)).\n    recurrent_constraint: Constraint function applied to\n        the `recurrent_kernel` weights matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n    return_sequences: Boolean. Whether to return the last output\n        in the output sequence, or the full sequence.\n    go_backwards: Boolean (default False).\n        If True, process the input sequence backwards.\n    stateful: Boolean (default False). If True, the last state\n        for each sample at index i in a batch will be used as initial\n        state for the sample of index i in the following batch.\n    dropout: Float between 0 and 1.\n        Fraction of the units to drop for\n        the linear transformation of the inputs.\n    recurrent_dropout: Float between 0 and 1.\n        Fraction of the units to drop for\n        the linear transformation of the recurrent state.\n\n# Input shape\n    - if data_format='channels_first'\n        5D tensor with shape:\n        `(samples, time, channels, rows, cols)`\n    - if data_format='channels_last'\n        5D tensor with shape:\n        `(samples, time, rows, cols, channels)`\n\n# Output shape\n    - if `return_sequences`\n         - if data_format='channels_first'\n            5D tensor with shape:\n            `(samples, time, filters, output_row, output_col)`\n         - if data_format='channels_last'\n            5D tensor with shape:\n            `(samples, time, output_row, output_col, filters)`\n    - else\n        - if data_format='channels_first'\n            4D tensor with shape:\n            `(samples, filters, output_row, output_col)`\n        - if data_format='channels_last'\n            4D tensor with shape:\n            `(samples, output_row, output_col, filters)`\n        where o_row and o_col depend on the shape of the filter and\n        the padding\n\n# Raises\n    ValueError: in case of invalid constructor arguments.\n\n# References\n    - [Convolutional LSTM Network: A Machine Learning Approach for\n    Precipitation Nowcasting](http://arxiv.org/abs/1506.04214v1)\n    The current implementation does not include the feedback loop on the\n    cells output",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "ConvLSTM2D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "filters",
                    "kernel_size",
                    "strides",
                    "padding",
                    "data_format",
                    "dilation_rate",
                    "activation",
                    "recurrent_activation",
                    "use_bias",
                    "kernel_initializer",
                    "recurrent_initializer",
                    "bias_initializer",
                    "unit_forget_bias",
                    "kernel_regularizer",
                    "recurrent_regularizer",
                    "bias_regularizer",
                    "kernel_constraint",
                    "recurrent_constraint",
                    "bias_constraint",
                    "dropout",
                    "recurrent_dropout"
                ],
                "Defaults": [
                    "1",
                    " 1",
                    " 'valid'",
                    " None",
                    " 1",
                    " 1",
                    " 'tanh'",
                    " 'hard_sigmoid'",
                    " True",
                    " 'glorot_uniform'",
                    " 'orthogonal'",
                    " 'zeros'",
                    " True",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None",
                    " 0.0",
                    " 0.0"
                ],
                "DocStr": "Cell class for the ConvLSTM2D layer.\n\n# Arguments\n    filters: Integer, the dimensionality of the output space\n        (i.e. the number of output filters in the convolution).\n    kernel_size: An integer or tuple/list of n integers, specifying the\n        dimensions of the convolution window.\n    strides: An integer or tuple/list of n integers,\n        specifying the strides of the convolution.\n        Specifying any stride value != 1 is incompatible with specifying\n        any `dilation_rate` value != 1.\n    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n    data_format: A string,\n        one of `\"channels_last\"` (default) or `\"channels_first\"`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be `\"channels_last\"`.\n    dilation_rate: An integer or tuple/list of n integers, specifying\n        the dilation rate to use for dilated convolution.\n        Currently, specifying any `dilation_rate` value != 1 is\n        incompatible with specifying any `strides` value != 1.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    recurrent_activation: Activation function to use\n        for the recurrent step\n        (see [activations](../activations.md)).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix,\n        used for the linear transformation of the inputs.\n        (see [initializers](../initializers.md)).\n    recurrent_initializer: Initializer for the `recurrent_kernel`\n        weights matrix,\n        used for the linear transformation of the recurrent state.\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    unit_forget_bias: Boolean.\n        If True, add 1 to the bias of the forget gate at initialization.\n        Use in combination with `bias_initializer=\"zeros\"`.\n        This is recommended in [Jozefowicz et al.]\n        (http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf).\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    recurrent_regularizer: Regularizer function applied to\n        the `recurrent_kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    kernel_constraint: Constraint function applied to\n        the `kernel` weights matrix\n        (see [constraints](../constraints.md)).\n    recurrent_constraint: Constraint function applied to\n        the `recurrent_kernel` weights matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n    dropout: Float between 0 and 1.\n        Fraction of the units to drop for\n        the linear transformation of the inputs.\n    recurrent_dropout: Float between 0 and 1.\n        Fraction of the units to drop for\n        the linear transformation of the recurrent state.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "ConvLSTM2DCell",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "cell",
                    "return_sequences",
                    "return_state",
                    "go_backwards",
                    "stateful",
                    "unroll"
                ],
                "Defaults": [
                    "False",
                    " False",
                    " False",
                    " False",
                    " False"
                ],
                "DocStr": "Base class for convolutional-recurrent layers.\n\n# Arguments\n    cell: A RNN cell instance. A RNN cell is a class that has:\n        - a `call(input_at_t, states_at_t)` method, returning\n          `(output_at_t, states_at_t_plus_1)`. The call method of the\n          cell can also take the optional argument `constants`, see\n          section \"Note on passing external constants\" below.\n        - a `state_size` attribute. This can be a single integer (single state)\n          in which case it is the number of channels of the recurrent state\n          (which should be the same as the number of channels of the cell\n          output). This can also be a list/tuple of integers\n          (one size per state). In this case, the first entry (`state_size[0]`)\n          should be the same as the size of the cell output.\n    return_sequences: Boolean. Whether to return the last output.\n        in the output sequence, or the full sequence.\n    return_state: Boolean. Whether to return the last state\n        in addition to the output.\n    go_backwards: Boolean (default False).\n        If True, process the input sequence backwards and return the\n        reversed sequence.\n    stateful: Boolean (default False). If True, the last state\n        for each sample at index i in a batch will be used as initial\n        state for the sample of index i in the following batch.\n    input_shape: Use this argument to specify the shape of the\n        input when this layer is the first one in a model.\n\n# Input shape\n    5D tensor with shape:\n    `(samples, timesteps, channels, rows, cols)` if data_format='channels_first'\n    or 5D tensor with shape:\n    `(samples, timesteps, rows, cols, channels)` if data_format='channels_last'.\n\n# Output shape\n    - if `return_state`: a list of tensors. The first tensor is\n        the output. The remaining tensors are the last states,\n        each 5D tensor with shape:\n        `(samples, timesteps,\n          filters, new_rows, new_cols)` if data_format='channels_first'\n        or 5D tensor with shape:\n        `(samples, timesteps,\n          new_rows, new_cols, filters)` if data_format='channels_last'.\n        `rows` and `cols` values might have changed due to padding.\n    - if `return_sequences`: 5D tensor with shape:\n        `(samples, timesteps,\n          filters, new_rows, new_cols)` if data_format='channels_first'\n        or 5D tensor with shape:\n        `(samples, timesteps,\n          new_rows, new_cols, filters)` if data_format='channels_last'.\n    - else, 4D tensor with shape:\n        `(samples, filters, new_rows, new_cols)` if data_format='channels_first'\n        or 4D tensor with shape:\n        `(samples, new_rows, new_cols, filters)` if data_format='channels_last'.\n\n# Masking\n    This layer supports masking for input data with a variable number\n    of timesteps. To introduce masks to your data,\n    use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n    set to `True`.\n\n# Note on using statefulness in RNNs\n    You can set RNN layers to be 'stateful', which means that the states\n    computed for the samples in one batch will be reused as initial states\n    for the samples in the next batch. This assumes a one-to-one mapping\n    between samples in different successive batches.\n\n    To enable statefulness:\n        - specify `stateful=True` in the layer constructor.\n        - specify a fixed batch size for your model, by passing\n             - if sequential model:\n                `batch_input_shape=(...)` to the first layer in your model.\n             - if functional model with 1 or more Input layers:\n                `batch_shape=(...)` to all the first layers in your model.\n                This is the expected shape of your inputs\n                *including the batch size*.\n                It should be a tuple of integers, e.g. `(32, 10, 100, 100, 32)`.\n                Note that the number of rows and columns should be specified too.\n        - specify `shuffle=False` when calling fit().\n\n    To reset the states of your model, call `.reset_states()` on either\n    a specific layer, or on your entire model.\n\n# Note on specifying the initial state of RNNs\n    You can specify the initial state of RNN layers symbolically by\n    calling them with the keyword argument `initial_state`. The value of\n    `initial_state` should be a tensor or list of tensors representing\n    the initial state of the RNN layer.\n\n    You can specify the initial state of RNN layers numerically by\n    calling `reset_states` with the keyword argument `states`. The value of\n    `states` should be a numpy array or list of numpy arrays representing\n    the initial state of the RNN layer.\n\n# Note on passing external constants to RNNs\n    You can pass \"external\" constants to the cell using the `constants`\n    keyword argument of `RNN.__call__` (as well as `RNN.call`) method. This\n    requires that the `cell.call` method accepts the same keyword argument\n    `constants`. Such constants can be used to condition the cell\n    transformation on additional static inputs (not changing over time),\n    a.k.a. an attention mechanism.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config",
                            "custom_objects"
                        ],
                        "Defaults": [
                            "None"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "ConvRNN2D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "filters",
                    "kernel_size",
                    "strides",
                    "padding",
                    "data_format",
                    "dilation_rate",
                    "return_sequences",
                    "go_backwards",
                    "stateful"
                ],
                "Defaults": [
                    "1",
                    " 1",
                    " 'valid'",
                    " None",
                    " 1",
                    " 1",
                    " False",
                    " False",
                    " False"
                ],
                "DocStr": "Abstract base class for convolutional recurrent layers.\n\nDo not use in a model -- it's not a functional layer!\n\n# Arguments\n    filters: Integer, the dimensionality of the output space\n        (i.e. the number output of filters in the convolution).\n    kernel_size: An integer or tuple/list of n integers, specifying the\n        dimensions of the convolution window.\n    strides: An integer or tuple/list of n integers,\n        specifying the strides of the convolution.\n        Specifying any stride value != 1 is incompatible with specifying\n        any `dilation_rate` value != 1.\n    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, time, ..., channels)`\n        while `channels_first` corresponds to\n        inputs with shape `(batch, time, channels, ...)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n    dilation_rate: An integer or tuple/list of n integers, specifying\n        the dilation rate to use for dilated convolution.\n        Currently, specifying any `dilation_rate` value != 1 is\n        incompatible with specifying any `strides` value != 1.\n    return_sequences: Boolean. Whether to return the last output\n        in the output sequence, or the full sequence.\n    go_backwards: Boolean (default False).\n        If True, process the input sequence backwards.\n    stateful: Boolean (default False). If True, the last state\n        for each sample at index i in a batch will be used as initial\n        state for the sample of index i in the following batch.\n\n# Input shape\n    5D tensor with shape `(num_samples, timesteps, channels, rows, cols)`.\n\n# Output shape\n    - if `return_sequences`: 5D tensor with shape\n        `(num_samples, timesteps, channels, rows, cols)`.\n    - else, 4D tensor with shape `(num_samples, channels, rows, cols)`.\n\n# Masking\n    This layer supports masking for input data with a variable number\n    of timesteps. To introduce masks to your data,\n    use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n    set to `True`.\n    **Note:** for the time being, masking is only supported with Theano.\n\n# Note on using statefulness in RNNs\n    You can set RNN layers to be 'stateful', which means that the states\n    computed for the samples in one batch will be reused as initial states\n    for the samples in the next batch.\n    This assumes a one-to-one mapping between\n    samples in different successive batches.\n\n    To enable statefulness:\n        - specify `stateful=True` in the layer constructor.\n        - specify a fixed batch size for your model, by passing\n            a `batch_input_size=(...)` to the first layer in your model.\n            This is the expected shape of your inputs *including the batch\n            size*.\n            It should be a tuple of integers, e.g. `(32, 10, 100)`.\n\n    To reset the states of your model, call `.reset_states()` on either\n    a specific layer, or on your entire model.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "ConvRecurrent2D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "1D convolution layer (e.g. temporal convolution).\n\nThis layer creates a convolution kernel that is convolved\nwith the layer input over a single spatial (or temporal) dimension\nto produce a tensor of outputs.\nIf `use_bias` is True, a bias vector is created and added to the outputs.\nFinally, if `activation` is not `None`,\nit is applied to the outputs as well.\n\nWhen using this layer as the first layer in a model,\nprovide an `input_shape` argument\n(tuple of integers or `None`, e.g.\n`(10, 128)` for sequences of 10 vectors of 128-dimensional vectors,\nor `(None, 128)` for variable-length sequences of 128-dimensional vectors.\n\n# Arguments\n    filters: Integer, the dimensionality of the output space\n        (i.e. the number of output filters in the convolution).\n    kernel_size: An integer or tuple/list of a single integer,\n        specifying the length of the 1D convolution window.\n    strides: An integer or tuple/list of a single integer,\n        specifying the stride length of the convolution.\n        Specifying any stride value != 1 is incompatible with specifying\n        any `dilation_rate` value != 1.\n    padding: One of `\"valid\"`, `\"causal\"` or `\"same\"` (case-insensitive).\n        `\"valid\"` means \"no padding\".\n        `\"same\"` results in padding the input such that\n        the output has the same length as the original input.\n        `\"causal\"` results in causal (dilated) convolutions,\n        e.g. `output[t]` does not depend on `input[t + 1:]`.\n        A zero padding is used such that\n        the output has the same length as the original input.\n        Useful when modeling temporal data where the model\n        should not violate the temporal order. See\n        [WaveNet: A Generative Model for Raw Audio, section 2.1]\n        (https://arxiv.org/abs/1609.03499).\n    data_format: A string,\n        one of `\"channels_last\"` (default) or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs.\n        `\"channels_last\"` corresponds to inputs with shape\n        `(batch, steps, channels)`\n        (default format for temporal data in Keras)\n        while `\"channels_first\"` corresponds to inputs\n        with shape `(batch, channels, steps)`.\n    dilation_rate: an integer or tuple/list of a single integer, specifying\n        the dilation rate to use for dilated convolution.\n        Currently, specifying any `dilation_rate` value != 1 is\n        incompatible with specifying any `strides` value != 1.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    activity_regularizer: Regularizer function applied to\n        the output of the layer (its \"activation\").\n        (see [regularizer](../regularizers.md)).\n    kernel_constraint: Constraint function applied to the kernel matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n\n# Input shape\n    3D tensor with shape: `(batch, steps, channels)`\n\n# Output shape\n    3D tensor with shape: `(batch, new_steps, filters)`\n    `steps` value might have changed due to padding or strides.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Convolution1D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "2D convolution layer (e.g. spatial convolution over images).\n\nThis layer creates a convolution kernel that is convolved\nwith the layer input to produce a tensor of\noutputs. If `use_bias` is True,\na bias vector is created and added to the outputs. Finally, if\n`activation` is not `None`, it is applied to the outputs as well.\n\nWhen using this layer as the first layer in a model,\nprovide the keyword argument `input_shape`\n(tuple of integers, does not include the sample axis),\ne.g. `input_shape=(128, 128, 3)` for 128x128 RGB pictures\nin `data_format=\"channels_last\"`.\n\n# Arguments\n    filters: Integer, the dimensionality of the output space\n        (i.e. the number of output filters in the convolution).\n    kernel_size: An integer or tuple/list of 2 integers, specifying the\n        height and width of the 2D convolution window.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n    strides: An integer or tuple/list of 2 integers,\n        specifying the strides of the convolution\n        along the height and width.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n        Specifying any stride value != 1 is incompatible with specifying\n        any `dilation_rate` value != 1.\n    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n        Note that `\"same\"` is slightly inconsistent across backends with\n        `strides` != 1, as described\n        [here](https://github.com/keras-team/keras/pull/9473#issuecomment-372166860)\n    data_format: A string,\n        one of `\"channels_last\"` or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs.\n        `\"channels_last\"` corresponds to inputs with shape\n        `(batch, height, width, channels)` while `\"channels_first\"`\n        corresponds to inputs with shape\n        `(batch, channels, height, width)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n    dilation_rate: an integer or tuple/list of 2 integers, specifying\n        the dilation rate to use for dilated convolution.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n        Currently, specifying any `dilation_rate` value != 1 is\n        incompatible with specifying any stride value != 1.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    activity_regularizer: Regularizer function applied to\n        the output of the layer (its \"activation\").\n        (see [regularizer](../regularizers.md)).\n    kernel_constraint: Constraint function applied to the kernel matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n\n# Input shape\n    4D tensor with shape:\n    `(batch, channels, rows, cols)`\n    if `data_format` is `\"channels_first\"`\n    or 4D tensor with shape:\n    `(batch, rows, cols, channels)`\n    if `data_format` is `\"channels_last\"`.\n\n# Output shape\n    4D tensor with shape:\n    `(batch, filters, new_rows, new_cols)`\n    if `data_format` is `\"channels_first\"`\n    or 4D tensor with shape:\n    `(batch, new_rows, new_cols, filters)`\n    if `data_format` is `\"channels_last\"`.\n    `rows` and `cols` values might have changed due to padding.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Convolution2D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Transposed convolution layer (sometimes called Deconvolution).\n\nThe need for transposed convolutions generally arises\nfrom the desire to use a transformation going in the opposite direction\nof a normal convolution, i.e., from something that has the shape of the\noutput of some convolution to something that has the shape of its input\nwhile maintaining a connectivity pattern that is compatible with\nsaid convolution.\n\nWhen using this layer as the first layer in a model,\nprovide the keyword argument `input_shape`\n(tuple of integers, does not include the sample axis),\ne.g. `input_shape=(128, 128, 3)` for 128x128 RGB pictures\nin `data_format=\"channels_last\"`.\n\n# Arguments\n    filters: Integer, the dimensionality of the output space\n        (i.e. the number of output filters in the convolution).\n    kernel_size: An integer or tuple/list of 2 integers, specifying the\n        height and width of the 2D convolution window.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n    strides: An integer or tuple/list of 2 integers,\n        specifying the strides of the convolution\n        along the height and width.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n        Specifying any stride value != 1 is incompatible with specifying\n        any `dilation_rate` value != 1.\n    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n    output_padding: An integer or tuple/list of 2 integers,\n        specifying the amount of padding along the height and width\n        of the output tensor.\n        Can be a single integer to specify the same value for all\n        spatial dimensions.\n        The amount of output padding along a given dimension must be\n        lower than the stride along that same dimension.\n        If set to `None` (default), the output shape is inferred.\n    data_format: A string,\n        one of `\"channels_last\"` or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs.\n        `\"channels_last\"` corresponds to inputs with shape\n        `(batch, height, width, channels)` while `\"channels_first\"`\n        corresponds to inputs with shape\n        `(batch, channels, height, width)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n    dilation_rate: an integer or tuple/list of 2 integers, specifying\n        the dilation rate to use for dilated convolution.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n        Currently, specifying any `dilation_rate` value != 1 is\n        incompatible with specifying any stride value != 1.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    activity_regularizer: Regularizer function applied to\n        the output of the layer (its \"activation\").\n        (see [regularizer](../regularizers.md)).\n    kernel_constraint: Constraint function applied to the kernel matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n\n# Input shape\n    4D tensor with shape:\n    `(batch, channels, rows, cols)`\n    if `data_format` is `\"channels_first\"`\n    or 4D tensor with shape:\n    `(batch, rows, cols, channels)`\n    if `data_format` is `\"channels_last\"`.\n\n# Output shape\n    4D tensor with shape:\n    `(batch, filters, new_rows, new_cols)`\n    if `data_format` is `\"channels_first\"`\n    or 4D tensor with shape:\n    `(batch, new_rows, new_cols, filters)`\n    if `data_format` is `\"channels_last\"`.\n    `rows` and `cols` values might have changed due to padding.\n    If `output_padding` is specified:\n\n    ```\n    new_rows = ((rows - 1) * strides[0] + kernel_size[0]\n                - 2 * padding[0] + output_padding[0])\n    new_cols = ((cols - 1) * strides[1] + kernel_size[1]\n                - 2 * padding[1] + output_padding[1])\n    ```\n\n# References\n    - [A guide to convolution arithmetic for deep learning]\n      (https://arxiv.org/abs/1603.07285v1)\n    - [Deconvolutional Networks]\n      (http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Convolution2DTranspose",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "3D convolution layer (e.g. spatial convolution over volumes).\n\nThis layer creates a convolution kernel that is convolved\nwith the layer input to produce a tensor of\noutputs. If `use_bias` is True,\na bias vector is created and added to the outputs. Finally, if\n`activation` is not `None`, it is applied to the outputs as well.\n\nWhen using this layer as the first layer in a model,\nprovide the keyword argument `input_shape`\n(tuple of integers, does not include the sample axis),\ne.g. `input_shape=(128, 128, 128, 1)` for 128x128x128 volumes\nwith a single channel,\nin `data_format=\"channels_last\"`.\n\n# Arguments\n    filters: Integer, the dimensionality of the output space\n        (i.e. the number of output filters in the convolution).\n    kernel_size: An integer or tuple/list of 3 integers, specifying the\n        depth, height and width of the 3D convolution window.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n    strides: An integer or tuple/list of 3 integers,\n        specifying the strides of the convolution along each spatial dimension.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n        Specifying any stride value != 1 is incompatible with specifying\n        any `dilation_rate` value != 1.\n    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n    data_format: A string,\n        one of `\"channels_last\"` or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs.\n        `\"channels_last\"` corresponds to inputs with shape\n        `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n        while `\"channels_first\"` corresponds to inputs with shape\n        `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n    dilation_rate: an integer or tuple/list of 3 integers, specifying\n        the dilation rate to use for dilated convolution.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n        Currently, specifying any `dilation_rate` value != 1 is\n        incompatible with specifying any stride value != 1.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    activity_regularizer: Regularizer function applied to\n        the output of the layer (its \"activation\").\n        (see [regularizer](../regularizers.md)).\n    kernel_constraint: Constraint function applied to the kernel matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n\n# Input shape\n    5D tensor with shape:\n    `(batch, channels, conv_dim1, conv_dim2, conv_dim3)`\n    if `data_format` is `\"channels_first\"`\n    or 5D tensor with shape:\n    `(batch, conv_dim1, conv_dim2, conv_dim3, channels)`\n    if `data_format` is `\"channels_last\"`.\n\n# Output shape\n    5D tensor with shape:\n    `(batch, filters, new_conv_dim1, new_conv_dim2, new_conv_dim3)`\n    if `data_format` is `\"channels_first\"`\n    or 5D tensor with shape:\n    `(batch, new_conv_dim1, new_conv_dim2, new_conv_dim3, filters)`\n    if `data_format` is `\"channels_last\"`.\n    `new_conv_dim1`, `new_conv_dim2` and `new_conv_dim3` values might have\n    changed due to padding.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Convolution3D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "cropping"
                ],
                "Defaults": [
                    "1",
                    " 1"
                ],
                "DocStr": "Cropping layer for 1D input (e.g. temporal sequence).\n\nIt crops along the time dimension (axis 1).\n\n# Arguments\n    cropping: int or tuple of int (length 2)\n        How many units should be trimmed off at the beginning and end of\n        the cropping dimension (axis 1).\n        If a single int is provided,\n        the same value will be used for both.\n\n# Input shape\n    3D tensor with shape `(batch, axis_to_crop, features)`\n\n# Output shape\n    3D tensor with shape `(batch, cropped_axis, features)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Cropping1D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Cropping layer for 2D input (e.g. picture).\n\nIt crops along spatial dimensions, i.e. height and width.\n\n# Arguments\n    cropping: int, or tuple of 2 ints, or tuple of 2 tuples of 2 ints.\n        - If int: the same symmetric cropping\n            is applied to height and width.\n        - If tuple of 2 ints:\n            interpreted as two different\n            symmetric cropping values for height and width:\n            `(symmetric_height_crop, symmetric_width_crop)`.\n        - If tuple of 2 tuples of 2 ints:\n            interpreted as\n            `((top_crop, bottom_crop), (left_crop, right_crop))`\n    data_format: A string,\n        one of `\"channels_last\"` or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs.\n        `\"channels_last\"` corresponds to inputs with shape\n        `(batch, height, width, channels)` while `\"channels_first\"`\n        corresponds to inputs with shape\n        `(batch, channels, height, width)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n\n# Input shape\n    4D tensor with shape:\n    - If `data_format` is `\"channels_last\"`:\n        `(batch, rows, cols, channels)`\n    - If `data_format` is `\"channels_first\"`:\n        `(batch, channels, rows, cols)`\n\n# Output shape\n    4D tensor with shape:\n    - If `data_format` is `\"channels_last\"`:\n        `(batch, cropped_rows, cropped_cols, channels)`\n    - If `data_format` is `\"channels_first\"`:\n        `(batch, channels, cropped_rows, cropped_cols)`\n\n# Examples\n\n```python\n    # Crop the input 2D images or feature maps\n    model = Sequential()\n    model.add(Cropping2D(cropping=((2, 2), (4, 4)),\n                         input_shape=(28, 28, 3)))\n    # now model.output_shape == (None, 24, 20, 3)\n    model.add(Conv2D(64, (3, 3), padding='same'))\n    model.add(Cropping2D(cropping=((2, 2), (2, 2))))\n    # now model.output_shape == (None, 20, 16, 64)\n```",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Cropping2D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Cropping layer for 3D data (e.g. spatial or spatio-temporal).\n\n# Arguments\n    cropping: int, or tuple of 3 ints, or tuple of 3 tuples of 2 ints.\n        - If int: the same symmetric cropping\n            is applied to depth, height, and width.\n        - If tuple of 3 ints:\n            interpreted as two different\n            symmetric cropping values for depth, height, and width:\n            `(symmetric_dim1_crop, symmetric_dim2_crop, symmetric_dim3_crop)`.\n        - If tuple of 3 tuples of 2 ints:\n            interpreted as\n            `((left_dim1_crop, right_dim1_crop),\n              (left_dim2_crop, right_dim2_crop),\n              (left_dim3_crop, right_dim3_crop))`\n    data_format: A string,\n        one of `\"channels_last\"` or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs.\n        `\"channels_last\"` corresponds to inputs with shape\n        `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n        while `\"channels_first\"` corresponds to inputs with shape\n        `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n\n# Input shape\n    5D tensor with shape:\n    - If `data_format` is `\"channels_last\"`:\n        `(batch, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop,\n          depth)`\n    - If `data_format` is `\"channels_first\"`:\n        `(batch, depth,\n          first_axis_to_crop, second_axis_to_crop, third_axis_to_crop)`\n\n# Output shape\n    5D tensor with shape:\n    - If `data_format` is `\"channels_last\"`:\n        `(batch, first_cropped_axis, second_cropped_axis, third_cropped_axis,\n          depth)`\n    - If `data_format` is `\"channels_first\"`:\n        `(batch, depth,\n          first_cropped_axis, second_cropped_axis, third_cropped_axis)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Cropping3D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "units",
                    "kernel_initializer",
                    "recurrent_initializer",
                    "bias_initializer",
                    "kernel_regularizer",
                    "recurrent_regularizer",
                    "bias_regularizer",
                    "activity_regularizer",
                    "kernel_constraint",
                    "recurrent_constraint",
                    "bias_constraint",
                    "return_sequences",
                    "return_state",
                    "stateful"
                ],
                "Defaults": [
                    "'glorot_uniform'",
                    " 'orthogonal'",
                    " 'zeros'",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None",
                    " False",
                    " False",
                    " False"
                ],
                "DocStr": "Fast GRU implementation backed by [CuDNN](https://developer.nvidia.com/cudnn).\n\nCan only be run on GPU, with the TensorFlow backend.\n\n# Arguments\n    units: Positive integer, dimensionality of the output space.\n    kernel_initializer: Initializer for the `kernel` weights matrix,\n        used for the linear transformation of the inputs.\n        (see [initializers](../initializers.md)).\n    recurrent_initializer: Initializer for the `recurrent_kernel`\n        weights matrix,\n        used for the linear transformation of the recurrent state.\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    recurrent_regularizer: Regularizer function applied to\n        the `recurrent_kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    activity_regularizer: Regularizer function applied to\n        the output of the layer (its \"activation\").\n        (see [regularizer](../regularizers.md)).\n    kernel_constraint: Constraint function applied to\n        the `kernel` weights matrix\n        (see [constraints](../constraints.md)).\n    recurrent_constraint: Constraint function applied to\n        the `recurrent_kernel` weights matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n    return_sequences: Boolean. Whether to return the last output.\n        in the output sequence, or the full sequence.\n    return_state: Boolean. Whether to return the last state\n        in addition to the output.\n    stateful: Boolean (default False). If True, the last state\n        for each sample at index i in a batch will be used as initial\n        state for the sample of index i in the following batch.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "CuDNNGRU",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "units",
                    "kernel_initializer",
                    "recurrent_initializer",
                    "bias_initializer",
                    "unit_forget_bias",
                    "kernel_regularizer",
                    "recurrent_regularizer",
                    "bias_regularizer",
                    "activity_regularizer",
                    "kernel_constraint",
                    "recurrent_constraint",
                    "bias_constraint",
                    "return_sequences",
                    "return_state",
                    "stateful"
                ],
                "Defaults": [
                    "'glorot_uniform'",
                    " 'orthogonal'",
                    " 'zeros'",
                    " True",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None",
                    " False",
                    " False",
                    " False"
                ],
                "DocStr": "Fast LSTM implementation with [CuDNN](https://developer.nvidia.com/cudnn).\n\nCan only be run on GPU, with the TensorFlow backend.\n\n# Arguments\n    units: Positive integer, dimensionality of the output space.\n    kernel_initializer: Initializer for the `kernel` weights matrix,\n        used for the linear transformation of the inputs.\n        (see [initializers](../initializers.md)).\n    unit_forget_bias: Boolean.\n        If True, add 1 to the bias of the forget gate at initialization.\n        Setting it to true will also force `bias_initializer=\"zeros\"`.\n        This is recommended in [Jozefowicz et al.]\n        (http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf).\n    recurrent_initializer: Initializer for the `recurrent_kernel`\n        weights matrix,\n        used for the linear transformation of the recurrent state.\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    recurrent_regularizer: Regularizer function applied to\n        the `recurrent_kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    activity_regularizer: Regularizer function applied to\n        the output of the layer (its \"activation\").\n        (see [regularizer](../regularizers.md)).\n    kernel_constraint: Constraint function applied to\n        the `kernel` weights matrix\n        (see [constraints](../constraints.md)).\n    recurrent_constraint: Constraint function applied to\n        the `recurrent_kernel` weights matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n    return_sequences: Boolean. Whether to return the last output.\n        in the output sequence, or the full sequence.\n    return_state: Boolean. Whether to return the last state\n        in addition to the output.\n    stateful: Boolean (default False). If True, the last state\n        for each sample at index i in a batch will be used as initial\n        state for the sample of index i in the following batch.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "CuDNNLSTM",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Transposed convolution layer (sometimes called Deconvolution).\n\nThe need for transposed convolutions generally arises\nfrom the desire to use a transformation going in the opposite direction\nof a normal convolution, i.e., from something that has the shape of the\noutput of some convolution to something that has the shape of its input\nwhile maintaining a connectivity pattern that is compatible with\nsaid convolution.\n\nWhen using this layer as the first layer in a model,\nprovide the keyword argument `input_shape`\n(tuple of integers, does not include the sample axis),\ne.g. `input_shape=(128, 128, 3)` for 128x128 RGB pictures\nin `data_format=\"channels_last\"`.\n\n# Arguments\n    filters: Integer, the dimensionality of the output space\n        (i.e. the number of output filters in the convolution).\n    kernel_size: An integer or tuple/list of 2 integers, specifying the\n        height and width of the 2D convolution window.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n    strides: An integer or tuple/list of 2 integers,\n        specifying the strides of the convolution\n        along the height and width.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n        Specifying any stride value != 1 is incompatible with specifying\n        any `dilation_rate` value != 1.\n    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n    output_padding: An integer or tuple/list of 2 integers,\n        specifying the amount of padding along the height and width\n        of the output tensor.\n        Can be a single integer to specify the same value for all\n        spatial dimensions.\n        The amount of output padding along a given dimension must be\n        lower than the stride along that same dimension.\n        If set to `None` (default), the output shape is inferred.\n    data_format: A string,\n        one of `\"channels_last\"` or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs.\n        `\"channels_last\"` corresponds to inputs with shape\n        `(batch, height, width, channels)` while `\"channels_first\"`\n        corresponds to inputs with shape\n        `(batch, channels, height, width)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n    dilation_rate: an integer or tuple/list of 2 integers, specifying\n        the dilation rate to use for dilated convolution.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n        Currently, specifying any `dilation_rate` value != 1 is\n        incompatible with specifying any stride value != 1.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    activity_regularizer: Regularizer function applied to\n        the output of the layer (its \"activation\").\n        (see [regularizer](../regularizers.md)).\n    kernel_constraint: Constraint function applied to the kernel matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n\n# Input shape\n    4D tensor with shape:\n    `(batch, channels, rows, cols)`\n    if `data_format` is `\"channels_first\"`\n    or 4D tensor with shape:\n    `(batch, rows, cols, channels)`\n    if `data_format` is `\"channels_last\"`.\n\n# Output shape\n    4D tensor with shape:\n    `(batch, filters, new_rows, new_cols)`\n    if `data_format` is `\"channels_first\"`\n    or 4D tensor with shape:\n    `(batch, new_rows, new_cols, filters)`\n    if `data_format` is `\"channels_last\"`.\n    `rows` and `cols` values might have changed due to padding.\n    If `output_padding` is specified:\n\n    ```\n    new_rows = ((rows - 1) * strides[0] + kernel_size[0]\n                - 2 * padding[0] + output_padding[0])\n    new_cols = ((cols - 1) * strides[1] + kernel_size[1]\n                - 2 * padding[1] + output_padding[1])\n    ```\n\n# References\n    - [A guide to convolution arithmetic for deep learning]\n      (https://arxiv.org/abs/1603.07285v1)\n    - [Deconvolutional Networks]\n      (http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Deconv2D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "filters",
                    "kernel_size",
                    "strides",
                    "padding",
                    "output_padding",
                    "data_format",
                    "activation",
                    "use_bias",
                    "kernel_initializer",
                    "bias_initializer",
                    "kernel_regularizer",
                    "bias_regularizer",
                    "activity_regularizer",
                    "kernel_constraint",
                    "bias_constraint"
                ],
                "Defaults": [
                    "1",
                    " 1",
                    " 1",
                    " 'valid'",
                    " None",
                    " None",
                    " None",
                    " True",
                    " 'glorot_uniform'",
                    " 'zeros'",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None"
                ],
                "DocStr": "Transposed convolution layer (sometimes called Deconvolution).\n\nThe need for transposed convolutions generally arises\nfrom the desire to use a transformation going in the opposite direction\nof a normal convolution, i.e., from something that has the shape of the\noutput of some convolution to something that has the shape of its input\nwhile maintaining a connectivity pattern that is compatible with\nsaid convolution.\n\nWhen using this layer as the first layer in a model,\nprovide the keyword argument `input_shape`\n(tuple of integers, does not include the sample axis),\ne.g. `input_shape=(128, 128, 128, 3)` for a 128x128x128 volume with 3 channels\nif `data_format=\"channels_last\"`.\n\n# Arguments\n    filters: Integer, the dimensionality of the output space\n        (i.e. the number of output filters in the convolution).\n    kernel_size: An integer or tuple/list of 3 integers, specifying the\n        depth, height and width of the 3D convolution window.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n    strides: An integer or tuple/list of 3 integers,\n        specifying the strides of the convolution\n        along the depth, height and width.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n        Specifying any stride value != 1 is incompatible with specifying\n        any `dilation_rate` value != 1.\n    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n    output_padding: An integer or tuple/list of 3 integers,\n        specifying the amount of padding along the depth, height, and\n        width.\n        Can be a single integer to specify the same value for all\n        spatial dimensions.\n        The amount of output padding along a given dimension must be\n        lower than the stride along that same dimension.\n        If set to `None` (default), the output shape is inferred.\n    data_format: A string,\n        one of `\"channels_last\"` or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs.\n        `\"channels_last\"` corresponds to inputs with shape\n        `(batch, depth, height, width, channels)` while `\"channels_first\"`\n        corresponds to inputs with shape\n        `(batch, channels, depth, height, width)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n    dilation_rate: an integer or tuple/list of 3 integers, specifying\n        the dilation rate to use for dilated convolution.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n        Currently, specifying any `dilation_rate` value != 1 is\n        incompatible with specifying any stride value != 1.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    activity_regularizer: Regularizer function applied to\n        the output of the layer (its \"activation\").\n        (see [regularizer](../regularizers.md)).\n    kernel_constraint: Constraint function applied to the kernel matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n\n# Input shape\n    5D tensor with shape:\n    `(batch, channels, depth, rows, cols)`\n    if `data_format` is `\"channels_first\"`\n    or 5D tensor with shape:\n    `(batch, depth, rows, cols, channels)`\n    if `data_format` is `\"channels_last\"`.\n\n# Output shape\n    5D tensor with shape:\n    `(batch, filters, new_depth, new_rows, new_cols)`\n    if `data_format` is `\"channels_first\"`\n    or 5D tensor with shape:\n    `(batch, new_depth, new_rows, new_cols, filters)`\n    if `data_format` is `\"channels_last\"`.\n    `depth` and `rows` and `cols` values might have changed due to padding.\n    If `output_padding` is specified::\n\n    ```\n    new_depth = ((depth - 1) * strides[0] + kernel_size[0]\n                 - 2 * padding[0] + output_padding[0])\n    new_rows = ((rows - 1) * strides[1] + kernel_size[1]\n                - 2 * padding[1] + output_padding[1])\n    new_cols = ((cols - 1) * strides[2] + kernel_size[2]\n                - 2 * padding[2] + output_padding[2])\n    ```\n\n# References\n    - [A guide to convolution arithmetic for deep learning]\n      (https://arxiv.org/abs/1603.07285v1)\n    - [Deconvolutional Networks]\n      (http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Deconv3D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Transposed convolution layer (sometimes called Deconvolution).\n\nThe need for transposed convolutions generally arises\nfrom the desire to use a transformation going in the opposite direction\nof a normal convolution, i.e., from something that has the shape of the\noutput of some convolution to something that has the shape of its input\nwhile maintaining a connectivity pattern that is compatible with\nsaid convolution.\n\nWhen using this layer as the first layer in a model,\nprovide the keyword argument `input_shape`\n(tuple of integers, does not include the sample axis),\ne.g. `input_shape=(128, 128, 3)` for 128x128 RGB pictures\nin `data_format=\"channels_last\"`.\n\n# Arguments\n    filters: Integer, the dimensionality of the output space\n        (i.e. the number of output filters in the convolution).\n    kernel_size: An integer or tuple/list of 2 integers, specifying the\n        height and width of the 2D convolution window.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n    strides: An integer or tuple/list of 2 integers,\n        specifying the strides of the convolution\n        along the height and width.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n        Specifying any stride value != 1 is incompatible with specifying\n        any `dilation_rate` value != 1.\n    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n    output_padding: An integer or tuple/list of 2 integers,\n        specifying the amount of padding along the height and width\n        of the output tensor.\n        Can be a single integer to specify the same value for all\n        spatial dimensions.\n        The amount of output padding along a given dimension must be\n        lower than the stride along that same dimension.\n        If set to `None` (default), the output shape is inferred.\n    data_format: A string,\n        one of `\"channels_last\"` or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs.\n        `\"channels_last\"` corresponds to inputs with shape\n        `(batch, height, width, channels)` while `\"channels_first\"`\n        corresponds to inputs with shape\n        `(batch, channels, height, width)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n    dilation_rate: an integer or tuple/list of 2 integers, specifying\n        the dilation rate to use for dilated convolution.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n        Currently, specifying any `dilation_rate` value != 1 is\n        incompatible with specifying any stride value != 1.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    activity_regularizer: Regularizer function applied to\n        the output of the layer (its \"activation\").\n        (see [regularizer](../regularizers.md)).\n    kernel_constraint: Constraint function applied to the kernel matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n\n# Input shape\n    4D tensor with shape:\n    `(batch, channels, rows, cols)`\n    if `data_format` is `\"channels_first\"`\n    or 4D tensor with shape:\n    `(batch, rows, cols, channels)`\n    if `data_format` is `\"channels_last\"`.\n\n# Output shape\n    4D tensor with shape:\n    `(batch, filters, new_rows, new_cols)`\n    if `data_format` is `\"channels_first\"`\n    or 4D tensor with shape:\n    `(batch, new_rows, new_cols, filters)`\n    if `data_format` is `\"channels_last\"`.\n    `rows` and `cols` values might have changed due to padding.\n    If `output_padding` is specified:\n\n    ```\n    new_rows = ((rows - 1) * strides[0] + kernel_size[0]\n                - 2 * padding[0] + output_padding[0])\n    new_cols = ((cols - 1) * strides[1] + kernel_size[1]\n                - 2 * padding[1] + output_padding[1])\n    ```\n\n# References\n    - [A guide to convolution arithmetic for deep learning]\n      (https://arxiv.org/abs/1603.07285v1)\n    - [Deconvolutional Networks]\n      (http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Deconvolution2D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "filters",
                    "kernel_size",
                    "strides",
                    "padding",
                    "output_padding",
                    "data_format",
                    "activation",
                    "use_bias",
                    "kernel_initializer",
                    "bias_initializer",
                    "kernel_regularizer",
                    "bias_regularizer",
                    "activity_regularizer",
                    "kernel_constraint",
                    "bias_constraint"
                ],
                "Defaults": [
                    "1",
                    " 1",
                    " 1",
                    " 'valid'",
                    " None",
                    " None",
                    " None",
                    " True",
                    " 'glorot_uniform'",
                    " 'zeros'",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None"
                ],
                "DocStr": "Transposed convolution layer (sometimes called Deconvolution).\n\nThe need for transposed convolutions generally arises\nfrom the desire to use a transformation going in the opposite direction\nof a normal convolution, i.e., from something that has the shape of the\noutput of some convolution to something that has the shape of its input\nwhile maintaining a connectivity pattern that is compatible with\nsaid convolution.\n\nWhen using this layer as the first layer in a model,\nprovide the keyword argument `input_shape`\n(tuple of integers, does not include the sample axis),\ne.g. `input_shape=(128, 128, 128, 3)` for a 128x128x128 volume with 3 channels\nif `data_format=\"channels_last\"`.\n\n# Arguments\n    filters: Integer, the dimensionality of the output space\n        (i.e. the number of output filters in the convolution).\n    kernel_size: An integer or tuple/list of 3 integers, specifying the\n        depth, height and width of the 3D convolution window.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n    strides: An integer or tuple/list of 3 integers,\n        specifying the strides of the convolution\n        along the depth, height and width.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n        Specifying any stride value != 1 is incompatible with specifying\n        any `dilation_rate` value != 1.\n    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n    output_padding: An integer or tuple/list of 3 integers,\n        specifying the amount of padding along the depth, height, and\n        width.\n        Can be a single integer to specify the same value for all\n        spatial dimensions.\n        The amount of output padding along a given dimension must be\n        lower than the stride along that same dimension.\n        If set to `None` (default), the output shape is inferred.\n    data_format: A string,\n        one of `\"channels_last\"` or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs.\n        `\"channels_last\"` corresponds to inputs with shape\n        `(batch, depth, height, width, channels)` while `\"channels_first\"`\n        corresponds to inputs with shape\n        `(batch, channels, depth, height, width)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n    dilation_rate: an integer or tuple/list of 3 integers, specifying\n        the dilation rate to use for dilated convolution.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n        Currently, specifying any `dilation_rate` value != 1 is\n        incompatible with specifying any stride value != 1.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    activity_regularizer: Regularizer function applied to\n        the output of the layer (its \"activation\").\n        (see [regularizer](../regularizers.md)).\n    kernel_constraint: Constraint function applied to the kernel matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n\n# Input shape\n    5D tensor with shape:\n    `(batch, channels, depth, rows, cols)`\n    if `data_format` is `\"channels_first\"`\n    or 5D tensor with shape:\n    `(batch, depth, rows, cols, channels)`\n    if `data_format` is `\"channels_last\"`.\n\n# Output shape\n    5D tensor with shape:\n    `(batch, filters, new_depth, new_rows, new_cols)`\n    if `data_format` is `\"channels_first\"`\n    or 5D tensor with shape:\n    `(batch, new_depth, new_rows, new_cols, filters)`\n    if `data_format` is `\"channels_last\"`.\n    `depth` and `rows` and `cols` values might have changed due to padding.\n    If `output_padding` is specified::\n\n    ```\n    new_depth = ((depth - 1) * strides[0] + kernel_size[0]\n                 - 2 * padding[0] + output_padding[0])\n    new_rows = ((rows - 1) * strides[1] + kernel_size[1]\n                - 2 * padding[1] + output_padding[1])\n    new_cols = ((cols - 1) * strides[2] + kernel_size[2]\n                - 2 * padding[2] + output_padding[2])\n    ```\n\n# References\n    - [A guide to convolution arithmetic for deep learning]\n      (https://arxiv.org/abs/1603.07285v1)\n    - [Deconvolutional Networks]\n      (http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Deconvolution3D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Just your regular densely-connected NN layer.\n\n`Dense` implements the operation:\n`output = activation(dot(input, kernel) + bias)`\nwhere `activation` is the element-wise activation function\npassed as the `activation` argument, `kernel` is a weights matrix\ncreated by the layer, and `bias` is a bias vector created by the layer\n(only applicable if `use_bias` is `True`).\n\nNote: if the input to the layer has a rank greater than 2, then\nit is flattened prior to the initial dot product with `kernel`.\n\n# Example\n\n```python\n    # as first layer in a sequential model:\n    model = Sequential()\n    model.add(Dense(32, input_shape=(16,)))\n    # now the model will take as input arrays of shape (*, 16)\n    # and output arrays of shape (*, 32)\n\n    # after the first layer, you don't need to specify\n    # the size of the input anymore:\n    model.add(Dense(32))\n```\n\n# Arguments\n    units: Positive integer, dimensionality of the output space.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    activity_regularizer: Regularizer function applied to\n        the output of the layer (its \"activation\").\n        (see [regularizer](../regularizers.md)).\n    kernel_constraint: Constraint function applied to\n        the `kernel` weights matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n\n# Input shape\n    nD tensor with shape: `(batch_size, ..., input_dim)`.\n    The most common situation would be\n    a 2D input with shape `(batch_size, input_dim)`.\n\n# Output shape\n    nD tensor with shape: `(batch_size, ..., units)`.\n    For instance, for a 2D input with shape `(batch_size, input_dim)`,\n    the output would have shape `(batch_size, units)`.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Dense",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "kernel_size",
                    "strides",
                    "padding",
                    "depth_multiplier",
                    "data_format",
                    "activation",
                    "use_bias",
                    "depthwise_initializer",
                    "bias_initializer",
                    "depthwise_regularizer",
                    "bias_regularizer",
                    "activity_regularizer",
                    "depthwise_constraint",
                    "bias_constraint"
                ],
                "Defaults": [
                    "1",
                    " 1",
                    " 'valid'",
                    " 1",
                    " None",
                    " None",
                    " True",
                    " 'glorot_uniform'",
                    " 'zeros'",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None"
                ],
                "DocStr": "Depthwise separable 2D convolution.\n\nDepthwise Separable convolutions consists in performing\njust the first step in a depthwise spatial convolution\n(which acts on each input channel separately).\nThe `depth_multiplier` argument controls how many\noutput channels are generated per input channel in the depthwise step.\n\n# Arguments\n    kernel_size: An integer or tuple/list of 2 integers, specifying the\n        height and width of the 2D convolution window.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n    strides: An integer or tuple/list of 2 integers,\n        specifying the strides of the convolution\n        along the height and width.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n        Specifying any stride value != 1 is incompatible with specifying\n        any `dilation_rate` value != 1.\n    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n    depth_multiplier: The number of depthwise convolution output channels\n        for each input channel.\n        The total number of depthwise convolution output\n        channels will be equal to `filters_in * depth_multiplier`.\n    data_format: A string,\n        one of `\"channels_last\"` or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs.\n        `\"channels_last\"` corresponds to inputs with shape\n        `(batch, height, width, channels)` while `\"channels_first\"`\n        corresponds to inputs with shape\n        `(batch, channels, height, width)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be 'channels_last'.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        If you don't specify anything, no activation is applied\n        (ie. 'linear' activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    depthwise_initializer: Initializer for the depthwise kernel matrix\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    depthwise_regularizer: Regularizer function applied to\n        the depthwise kernel matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    activity_regularizer: Regularizer function applied to\n        the output of the layer (its 'activation').\n        (see [regularizer](../regularizers.md)).\n    depthwise_constraint: Constraint function applied to\n        the depthwise kernel matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n\n# Input shape\n    4D tensor with shape:\n    `(batch, channels, rows, cols)`\n    if `data_format` is `\"channels_first\"`\n    or 4D tensor with shape:\n    `(batch, rows, cols, channels)`\n    if `data_format` is `\"channels_last\"`.\n\n# Output shape\n    4D tensor with shape:\n    `(batch, filters, new_rows, new_cols)`\n    if `data_format` is `\"channels_first\"`\n    or 4D tensor with shape:\n    `(batch, new_rows, new_cols, filters)`\n    if `data_format` is `\"channels_last\"`.\n    `rows` and `cols` values might have changed due to padding.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "DepthwiseConv2D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "axes",
                    "normalize"
                ],
                "Defaults": [
                    "False"
                ],
                "DocStr": "Layer that computes a dot product between samples in two tensors.\n\nE.g. if applied to a list of two tensors `a` and `b` of shape `(batch_size, n)`,\nthe output will be a tensor of shape `(batch_size, 1)`\nwhere each entry `i` will be the dot product between\n`a[i]` and `b[i]`.\n\n# Arguments\n    axes: Integer or tuple of integers,\n        axis or axes along which to take the dot product.\n    normalize: Whether to L2-normalize samples along the\n        dot product axis before taking the dot product.\n        If set to True, then the output of the dot product\n        is the cosine proximity between the two samples.\n    **kwargs: Standard layer keyword arguments.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Dot",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Applies Dropout to the input.\n\nDropout consists in randomly setting\na fraction `rate` of input units to 0 at each update during training time,\nwhich helps prevent overfitting.\n\n# Arguments\n    rate: float between 0 and 1. Fraction of the input units to drop.\n    noise_shape: 1D integer tensor representing the shape of the\n        binary dropout mask that will be multiplied with the input.\n        For instance, if your inputs have shape\n        `(batch_size, timesteps, features)` and\n        you want the dropout mask to be the same for all timesteps,\n        you can use `noise_shape=(batch_size, 1, features)`.\n    seed: A Python integer to use as random seed.\n\n# References\n    - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting]\n      (http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Dropout",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "alpha"
                ],
                "Defaults": [
                    "1.0"
                ],
                "DocStr": "Exponential Linear Unit.\n\nIt follows:\n`f(x) =  alpha * (exp(x) - 1.) for x < 0`,\n`f(x) = x for x >= 0`.\n\n# Input shape\n    Arbitrary. Use the keyword argument `input_shape`\n    (tuple of integers, does not include the samples axis)\n    when using this layer as the first layer in a model.\n\n# Output shape\n    Same shape as the input.\n\n# Arguments\n    alpha: scale for the negative factor.\n\n# References\n    - [Fast and Accurate Deep Network Learning by Exponential Linear Units\n       (ELUs)](https://arxiv.org/abs/1511.07289v1)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "ELU",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Turns positive integers (indexes) into dense vectors of fixed size.\neg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n\nThis layer can only be used as the first layer in a model.\n\n# Example\n\n```python\n  model = Sequential()\n  model.add(Embedding(1000, 64, input_length=10))\n  # the model will take as input an integer matrix of size (batch, input_length).\n  # the largest integer (i.e. word index) in the input should be\n  # no larger than 999 (vocabulary size).\n  # now model.output_shape == (None, 10, 64), where None is the batch dimension.\n\n  input_array = np.random.randint(1000, size=(32, 10))\n\n  model.compile('rmsprop', 'mse')\n  output_array = model.predict(input_array)\n  assert output_array.shape == (32, 10, 64)\n```\n\n# Arguments\n    input_dim: int > 0. Size of the vocabulary,\n        i.e. maximum integer index + 1.\n    output_dim: int >= 0. Dimension of the dense embedding.\n    embeddings_initializer: Initializer for the `embeddings` matrix\n        (see [initializers](../initializers.md)).\n    embeddings_regularizer: Regularizer function applied to\n        the `embeddings` matrix\n        (see [regularizer](../regularizers.md)).\n    embeddings_constraint: Constraint function applied to\n        the `embeddings` matrix\n        (see [constraints](../constraints.md)).\n    mask_zero: Whether or not the input value 0 is a special \"padding\"\n        value that should be masked out.\n        This is useful when using [recurrent layers](recurrent.md)\n        which may take variable length input.\n        If this is `True` then all subsequent layers\n        in the model need to support masking or an exception will be raised.\n        If mask_zero is set to True, as a consequence, index 0 cannot be\n        used in the vocabulary (input_dim should equal size of\n        vocabulary + 1).\n    input_length: Length of input sequences, when it is constant.\n        This argument is required if you are going to connect\n        `Flatten` then `Dense` layers upstream\n        (without it, the shape of the dense outputs cannot be computed).\n\n# Input shape\n    2D tensor with shape: `(batch_size, sequence_length)`.\n\n# Output shape\n    3D tensor with shape: `(batch_size, sequence_length, output_dim)`.\n\n# References\n    - [A Theoretically Grounded Application of Dropout in\n       Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Embedding",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "data_format"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": "Flattens the input. Does not affect the batch size.\n\n# Arguments\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        The purpose of this argument is to preserve weight\n        ordering when switching a model from one data format\n        to another.\n        `channels_last` corresponds to inputs with shape\n        `(batch, ..., channels)` while `channels_first` corresponds to\n        inputs with shape `(batch, channels, ...)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n\n# Example\n\n```python\n    model = Sequential()\n    model.add(Conv2D(64, (3, 3),\n                     input_shape=(3, 32, 32), padding='same',))\n    # now: model.output_shape == (None, 64, 32, 32)\n\n    model.add(Flatten())\n    # now: model.output_shape == (None, 65536)\n```",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Flatten",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Gated Recurrent Unit - Cho et al. 2014.\n\nThere are two variants. The default one is based on 1406.1078v3 and\nhas reset gate applied to hidden state before matrix multiplication. The\nother one is based on original 1406.1078v1 and has the order reversed.\n\nThe second variant is compatible with CuDNNGRU (GPU-only) and allows\ninference on CPU. Thus it has separate biases for `kernel` and\n`recurrent_kernel`. Use `'reset_after'=True` and\n`recurrent_activation='sigmoid'`.\n\n# Arguments\n    units: Positive integer, dimensionality of the output space.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        Default: hyperbolic tangent (`tanh`).\n        If you pass `None`, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    recurrent_activation: Activation function to use\n        for the recurrent step\n        (see [activations](../activations.md)).\n        Default: hard sigmoid (`hard_sigmoid`).\n        If you pass `None`, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix,\n        used for the linear transformation of the inputs\n        (see [initializers](../initializers.md)).\n    recurrent_initializer: Initializer for the `recurrent_kernel`\n        weights matrix,\n        used for the linear transformation of the recurrent state\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    recurrent_regularizer: Regularizer function applied to\n        the `recurrent_kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    activity_regularizer: Regularizer function applied to\n        the output of the layer (its \"activation\").\n        (see [regularizer](../regularizers.md)).\n    kernel_constraint: Constraint function applied to\n        the `kernel` weights matrix\n        (see [constraints](../constraints.md)).\n    recurrent_constraint: Constraint function applied to\n        the `recurrent_kernel` weights matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n    dropout: Float between 0 and 1.\n        Fraction of the units to drop for\n        the linear transformation of the inputs.\n    recurrent_dropout: Float between 0 and 1.\n        Fraction of the units to drop for\n        the linear transformation of the recurrent state.\n    implementation: Implementation mode, either 1 or 2.\n        Mode 1 will structure its operations as a larger number of\n        smaller dot products and additions, whereas mode 2 will\n        batch them into fewer, larger operations. These modes will\n        have different performance profiles on different hardware and\n        for different applications.\n    return_sequences: Boolean. Whether to return the last output\n        in the output sequence, or the full sequence.\n    return_state: Boolean. Whether to return the last state\n        in addition to the output.\n    go_backwards: Boolean (default False).\n        If True, process the input sequence backwards and return the\n        reversed sequence.\n    stateful: Boolean (default False). If True, the last state\n        for each sample at index i in a batch will be used as initial\n        state for the sample of index i in the following batch.\n    unroll: Boolean (default False).\n        If True, the network will be unrolled,\n        else a symbolic loop will be used.\n        Unrolling can speed-up a RNN,\n        although it tends to be more memory-intensive.\n        Unrolling is only suitable for short sequences.\n    reset_after: GRU convention (whether to apply reset gate after or\n        before matrix multiplication). False = \"before\" (default),\n        True = \"after\" (CuDNN compatible).\n\n# References\n    - [Learning Phrase Representations using RNN Encoder-Decoder for\n       Statistical Machine Translation](https://arxiv.org/abs/1406.1078)\n    - [On the Properties of Neural Machine Translation:\n       Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n    - [Empirical Evaluation of Gated Recurrent Neural Networks on\n       Sequence Modeling](https://arxiv.org/abs/1412.3555v1)\n    - [A Theoretically Grounded Application of Dropout in\n       Recurrent Neural Networks](https://arxiv.org/abs/1512.05287)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "GRU",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "units",
                    "activation",
                    "recurrent_activation",
                    "use_bias",
                    "kernel_initializer",
                    "recurrent_initializer",
                    "bias_initializer",
                    "kernel_regularizer",
                    "recurrent_regularizer",
                    "bias_regularizer",
                    "kernel_constraint",
                    "recurrent_constraint",
                    "bias_constraint",
                    "dropout",
                    "recurrent_dropout",
                    "implementation",
                    "reset_after"
                ],
                "Defaults": [
                    "'tanh'",
                    " 'hard_sigmoid'",
                    " True",
                    " 'glorot_uniform'",
                    " 'orthogonal'",
                    " 'zeros'",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None",
                    " 0.0",
                    " 0.0",
                    " 1",
                    " False"
                ],
                "DocStr": "Cell class for the GRU layer.\n\n# Arguments\n    units: Positive integer, dimensionality of the output space.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        Default: hyperbolic tangent (`tanh`).\n        If you pass `None`, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    recurrent_activation: Activation function to use\n        for the recurrent step\n        (see [activations](../activations.md)).\n        Default: hard sigmoid (`hard_sigmoid`).\n        If you pass `None`, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix,\n        used for the linear transformation of the inputs\n        (see [initializers](../initializers.md)).\n    recurrent_initializer: Initializer for the `recurrent_kernel`\n        weights matrix,\n        used for the linear transformation of the recurrent state\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    recurrent_regularizer: Regularizer function applied to\n        the `recurrent_kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    kernel_constraint: Constraint function applied to\n        the `kernel` weights matrix\n        (see [constraints](../constraints.md)).\n    recurrent_constraint: Constraint function applied to\n        the `recurrent_kernel` weights matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n    dropout: Float between 0 and 1.\n        Fraction of the units to drop for\n        the linear transformation of the inputs.\n    recurrent_dropout: Float between 0 and 1.\n        Fraction of the units to drop for\n        the linear transformation of the recurrent state.\n    implementation: Implementation mode, either 1 or 2.\n        Mode 1 will structure its operations as a larger number of\n        smaller dot products and additions, whereas mode 2 will\n        batch them into fewer, larger operations. These modes will\n        have different performance profiles on different hardware and\n        for different applications.\n    reset_after: GRU convention (whether to apply reset gate after or\n        before matrix multiplication). False = \"before\" (default),\n        True = \"after\" (CuDNN compatible).",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "GRUCell",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Apply multiplicative 1-centered Gaussian noise.\n\nAs it is a regularization layer, it is only active at training time.\n\n# Arguments\n    rate: float, drop probability (as with `Dropout`).\n        The multiplicative noise will have\n        standard deviation `sqrt(rate / (1 - rate))`.\n\n# Input shape\n    Arbitrary. Use the keyword argument `input_shape`\n    (tuple of integers, does not include the samples axis)\n    when using this layer as the first layer in a model.\n\n# Output shape\n    Same shape as input.\n\n# References\n    - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting]\n      (http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "GaussianDropout",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Apply additive zero-centered Gaussian noise.\n\nThis is useful to mitigate overfitting\n(you could see it as a form of random data augmentation).\nGaussian Noise (GS) is a natural choice as corruption process\nfor real valued inputs.\n\nAs it is a regularization layer, it is only active at training time.\n\n# Arguments\n    stddev: float, standard deviation of the noise distribution.\n\n# Input shape\n    Arbitrary. Use the keyword argument `input_shape`\n    (tuple of integers, does not include the samples axis)\n    when using this layer as the first layer in a model.\n\n# Output shape\n    Same shape as input.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "GaussianNoise",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "data_format"
                ],
                "Defaults": [
                    "'channels_last'"
                ],
                "DocStr": "Global average pooling operation for temporal data.\n\n# Arguments\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, steps, features)` while `channels_first`\n        corresponds to inputs with shape\n        `(batch, features, steps)`.\n\n# Input shape\n    - If `data_format='channels_last'`:\n        3D tensor with shape:\n        `(batch_size, steps, features)`\n    - If `data_format='channels_first'`:\n        3D tensor with shape:\n        `(batch_size, features, steps)`\n\n# Output shape\n    2D tensor with shape:\n    `(batch_size, features)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "GlobalAveragePooling1D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Global average pooling operation for spatial data.\n\n# Arguments\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, height, width, channels)` while `channels_first`\n        corresponds to inputs with shape\n        `(batch, channels, height, width)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n\n# Input shape\n    - If `data_format='channels_last'`:\n        4D tensor with shape:\n        `(batch_size, rows, cols, channels)`\n    - If `data_format='channels_first'`:\n        4D tensor with shape:\n        `(batch_size, channels, rows, cols)`\n\n# Output shape\n    2D tensor with shape:\n    `(batch_size, channels)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "GlobalAveragePooling2D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Global Average pooling operation for 3D data.\n\n# Arguments\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n        while `channels_first` corresponds to inputs with shape\n        `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n\n# Input shape\n    - If `data_format='channels_last'`:\n        5D tensor with shape:\n        `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n    - If `data_format='channels_first'`:\n        5D tensor with shape:\n        `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`\n\n# Output shape\n    2D tensor with shape:\n    `(batch_size, channels)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "GlobalAveragePooling3D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "data_format"
                ],
                "Defaults": [
                    "'channels_last'"
                ],
                "DocStr": "Global average pooling operation for temporal data.\n\n# Arguments\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, steps, features)` while `channels_first`\n        corresponds to inputs with shape\n        `(batch, features, steps)`.\n\n# Input shape\n    - If `data_format='channels_last'`:\n        3D tensor with shape:\n        `(batch_size, steps, features)`\n    - If `data_format='channels_first'`:\n        3D tensor with shape:\n        `(batch_size, features, steps)`\n\n# Output shape\n    2D tensor with shape:\n    `(batch_size, features)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "GlobalAvgPool1D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Global average pooling operation for spatial data.\n\n# Arguments\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, height, width, channels)` while `channels_first`\n        corresponds to inputs with shape\n        `(batch, channels, height, width)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n\n# Input shape\n    - If `data_format='channels_last'`:\n        4D tensor with shape:\n        `(batch_size, rows, cols, channels)`\n    - If `data_format='channels_first'`:\n        4D tensor with shape:\n        `(batch_size, channels, rows, cols)`\n\n# Output shape\n    2D tensor with shape:\n    `(batch_size, channels)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "GlobalAvgPool2D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Global Average pooling operation for 3D data.\n\n# Arguments\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n        while `channels_first` corresponds to inputs with shape\n        `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n\n# Input shape\n    - If `data_format='channels_last'`:\n        5D tensor with shape:\n        `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n    - If `data_format='channels_first'`:\n        5D tensor with shape:\n        `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`\n\n# Output shape\n    2D tensor with shape:\n    `(batch_size, channels)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "GlobalAvgPool3D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "data_format"
                ],
                "Defaults": [
                    "'channels_last'"
                ],
                "DocStr": "Global max pooling operation for temporal data.\n\n# Arguments\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, steps, features)` while `channels_first`\n        corresponds to inputs with shape\n        `(batch, features, steps)`.\n\n# Input shape\n    - If `data_format='channels_last'`:\n        3D tensor with shape:\n        `(batch_size, steps, features)`\n    - If `data_format='channels_first'`:\n        3D tensor with shape:\n        `(batch_size, features, steps)`\n\n# Output shape\n    2D tensor with shape:\n    `(batch_size, features)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "GlobalMaxPool1D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Global max pooling operation for spatial data.\n\n# Arguments\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, height, width, channels)` while `channels_first`\n        corresponds to inputs with shape\n        `(batch, channels, height, width)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n\n# Input shape\n    - If `data_format='channels_last'`:\n        4D tensor with shape:\n        `(batch_size, rows, cols, channels)`\n    - If `data_format='channels_first'`:\n        4D tensor with shape:\n        `(batch_size, channels, rows, cols)`\n\n# Output shape\n    2D tensor with shape:\n    `(batch_size, channels)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "GlobalMaxPool2D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Global Max pooling operation for 3D data.\n\n# Arguments\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n        while `channels_first` corresponds to inputs with shape\n        `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n\n# Input shape\n    - If `data_format='channels_last'`:\n        5D tensor with shape:\n        `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n    - If `data_format='channels_first'`:\n        5D tensor with shape:\n        `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`\n\n# Output shape\n    2D tensor with shape:\n    `(batch_size, channels)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "GlobalMaxPool3D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "data_format"
                ],
                "Defaults": [
                    "'channels_last'"
                ],
                "DocStr": "Global max pooling operation for temporal data.\n\n# Arguments\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, steps, features)` while `channels_first`\n        corresponds to inputs with shape\n        `(batch, features, steps)`.\n\n# Input shape\n    - If `data_format='channels_last'`:\n        3D tensor with shape:\n        `(batch_size, steps, features)`\n    - If `data_format='channels_first'`:\n        3D tensor with shape:\n        `(batch_size, features, steps)`\n\n# Output shape\n    2D tensor with shape:\n    `(batch_size, features)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "GlobalMaxPooling1D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Global max pooling operation for spatial data.\n\n# Arguments\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, height, width, channels)` while `channels_first`\n        corresponds to inputs with shape\n        `(batch, channels, height, width)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n\n# Input shape\n    - If `data_format='channels_last'`:\n        4D tensor with shape:\n        `(batch_size, rows, cols, channels)`\n    - If `data_format='channels_first'`:\n        4D tensor with shape:\n        `(batch_size, channels, rows, cols)`\n\n# Output shape\n    2D tensor with shape:\n    `(batch_size, channels)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "GlobalMaxPooling2D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Global Max pooling operation for 3D data.\n\n# Arguments\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n        while `channels_first` corresponds to inputs with shape\n        `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n\n# Input shape\n    - If `data_format='channels_last'`:\n        5D tensor with shape:\n        `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n    - If `data_format='channels_first'`:\n        5D tensor with shape:\n        `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`\n\n# Output shape\n    2D tensor with shape:\n    `(batch_size, channels)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "GlobalMaxPooling3D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "init",
                    "activation",
                    "weights",
                    "W_regularizer",
                    "b_regularizer",
                    "activity_regularizer",
                    "W_constraint",
                    "b_constraint",
                    "bias",
                    "input_dim"
                ],
                "Defaults": [
                    "'glorot_uniform'",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None",
                    " True",
                    " None"
                ],
                "DocStr": "Densely connected highway network.\nHighway layers are a natural extension of LSTMs to feedforward networks.\n# Arguments\n    init: name of initialization function for the weights of the layer\n        (see [initializations](../initializations.md)),\n        or alternatively, Theano function to use for weights\n        initialization. This parameter is only relevant\n        if you don't pass a `weights` argument.\n    activation: name of activation function to use\n        (see [activations](../activations.md)),\n        or alternatively, elementwise Theano function.\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: a(x) = x).\n    weights: list of Numpy arrays to set as initial weights.\n        The list should have 2 elements, of shape `(input_dim, output_dim)`\n        and (output_dim,) for weights and biases respectively.\n    W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n        (eg. L1 or L2 regularization), applied to the main weights matrix.\n    b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n        applied to the bias.\n    activity_regularizer: instance of [ActivityRegularizer](../regularizers.md),\n        applied to the network output.\n    W_constraint: instance of the [constraints](../constraints.md) module\n        (eg. maxnorm, nonneg), applied to the main weights matrix.\n    b_constraint: instance of the [constraints](../constraints.md) module,\n        applied to the bias.\n    bias: whether to include a bias\n        (i.e. make the layer affine rather than linear).\n    input_dim: dimensionality of the input (integer). This argument\n        (or alternatively, the keyword argument `input_shape`)\n        is required when using this layer as the first layer in a model.\n# Input shape\n    2D tensor with shape: `(nb_samples, input_dim)`.\n# Output shape\n    2D tensor with shape: `(nb_samples, input_dim)`.\n# References\n    - [Highway Networks](http://arxiv.org/abs/1505.00387v2)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Highway",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Layer to be used as an entry point into a model.\n\nIt can either wrap an existing tensor (pass an `input_tensor` argument)\nor create its a placeholder tensor (pass arguments `input_shape`\nor `batch_input_shape` as well as `dtype`).\n\n# Arguments\n    input_shape: Shape tuple, not including the batch axis.\n    batch_size: Optional input batch size (integer or None).\n    batch_input_shape: Shape tuple, including the batch axis.\n    dtype: Datatype of the input.\n    input_tensor: Optional tensor to use as layer input\n        instead of creating a placeholder.\n    sparse: Boolean, whether the placeholder created\n        is meant to be sparse.\n    name: Name of the layer (string).",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "InputLayer",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "dtype",
                    "shape",
                    "ndim",
                    "max_ndim",
                    "min_ndim",
                    "axes"
                ],
                "Defaults": [
                    "None",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None"
                ],
                "DocStr": "Specifies the ndim, dtype and shape of every input to a layer.\n\nEvery layer should expose (if appropriate) an `input_spec` attribute:\na list of instances of InputSpec (one per input tensor).\n\nA None entry in a shape is compatible with any dimension,\na None shape is compatible with any shape.\n\n# Arguments\n    dtype: Expected datatype of the input.\n    shape: Shape tuple, expected shape of the input\n        (may include None for unchecked axes).\n    ndim: Integer, expected rank of the input.\n    max_ndim: Integer, maximum rank of the input.\n    min_ndim: Integer, minimum rank of the input.\n    axes: Dictionary mapping integer axes to\n        a specific dimension value.",
                "Functions": [],
                "Name": "InputSpec",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Long Short-Term Memory layer - Hochreiter 1997.\n\n# Arguments\n    units: Positive integer, dimensionality of the output space.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        Default: hyperbolic tangent (`tanh`).\n        If you pass `None`, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    recurrent_activation: Activation function to use\n        for the recurrent step\n        (see [activations](../activations.md)).\n        Default: hard sigmoid (`hard_sigmoid`).\n        If you pass `None`, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix,\n        used for the linear transformation of the inputs.\n        (see [initializers](../initializers.md)).\n    recurrent_initializer: Initializer for the `recurrent_kernel`\n        weights matrix,\n        used for the linear transformation of the recurrent state.\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    unit_forget_bias: Boolean.\n        If True, add 1 to the bias of the forget gate at initialization.\n        Setting it to true will also force `bias_initializer=\"zeros\"`.\n        This is recommended in [Jozefowicz et al.]\n        (http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf).\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    recurrent_regularizer: Regularizer function applied to\n        the `recurrent_kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    activity_regularizer: Regularizer function applied to\n        the output of the layer (its \"activation\").\n        (see [regularizer](../regularizers.md)).\n    kernel_constraint: Constraint function applied to\n        the `kernel` weights matrix\n        (see [constraints](../constraints.md)).\n    recurrent_constraint: Constraint function applied to\n        the `recurrent_kernel` weights matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n    dropout: Float between 0 and 1.\n        Fraction of the units to drop for\n        the linear transformation of the inputs.\n    recurrent_dropout: Float between 0 and 1.\n        Fraction of the units to drop for\n        the linear transformation of the recurrent state.\n    implementation: Implementation mode, either 1 or 2.\n        Mode 1 will structure its operations as a larger number of\n        smaller dot products and additions, whereas mode 2 will\n        batch them into fewer, larger operations. These modes will\n        have different performance profiles on different hardware and\n        for different applications.\n    return_sequences: Boolean. Whether to return the last output\n        in the output sequence, or the full sequence.\n    return_state: Boolean. Whether to return the last state\n        in addition to the output.\n    go_backwards: Boolean (default False).\n        If True, process the input sequence backwards and return the\n        reversed sequence.\n    stateful: Boolean (default False). If True, the last state\n        for each sample at index i in a batch will be used as initial\n        state for the sample of index i in the following batch.\n    unroll: Boolean (default False).\n        If True, the network will be unrolled,\n        else a symbolic loop will be used.\n        Unrolling can speed-up a RNN,\n        although it tends to be more memory-intensive.\n        Unrolling is only suitable for short sequences.\n\n# References\n    - [Long short-term memory]\n      (http://www.bioinf.jku.at/publications/older/2604.pdf)\n    - [Learning to forget: Continual prediction with LSTM]\n      (http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n    - [Supervised sequence labeling with recurrent neural networks]\n      (http://www.cs.toronto.edu/~graves/preprint.pdf)\n    - [A Theoretically Grounded Application of Dropout in\n       Recurrent Neural Networks](https://arxiv.org/abs/1512.05287)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "LSTM",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "units",
                    "activation",
                    "recurrent_activation",
                    "use_bias",
                    "kernel_initializer",
                    "recurrent_initializer",
                    "bias_initializer",
                    "unit_forget_bias",
                    "kernel_regularizer",
                    "recurrent_regularizer",
                    "bias_regularizer",
                    "kernel_constraint",
                    "recurrent_constraint",
                    "bias_constraint",
                    "dropout",
                    "recurrent_dropout",
                    "implementation"
                ],
                "Defaults": [
                    "'tanh'",
                    " 'hard_sigmoid'",
                    " True",
                    " 'glorot_uniform'",
                    " 'orthogonal'",
                    " 'zeros'",
                    " True",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None",
                    " 0.0",
                    " 0.0",
                    " 1"
                ],
                "DocStr": "Cell class for the LSTM layer.\n\n# Arguments\n    units: Positive integer, dimensionality of the output space.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        Default: hyperbolic tangent (`tanh`).\n        If you pass `None`, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    recurrent_activation: Activation function to use\n        for the recurrent step\n        (see [activations](../activations.md)).\n        Default: hard sigmoid (`hard_sigmoid`).\n        If you pass `None`, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).x\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix,\n        used for the linear transformation of the inputs\n        (see [initializers](../initializers.md)).\n    recurrent_initializer: Initializer for the `recurrent_kernel`\n        weights matrix,\n        used for the linear transformation of the recurrent state\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    unit_forget_bias: Boolean.\n        If True, add 1 to the bias of the forget gate at initialization.\n        Setting it to true will also force `bias_initializer=\"zeros\"`.\n        This is recommended in [Jozefowicz et al.]\n        (http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf).\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    recurrent_regularizer: Regularizer function applied to\n        the `recurrent_kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    kernel_constraint: Constraint function applied to\n        the `kernel` weights matrix\n        (see [constraints](../constraints.md)).\n    recurrent_constraint: Constraint function applied to\n        the `recurrent_kernel` weights matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n    dropout: Float between 0 and 1.\n        Fraction of the units to drop for\n        the linear transformation of the inputs.\n    recurrent_dropout: Float between 0 and 1.\n        Fraction of the units to drop for\n        the linear transformation of the recurrent state.\n    implementation: Implementation mode, either 1 or 2.\n        Mode 1 will structure its operations as a larger number of\n        smaller dot products and additions, whereas mode 2 will\n        batch them into fewer, larger operations. These modes will\n        have different performance profiles on different hardware and\n        for different applications.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "LSTMCell",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Wraps arbitrary expression as a `Layer` object.\n\n# Examples\n\n```python\n    # add a x -> x^2 layer\n    model.add(Lambda(lambda x: x ** 2))\n```\n```python\n    # add a layer that returns the concatenation\n    # of the positive part of the input and\n    # the opposite of the negative part\n\n    def antirectifier(x):\n        x -= K.mean(x, axis=1, keepdims=True)\n        x = K.l2_normalize(x, axis=1)\n        pos = K.relu(x)\n        neg = K.relu(-x)\n        return K.concatenate([pos, neg], axis=1)\n\n    def antirectifier_output_shape(input_shape):\n        shape = list(input_shape)\n        assert len(shape) == 2  # only valid for 2D tensors\n        shape[-1] *= 2\n        return tuple(shape)\n\n    model.add(Lambda(antirectifier,\n                     output_shape=antirectifier_output_shape))\n```\n\n# Arguments\n    function: The function to be evaluated.\n        Takes input tensor as first argument.\n    output_shape: Expected output shape from function.\n        Only relevant when using Theano.\n        Can be a tuple or function.\n        If a tuple, it only specifies the first dimension onward;\n             sample dimension is assumed either the same as the input:\n             `output_shape = (input_shape[0], ) + output_shape`\n             or, the input is `None` and\n             the sample dimension is also `None`:\n             `output_shape = (None, ) + output_shape`\n        If a function, it specifies the entire shape as a function of the\n        input shape: `output_shape = f(input_shape)`\n    arguments: optional dictionary of keyword arguments to be passed\n        to the function.\n\n# Input shape\n    Arbitrary. Use the keyword argument input_shape\n    (tuple of integers, does not include the samples axis)\n    when using this layer as the first layer in a model.\n\n# Output shape\n    Specified by `output_shape` argument\n    (or auto-inferred when using TensorFlow or CNTK).",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config",
                            "custom_objects"
                        ],
                        "Defaults": [
                            "None"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Lambda",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self"
                ],
                "DocStr": "Abstract base layer class.\n\n# Properties\n    input, output: Input/output tensor(s). Note that if the layer\n        is used more than once (shared layer), this is ill-defined\n        and will raise an exception. In such cases, use\n        `layer.get_input_at(node_index)`.\n    input_mask, output_mask: Mask tensors. Same caveats apply as\n        input, output.\n    input_shape: Shape tuple. Provided for convenience, but note\n        that there may be cases in which this attribute is\n        ill-defined (e.g. a shared layer with multiple input\n        shapes), in which case requesting `input_shape` will raise\n        an Exception. Prefer using\n        `layer.get_input_shape_at(node_index)`.\n    input_spec: List of InputSpec class instances\n        each entry describes one required input:\n            - ndim\n            - dtype\n        A layer with `n` input tensors must have\n        an `input_spec` of length `n`.\n    name: String, must be unique within a model.\n    non_trainable_weights: List of variables.\n    output_shape: Shape tuple. See `input_shape`.\n    stateful: Boolean indicating whether the layer carries\n        additional non-weight state. Used in, for instance, RNN\n        cells to carry information between batches.\n    supports_masking: Boolean indicator of whether the layer\n        supports masking, typically for unused timesteps in a\n        sequence.\n    trainable: Boolean, whether the layer weights\n        will be updated during training.\n    trainable_weights: List of variables.\n    uses_learning_phase: Whether any operation\n        of the layer uses `K.in_training_phase()`\n        or `K.in_test_phase()`.\n    weights: The concatenation of the lists trainable_weights and\n        non_trainable_weights (in this order).\n\n\n# Methods\n    call(x, mask=None): Where the layer's logic lives.\n    __call__(x, mask=None): Wrapper around the layer logic (`call`).\n        If x is a Keras tensor:\n            - Connect current layer with last layer from tensor:\n                `self._add_inbound_node(last_layer)`\n            - Add layer to tensor history\n        If layer is not built:\n            - Build from x._keras_shape\n    compute_mask(x, mask)\n    compute_output_shape(input_shape)\n    count_params()\n    get_config()\n    get_input_at(node_index)\n    get_input_mask_at(node_index)\n    get_input_shape_at(node_index)\n    get_output_at(node_index)\n    get_output_mask_at(node_index)\n    get_output_shape_at(node_index)\n    get_weights()\n    set_weights(weights)\n\n# Class Methods\n    from_config(config)\n\n# Internal methods:\n    _add_inbound_node(layer, index=0)\n    assert_input_compatibility()\n    build(input_shape)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Layer",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "alpha"
                ],
                "Defaults": [
                    "0.3"
                ],
                "DocStr": "Leaky version of a Rectified Linear Unit.\n\nIt allows a small gradient when the unit is not active:\n`f(x) = alpha * x for x < 0`,\n`f(x) = x for x >= 0`.\n\n# Input shape\n    Arbitrary. Use the keyword argument `input_shape`\n    (tuple of integers, does not include the samples axis)\n    when using this layer as the first layer in a model.\n\n# Output shape\n    Same shape as the input.\n\n# Arguments\n    alpha: float >= 0. Negative slope coefficient.\n\n# References\n    - [Rectifier Nonlinearities Improve Neural Network Acoustic Models]\n      (https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "LeakyReLU",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Locally-connected layer for 1D inputs.\n\nThe `LocallyConnected1D` layer works similarly to\nthe `Conv1D` layer, except that weights are unshared,\nthat is, a different set of filters is applied at each different patch\nof the input.\n\n# Example\n```python\n    # apply a unshared weight convolution 1d of length 3 to a sequence with\n    # 10 timesteps, with 64 output filters\n    model = Sequential()\n    model.add(LocallyConnected1D(64, 3, input_shape=(10, 32)))\n    # now model.output_shape == (None, 8, 64)\n    # add a new conv1d on top\n    model.add(LocallyConnected1D(32, 3))\n    # now model.output_shape == (None, 6, 32)\n```\n\n# Arguments\n    filters: Integer, the dimensionality of the output space\n        (i.e. the number of output filters in the convolution).\n    kernel_size: An integer or tuple/list of a single integer,\n        specifying the length of the 1D convolution window.\n    strides: An integer or tuple/list of a single integer,\n        specifying the stride length of the convolution.\n        Specifying any stride value != 1 is incompatible with specifying\n        any `dilation_rate` value != 1.\n    padding: Currently only supports `\"valid\"` (case-insensitive).\n        `\"same\"` may be supported in the future.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    activity_regularizer: Regularizer function applied to\n        the output of the layer (its \"activation\").\n        (see [regularizer](../regularizers.md)).\n    kernel_constraint: Constraint function applied to the kernel matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n\n# Input shape\n    3D tensor with shape: `(batch_size, steps, input_dim)`\n\n# Output shape\n    3D tensor with shape: `(batch_size, new_steps, filters)`\n    `steps` value might have changed due to padding or strides.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "LocallyConnected1D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Locally-connected layer for 2D inputs.\n\nThe `LocallyConnected2D` layer works similarly\nto the `Conv2D` layer, except that weights are unshared,\nthat is, a different set of filters is applied at each\ndifferent patch of the input.\n\n# Examples\n```python\n    # apply a 3x3 unshared weights convolution with 64 output filters\n    # on a 32x32 image with `data_format=\"channels_last\"`:\n    model = Sequential()\n    model.add(LocallyConnected2D(64, (3, 3), input_shape=(32, 32, 3)))\n    # now model.output_shape == (None, 30, 30, 64)\n    # notice that this layer will consume (30*30)*(3*3*3*64)\n    # + (30*30)*64 parameters\n\n    # add a 3x3 unshared weights convolution on top, with 32 output filters:\n    model.add(LocallyConnected2D(32, (3, 3)))\n    # now model.output_shape == (None, 28, 28, 32)\n```\n\n# Arguments\n    filters: Integer, the dimensionality of the output space\n        (i.e. the number of output filters in the convolution).\n    kernel_size: An integer or tuple/list of 2 integers, specifying the\n        width and height of the 2D convolution window.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n    strides: An integer or tuple/list of 2 integers,\n        specifying the strides of the convolution along the width and height.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n    padding: Currently only support `\"valid\"` (case-insensitive).\n        `\"same\"` will be supported in future.\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, height, width, channels)` while `channels_first`\n        corresponds to inputs with shape\n        `(batch, channels, height, width)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    activity_regularizer: Regularizer function applied to\n        the output of the layer (its \"activation\").\n        (see [regularizer](../regularizers.md)).\n    kernel_constraint: Constraint function applied to the kernel matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n\n# Input shape\n    4D tensor with shape:\n    `(samples, channels, rows, cols)` if data_format='channels_first'\n    or 4D tensor with shape:\n    `(samples, rows, cols, channels)` if data_format='channels_last'.\n\n# Output shape\n    4D tensor with shape:\n    `(samples, filters, new_rows, new_cols)` if data_format='channels_first'\n    or 4D tensor with shape:\n    `(samples, new_rows, new_cols, filters)` if data_format='channels_last'.\n    `rows` and `cols` values might have changed due to padding.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "LocallyConnected2D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "mask_value"
                ],
                "Defaults": [
                    "0.0"
                ],
                "DocStr": "Masks a sequence by using a mask value to skip timesteps.\n\nFor each timestep in the input tensor (dimension #1 in the tensor),\nif all values in the input tensor at that timestep\nare equal to `mask_value`, then the timestep will be masked (skipped)\nin all downstream layers (as long as they support masking).\n\nIf any downstream layer does not support masking yet receives such\nan input mask, an exception will be raised.\n\n# Example\n\nConsider a Numpy data array `x` of shape `(samples, timesteps, features)`,\nto be fed to an LSTM layer.\nYou want to mask timestep #3 and #5 because you lack data for\nthese timesteps. You can:\n\n    - set `x[:, 3, :] = 0.` and `x[:, 5, :] = 0.`\n    - insert a `Masking` layer with `mask_value=0.` before the LSTM layer:\n\n```python\n    model = Sequential()\n    model.add(Masking(mask_value=0., input_shape=(timesteps, features)))\n    model.add(LSTM(32))\n```",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Masking",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Max pooling operation for temporal data.\n\n# Arguments\n    pool_size: Integer, size of the max pooling windows.\n    strides: Integer, or None. Factor by which to downscale.\n        E.g. 2 will halve the input.\n        If None, it will default to `pool_size`.\n    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, steps, features)` while `channels_first`\n        corresponds to inputs with shape\n        `(batch, features, steps)`.\n\n# Input shape\n    - If `data_format='channels_last'`:\n        3D tensor with shape:\n        `(batch_size, steps, features)`\n    - If `data_format='channels_first'`:\n        3D tensor with shape:\n        `(batch_size, features, steps)`\n\n# Output shape\n    - If `data_format='channels_last'`:\n        3D tensor with shape:\n        `(batch_size, downsampled_steps, features)`\n    - If `data_format='channels_first'`:\n        3D tensor with shape:\n        `(batch_size, features, downsampled_steps)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "MaxPool1D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Max pooling operation for spatial data.\n\n# Arguments\n    pool_size: integer or tuple of 2 integers,\n        factors by which to downscale (vertical, horizontal).\n        (2, 2) will halve the input in both spatial dimension.\n        If only one integer is specified, the same window length\n        will be used for both dimensions.\n    strides: Integer, tuple of 2 integers, or None.\n        Strides values.\n        If None, it will default to `pool_size`.\n    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, height, width, channels)` while `channels_first`\n        corresponds to inputs with shape\n        `(batch, channels, height, width)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n\n# Input shape\n    - If `data_format='channels_last'`:\n        4D tensor with shape:\n        `(batch_size, rows, cols, channels)`\n    - If `data_format='channels_first'`:\n        4D tensor with shape:\n        `(batch_size, channels, rows, cols)`\n\n# Output shape\n    - If `data_format='channels_last'`:\n        4D tensor with shape:\n        `(batch_size, pooled_rows, pooled_cols, channels)`\n    - If `data_format='channels_first'`:\n        4D tensor with shape:\n        `(batch_size, channels, pooled_rows, pooled_cols)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "MaxPool2D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Max pooling operation for 3D data (spatial or spatio-temporal).\n\n# Arguments\n    pool_size: tuple of 3 integers,\n        factors by which to downscale (dim1, dim2, dim3).\n        (2, 2, 2) will halve the size of the 3D input in each dimension.\n    strides: tuple of 3 integers, or None. Strides values.\n    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n        while `channels_first` corresponds to inputs with shape\n        `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n\n# Input shape\n    - If `data_format='channels_last'`:\n        5D tensor with shape:\n        `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n    - If `data_format='channels_first'`:\n        5D tensor with shape:\n        `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`\n\n# Output shape\n    - If `data_format='channels_last'`:\n        5D tensor with shape:\n        `(batch_size, pooled_dim1, pooled_dim2, pooled_dim3, channels)`\n    - If `data_format='channels_first'`:\n        5D tensor with shape:\n        `(batch_size, channels, pooled_dim1, pooled_dim2, pooled_dim3)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "MaxPool3D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Max pooling operation for temporal data.\n\n# Arguments\n    pool_size: Integer, size of the max pooling windows.\n    strides: Integer, or None. Factor by which to downscale.\n        E.g. 2 will halve the input.\n        If None, it will default to `pool_size`.\n    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, steps, features)` while `channels_first`\n        corresponds to inputs with shape\n        `(batch, features, steps)`.\n\n# Input shape\n    - If `data_format='channels_last'`:\n        3D tensor with shape:\n        `(batch_size, steps, features)`\n    - If `data_format='channels_first'`:\n        3D tensor with shape:\n        `(batch_size, features, steps)`\n\n# Output shape\n    - If `data_format='channels_last'`:\n        3D tensor with shape:\n        `(batch_size, downsampled_steps, features)`\n    - If `data_format='channels_first'`:\n        3D tensor with shape:\n        `(batch_size, features, downsampled_steps)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "MaxPooling1D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Max pooling operation for spatial data.\n\n# Arguments\n    pool_size: integer or tuple of 2 integers,\n        factors by which to downscale (vertical, horizontal).\n        (2, 2) will halve the input in both spatial dimension.\n        If only one integer is specified, the same window length\n        will be used for both dimensions.\n    strides: Integer, tuple of 2 integers, or None.\n        Strides values.\n        If None, it will default to `pool_size`.\n    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, height, width, channels)` while `channels_first`\n        corresponds to inputs with shape\n        `(batch, channels, height, width)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n\n# Input shape\n    - If `data_format='channels_last'`:\n        4D tensor with shape:\n        `(batch_size, rows, cols, channels)`\n    - If `data_format='channels_first'`:\n        4D tensor with shape:\n        `(batch_size, channels, rows, cols)`\n\n# Output shape\n    - If `data_format='channels_last'`:\n        4D tensor with shape:\n        `(batch_size, pooled_rows, pooled_cols, channels)`\n    - If `data_format='channels_first'`:\n        4D tensor with shape:\n        `(batch_size, channels, pooled_rows, pooled_cols)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "MaxPooling2D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Max pooling operation for 3D data (spatial or spatio-temporal).\n\n# Arguments\n    pool_size: tuple of 3 integers,\n        factors by which to downscale (dim1, dim2, dim3).\n        (2, 2, 2) will halve the size of the 3D input in each dimension.\n    strides: tuple of 3 integers, or None. Strides values.\n    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n        while `channels_first` corresponds to inputs with shape\n        `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n\n# Input shape\n    - If `data_format='channels_last'`:\n        5D tensor with shape:\n        `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n    - If `data_format='channels_first'`:\n        5D tensor with shape:\n        `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`\n\n# Output shape\n    - If `data_format='channels_last'`:\n        5D tensor with shape:\n        `(batch_size, pooled_dim1, pooled_dim2, pooled_dim3, channels)`\n    - If `data_format='channels_first'`:\n        5D tensor with shape:\n        `(batch_size, channels, pooled_dim1, pooled_dim2, pooled_dim3)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "MaxPooling3D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self"
                ],
                "DocStr": "Layer that computes the maximum (element-wise) a list of inputs.\n\nIt takes as input a list of tensors,\nall of the same shape, and returns\na single tensor (also of the same shape).",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Maximum",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "output_dim",
                    "nb_feature",
                    "init",
                    "weights",
                    "W_regularizer",
                    "b_regularizer",
                    "activity_regularizer",
                    "W_constraint",
                    "b_constraint",
                    "bias",
                    "input_dim"
                ],
                "Defaults": [
                    "4",
                    " 'glorot_uniform'",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None",
                    " True",
                    " None"
                ],
                "DocStr": "A dense maxout layer.\nA `MaxoutDense` layer takes the element-wise maximum of\n`nb_feature` `Dense(input_dim, output_dim)` linear layers.\nThis allows the layer to learn a convex,\npiecewise linear activation function over the inputs.\nNote that this is a *linear* layer;\nif you wish to apply activation function\n(you shouldn't need to --they are universal function approximators),\nan `Activation` layer must be added after.\n# Arguments\n    output_dim: int > 0.\n    nb_feature: number of Dense layers to use internally.\n    init: name of initialization function for the weights of the layer\n        (see [initializations](../initializations.md)),\n        or alternatively, Theano function to use for weights\n        initialization. This parameter is only relevant\n        if you don't pass a `weights` argument.\n    weights: list of Numpy arrays to set as initial weights.\n        The list should have 2 elements, of shape `(input_dim, output_dim)`\n        and (output_dim,) for weights and biases respectively.\n    W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n        (eg. L1 or L2 regularization), applied to the main weights matrix.\n    b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n        applied to the bias.\n    activity_regularizer: instance of [ActivityRegularizer](../regularizers.md),\n        applied to the network output.\n    W_constraint: instance of the [constraints](../constraints.md) module\n        (eg. maxnorm, nonneg), applied to the main weights matrix.\n    b_constraint: instance of the [constraints](../constraints.md) module,\n        applied to the bias.\n    bias: whether to include a bias\n        (i.e. make the layer affine rather than linear).\n    input_dim: dimensionality of the input (integer). This argument\n        (or alternatively, the keyword argument `input_shape`)\n        is required when using this layer as the first layer in a model.\n# Input shape\n    2D tensor with shape: `(nb_samples, input_dim)`.\n# Output shape\n    2D tensor with shape: `(nb_samples, output_dim)`.\n# References\n    - [Maxout Networks](http://arxiv.org/abs/1302.4389)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "MaxoutDense",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self"
                ],
                "DocStr": "Layer that computes the minimum (element-wise) a list of inputs.\n\nIt takes as input a list of tensors,\nall of the same shape, and returns\na single tensor (also of the same shape).",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Minimum",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self"
                ],
                "DocStr": "Layer that multiplies (element-wise) a list of inputs.\n\nIt takes as input a list of tensors,\nall of the same shape, and returns\na single tensor (also of the same shape).",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Multiply",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Parametric Rectified Linear Unit.\n\nIt follows:\n`f(x) = alpha * x for x < 0`,\n`f(x) = x for x >= 0`,\nwhere `alpha` is a learned array with the same shape as x.\n\n# Input shape\n    Arbitrary. Use the keyword argument `input_shape`\n    (tuple of integers, does not include the samples axis)\n    when using this layer as the first layer in a model.\n\n# Output shape\n    Same shape as the input.\n\n# Arguments\n    alpha_initializer: initializer function for the weights.\n    alpha_regularizer: regularizer for the weights.\n    alpha_constraint: constraint for the weights.\n    shared_axes: the axes along which to share learnable\n        parameters for the activation function.\n        For example, if the incoming feature maps\n        are from a 2D convolution\n        with output shape `(batch, height, width, channels)`,\n        and you wish to share parameters across space\n        so that each filter only has one set of parameters,\n        set `shared_axes=[1, 2]`.\n\n# References\n    - [Delving Deep into Rectifiers: Surpassing Human-Level Performance on\n       ImageNet Classification](https://arxiv.org/abs/1502.01852)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "PReLU",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "dims"
                ],
                "DocStr": "Permutes the dimensions of the input according to a given pattern.\n\nUseful for e.g. connecting RNNs and convnets together.\n\n# Example\n\n```python\n    model = Sequential()\n    model.add(Permute((2, 1), input_shape=(10, 64)))\n    # now: model.output_shape == (None, 64, 10)\n    # note: `None` is the batch dimension\n```\n\n# Arguments\n    dims: Tuple of integers. Permutation pattern, does not include the\n        samples dimension. Indexing starts at 1.\n        For instance, `(2, 1)` permutes the first and second dimension\n        of the input.\n\n# Input shape\n    Arbitrary. Use the keyword argument `input_shape`\n    (tuple of integers, does not include the samples axis)\n    when using this layer as the first layer in a model.\n\n# Output shape\n    Same as the input shape, but with the dimensions re-ordered according\n    to the specified pattern.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Permute",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "cell",
                    "return_sequences",
                    "return_state",
                    "go_backwards",
                    "stateful",
                    "unroll"
                ],
                "Defaults": [
                    "False",
                    " False",
                    " False",
                    " False",
                    " False"
                ],
                "DocStr": "Base class for recurrent layers.\n\n# Arguments\n    cell: A RNN cell instance. A RNN cell is a class that has:\n        - a `call(input_at_t, states_at_t)` method, returning\n            `(output_at_t, states_at_t_plus_1)`. The call method of the\n            cell can also take the optional argument `constants`, see\n            section \"Note on passing external constants\" below.\n        - a `state_size` attribute. This can be a single integer\n            (single state) in which case it is\n            the size of the recurrent state\n            (which should be the same as the size of the cell output).\n            This can also be a list/tuple of integers\n            (one size per state).\n        - a `output_size` attribute. This can be a single integer or a\n            TensorShape, which represent the shape of the output. For\n            backward compatible reason, if this attribute is not available\n            for the cell, the value will be inferred by the first element\n            of the `state_size`.\n        It is also possible for `cell` to be a list of RNN cell instances,\n        in which cases the cells get stacked on after the other in the RNN,\n        implementing an efficient stacked RNN.\n    return_sequences: Boolean. Whether to return the last output\n        in the output sequence, or the full sequence.\n    return_state: Boolean. Whether to return the last state\n        in addition to the output.\n    go_backwards: Boolean (default False).\n        If True, process the input sequence backwards and return the\n        reversed sequence.\n    stateful: Boolean (default False). If True, the last state\n        for each sample at index i in a batch will be used as initial\n        state for the sample of index i in the following batch.\n    unroll: Boolean (default False).\n        If True, the network will be unrolled,\n        else a symbolic loop will be used.\n        Unrolling can speed-up a RNN,\n        although it tends to be more memory-intensive.\n        Unrolling is only suitable for short sequences.\n    input_dim: dimensionality of the input (integer).\n        This argument (or alternatively,\n        the keyword argument `input_shape`)\n        is required when using this layer as the first layer in a model.\n    input_length: Length of input sequences, to be specified\n        when it is constant.\n        This argument is required if you are going to connect\n        `Flatten` then `Dense` layers upstream\n        (without it, the shape of the dense outputs cannot be computed).\n        Note that if the recurrent layer is not the first layer\n        in your model, you would need to specify the input length\n        at the level of the first layer\n        (e.g. via the `input_shape` argument)\n\n# Input shape\n    3D tensor with shape `(batch_size, timesteps, input_dim)`.\n\n# Output shape\n    - if `return_state`: a list of tensors. The first tensor is\n        the output. The remaining tensors are the last states,\n        each with shape `(batch_size, units)`.\n    - if `return_sequences`: 3D tensor with shape\n        `(batch_size, timesteps, units)`.\n    - else, 2D tensor with shape `(batch_size, units)`.\n\n# Masking\n    This layer supports masking for input data with a variable number\n    of timesteps. To introduce masks to your data,\n    use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n    set to `True`.\n\n# Note on using statefulness in RNNs\n    You can set RNN layers to be 'stateful', which means that the states\n    computed for the samples in one batch will be reused as initial states\n    for the samples in the next batch. This assumes a one-to-one mapping\n    between samples in different successive batches.\n\n    To enable statefulness:\n        - specify `stateful=True` in the layer constructor.\n        - specify a fixed batch size for your model, by passing\n            if sequential model:\n              `batch_input_shape=(...)` to the first layer in your model.\n            else for functional model with 1 or more Input layers:\n              `batch_shape=(...)` to all the first layers in your model.\n            This is the expected shape of your inputs\n            *including the batch size*.\n            It should be a tuple of integers, e.g. `(32, 10, 100)`.\n        - specify `shuffle=False` when calling fit().\n\n    To reset the states of your model, call `.reset_states()` on either\n    a specific layer, or on your entire model.\n\n# Note on specifying the initial state of RNNs\n    You can specify the initial state of RNN layers symbolically by\n    calling them with the keyword argument `initial_state`. The value of\n    `initial_state` should be a tensor or list of tensors representing\n    the initial state of the RNN layer.\n\n    You can specify the initial state of RNN layers numerically by\n    calling `reset_states` with the keyword argument `states`. The value of\n    `states` should be a numpy array or list of numpy arrays representing\n    the initial state of the RNN layer.\n\n# Note on passing external constants to RNNs\n    You can pass \"external\" constants to the cell using the `constants`\n    keyword argument of `RNN.__call__` (as well as `RNN.call`) method. This\n    requires that the `cell.call` method accepts the same keyword argument\n    `constants`. Such constants can be used to condition the cell\n    transformation on additional static inputs (not changing over time),\n    a.k.a. an attention mechanism.\n\n# Examples\n\n```python\n    # First, let's define a RNN Cell, as a layer subclass.\n\n    class MinimalRNNCell(keras.layers.Layer):\n\n        def __init__(self, units, **kwargs):\n            self.units = units\n            self.state_size = units\n            super(MinimalRNNCell, self).__init__(**kwargs)\n\n        def build(self, input_shape):\n            self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n                                          initializer='uniform',\n                                          name='kernel')\n            self.recurrent_kernel = self.add_weight(\n                shape=(self.units, self.units),\n                initializer='uniform',\n                name='recurrent_kernel')\n            self.built = True\n\n        def call(self, inputs, states):\n            prev_output = states[0]\n            h = K.dot(inputs, self.kernel)\n            output = h + K.dot(prev_output, self.recurrent_kernel)\n            return output, [output]\n\n    # Let's use this cell in a RNN layer:\n\n    cell = MinimalRNNCell(32)\n    x = keras.Input((None, 5))\n    layer = RNN(cell)\n    y = layer(x)\n\n    # Here's how to use the cell to build a stacked RNN:\n\n    cells = [MinimalRNNCell(32), MinimalRNNCell(64)]\n    x = keras.Input((None, 5))\n    layer = RNN(cells)\n    y = layer(x)\n```",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config",
                            "custom_objects"
                        ],
                        "Defaults": [
                            "None"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "RNN",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "max_value",
                    "negative_slope",
                    "threshold"
                ],
                "Defaults": [
                    "None",
                    " 0.0",
                    " 0.0"
                ],
                "DocStr": "Rectified Linear Unit activation function.\n\nWith default values, it returns element-wise `max(x, 0)`.\n\nOtherwise, it follows:\n`f(x) = max_value` for `x >= max_value`,\n`f(x) = x` for `threshold <= x < max_value`,\n`f(x) = negative_slope * (x - threshold)` otherwise.\n\n# Input shape\n    Arbitrary. Use the keyword argument `input_shape`\n    (tuple of integers, does not include the samples axis)\n    when using this layer as the first layer in a model.\n\n# Output shape\n    Same shape as the input.\n\n# Arguments\n    max_value: float >= 0. Maximum activation value.\n    negative_slope: float >= 0. Negative slope coefficient.\n    threshold: float. Threshold value for thresholded activation.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "ReLU",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "return_sequences",
                    "return_state",
                    "go_backwards",
                    "stateful",
                    "unroll",
                    "implementation"
                ],
                "Defaults": [
                    "False",
                    " False",
                    " False",
                    " False",
                    " False",
                    " 0"
                ],
                "DocStr": "Abstract base class for recurrent layers.\n\nDo not use in a model -- it's not a valid layer!\nUse its children classes `LSTM`, `GRU` and `SimpleRNN` instead.\nAll recurrent layers (`LSTM`, `GRU`, `SimpleRNN`) also\nfollow the specifications of this class and accept\nthe keyword arguments listed below.\n\n# Example\n\n```python\n    # as the first layer in a Sequential model\n    model = Sequential()\n    model.add(LSTM(32, input_shape=(10, 64)))\n    # now model.output_shape == (None, 32)\n    # note: `None` is the batch dimension.\n    # for subsequent layers, no need to specify the input size:\n    model.add(LSTM(16))\n    # to stack recurrent layers, you must use return_sequences=True\n    # on any recurrent layer that feeds into another recurrent layer.\n    # note that you only need to specify the input size on the first layer.\n    model = Sequential()\n    model.add(LSTM(64, input_dim=64, input_length=10, return_sequences=True))\n    model.add(LSTM(32, return_sequences=True))\n    model.add(LSTM(10))\n```\n\n# Arguments\n    weights: list of Numpy arrays to set as initial weights.\n        The list should have 3 elements, of shapes:\n        `[(input_dim, output_dim), (output_dim, output_dim), (output_dim,)]`.\n    return_sequences: Boolean. Whether to return the last output\n        in the output sequence, or the full sequence.\n    return_state: Boolean. Whether to return the last state\n        in addition to the output.\n    go_backwards: Boolean (default False).\n        If True, process the input sequence backwards and return the\n        reversed sequence.\n    stateful: Boolean (default False). If True, the last state\n        for each sample at index i in a batch will be used as initial\n        state for the sample of index i in the following batch.\n    unroll: Boolean (default False).\n        If True, the network will be unrolled,\n        else a symbolic loop will be used.\n        Unrolling can speed-up a RNN,\n        although it tends to be more memory-intensive.\n        Unrolling is only suitable for short sequences.\n    implementation: one of {0, 1, or 2}.\n        If set to 0, the RNN will use\n        an implementation that uses fewer, larger matrix products,\n        thus running faster on CPU but consuming more memory.\n        If set to 1, the RNN will use more matrix products,\n        but smaller ones, thus running slower\n        (may actually be faster on GPU) while consuming less memory.\n        If set to 2 (LSTM/GRU only),\n        the RNN will combine the input gate,\n        the forget gate and the output gate into a single matrix,\n        enabling more time-efficient parallelization on the GPU.\n        Note: RNN dropout must be shared for all gates,\n        resulting in a slightly reduced regularization.\n    input_dim: dimensionality of the input (integer).\n        This argument (or alternatively, the keyword argument `input_shape`)\n        is required when using this layer as the first layer in a model.\n    input_length: Length of input sequences, to be specified\n        when it is constant.\n        This argument is required if you are going to connect\n        `Flatten` then `Dense` layers upstream\n        (without it, the shape of the dense outputs cannot be computed).\n        Note that if the recurrent layer is not the first layer\n        in your model, you would need to specify the input length\n        at the level of the first layer\n        (e.g. via the `input_shape` argument)\n\n# Input shapes\n    3D tensor with shape `(batch_size, timesteps, input_dim)`,\n    (Optional) 2D tensors with shape `(batch_size, output_dim)`.\n\n# Output shape\n    - if `return_state`: a list of tensors. The first tensor is\n        the output. The remaining tensors are the last states,\n        each with shape `(batch_size, units)`.\n    - if `return_sequences`: 3D tensor with shape\n        `(batch_size, timesteps, units)`.\n    - else, 2D tensor with shape `(batch_size, units)`.\n\n# Masking\n    This layer supports masking for input data with a variable number\n    of timesteps. To introduce masks to your data,\n    use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n    set to `True`.\n\n# Note on using statefulness in RNNs\n    You can set RNN layers to be 'stateful', which means that the states\n    computed for the samples in one batch will be reused as initial states\n    for the samples in the next batch. This assumes a one-to-one mapping\n    between samples in different successive batches.\n    To enable statefulness:\n        - specify `stateful=True` in the layer constructor.\n        - specify a fixed batch size for your model, by passing\n            if sequential model:\n              `batch_input_shape=(...)` to the first layer in your model.\n            else for functional model with 1 or more Input layers:\n              `batch_shape=(...)` to all the first layers in your model.\n            This is the expected shape of your inputs\n            *including the batch size*.\n            It should be a tuple of integers, e.g. `(32, 10, 100)`.\n        - specify `shuffle=False` when calling fit().\n    To reset the states of your model, call `.reset_states()` on either\n    a specific layer, or on your entire model.\n\n# Note on specifying the initial state of RNNs\n    You can specify the initial state of RNN layers symbolically by\n    calling them with the keyword argument `initial_state`. The value of\n    `initial_state` should be a tensor or list of tensors representing\n    the initial state of the RNN layer.\n    You can specify the initial state of RNN layers numerically by\n    calling `reset_states` with the keyword argument `states`. The value of\n    `states` should be a numpy array or list of numpy arrays representing\n    the initial state of the RNN layer.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Recurrent",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "n"
                ],
                "DocStr": "Repeats the input n times.\n\n# Example\n\n```python\n    model = Sequential()\n    model.add(Dense(32, input_dim=32))\n    # now: model.output_shape == (None, 32)\n    # note: `None` is the batch dimension\n\n    model.add(RepeatVector(3))\n    # now: model.output_shape == (None, 3, 32)\n```\n\n# Arguments\n    n: integer, repetition factor.\n\n# Input shape\n    2D tensor of shape `(num_samples, features)`.\n\n# Output shape\n    3D tensor of shape `(num_samples, n, features)`.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "RepeatVector",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "target_shape"
                ],
                "DocStr": "Reshapes an output to a certain shape.\n\n# Arguments\n    target_shape: target shape. Tuple of integers.\n        Does not include the batch axis.\n\n# Input shape\n    Arbitrary, although all dimensions in the input shaped must be fixed.\n    Use the keyword argument `input_shape`\n    (tuple of integers, does not include the batch axis)\n    when using this layer as the first layer in a model.\n\n# Output shape\n    `(batch_size,) + target_shape`\n\n# Example\n\n```python\n    # as first layer in a Sequential model\n    model = Sequential()\n    model.add(Reshape((3, 4), input_shape=(12,)))\n    # now: model.output_shape == (None, 3, 4)\n    # note: `None` is the batch dimension\n\n    # as intermediate layer in a Sequential model\n    model.add(Reshape((6, 2)))\n    # now: model.output_shape == (None, 6, 2)\n\n    # also supports shape inference using `-1` as dimension\n    model.add(Reshape((-1, 2, 2)))\n    # now: model.output_shape == (None, 3, 2, 2)\n```",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Reshape",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "filters",
                    "kernel_size",
                    "strides",
                    "padding",
                    "data_format",
                    "dilation_rate",
                    "depth_multiplier",
                    "activation",
                    "use_bias",
                    "depthwise_initializer",
                    "pointwise_initializer",
                    "bias_initializer",
                    "depthwise_regularizer",
                    "pointwise_regularizer",
                    "bias_regularizer",
                    "activity_regularizer",
                    "depthwise_constraint",
                    "pointwise_constraint",
                    "bias_constraint"
                ],
                "Defaults": [
                    "1",
                    " 'valid'",
                    " 'channels_last'",
                    " 1",
                    " 1",
                    " None",
                    " True",
                    " 'glorot_uniform'",
                    " 'glorot_uniform'",
                    " 'zeros'",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None"
                ],
                "DocStr": "Depthwise separable 1D convolution.\n\nSeparable convolutions consist in first performing\na depthwise spatial convolution\n(which acts on each input channel separately)\nfollowed by a pointwise convolution which mixes together the resulting\noutput channels. The `depth_multiplier` argument controls how many\noutput channels are generated per input channel in the depthwise step.\n\nIntuitively, separable convolutions can be understood as\na way to factorize a convolution kernel into two smaller kernels,\nor as an extreme version of an Inception block.\n\n# Arguments\n    filters: Integer, the dimensionality of the output space\n        (i.e. the number of output filters in the convolution).\n    kernel_size: An integer or tuple/list of single integer,\n        specifying the length of the 1D convolution window.\n    strides: An integer or tuple/list of single integer,\n        specifying the stride length of the convolution.\n        Specifying any stride value != 1 is incompatible with specifying\n        any `dilation_rate` value != 1.\n    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n    data_format: A string,\n        one of `\"channels_last\"` or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs.\n        `\"channels_last\"` corresponds to inputs with shape\n        `(batch, steps, channels)` while `\"channels_first\"`\n        corresponds to inputs with shape\n        `(batch, channels, steps)`.\n    dilation_rate: An integer or tuple/list of a single integer, specifying\n        the dilation rate to use for dilated convolution.\n        Currently, specifying any `dilation_rate` value != 1 is\n        incompatible with specifying any `strides` value != 1.\n    depth_multiplier: The number of depthwise convolution output channels\n        for each input channel.\n        The total number of depthwise convolution output\n        channels will be equal to `filters_in * depth_multiplier`.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    depthwise_initializer: Initializer for the depthwise kernel matrix\n        (see [initializers](../initializers.md)).\n    pointwise_initializer: Initializer for the pointwise kernel matrix\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    depthwise_regularizer: Regularizer function applied to\n        the depthwise kernel matrix\n        (see [regularizer](../regularizers.md)).\n    pointwise_regularizer: Regularizer function applied to\n        the pointwise kernel matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    activity_regularizer: Regularizer function applied to\n        the output of the layer (its \"activation\").\n        (see [regularizer](../regularizers.md)).\n    depthwise_constraint: Constraint function applied to\n        the depthwise kernel matrix\n        (see [constraints](../constraints.md)).\n    pointwise_constraint: Constraint function applied to\n        the pointwise kernel matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n\n# Input shape\n    3D tensor with shape:\n    `(batch, channels, steps)`\n    if `data_format` is `\"channels_first\"`\n    or 3D tensor with shape:\n    `(batch, steps, channels)`\n    if `data_format` is `\"channels_last\"`.\n\n# Output shape\n    3D tensor with shape:\n    `(batch, filters, new_steps)`\n    if `data_format` is `\"channels_first\"`\n    or 3D tensor with shape:\n    `(batch, new_steps, filters)`\n    if `data_format` is `\"channels_last\"`.\n    `new_steps` values might have changed due to padding or strides.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "SeparableConv1D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Depthwise separable 2D convolution.\n\nSeparable convolutions consist in first performing\na depthwise spatial convolution\n(which acts on each input channel separately)\nfollowed by a pointwise convolution which mixes together the resulting\noutput channels. The `depth_multiplier` argument controls how many\noutput channels are generated per input channel in the depthwise step.\n\nIntuitively, separable convolutions can be understood as\na way to factorize a convolution kernel into two smaller kernels,\nor as an extreme version of an Inception block.\n\n# Arguments\n    filters: Integer, the dimensionality of the output space\n        (i.e. the number of output filters in the convolution).\n    kernel_size: An integer or tuple/list of 2 integers, specifying the\n        height and width of the 2D convolution window.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n    strides: An integer or tuple/list of 2 integers,\n        specifying the strides of the convolution\n        along the height and width.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n        Specifying any stride value != 1 is incompatible with specifying\n        any `dilation_rate` value != 1.\n    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n    data_format: A string,\n        one of `\"channels_last\"` or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs.\n        `\"channels_last\"` corresponds to inputs with shape\n        `(batch, height, width, channels)` while `\"channels_first\"`\n        corresponds to inputs with shape\n        `(batch, channels, height, width)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n    dilation_rate: An integer or tuple/list of 2 integers, specifying\n        the dilation rate to use for dilated convolution.\n        Currently, specifying any `dilation_rate` value != 1 is\n        incompatible with specifying any `strides` value != 1.\n    depth_multiplier: The number of depthwise convolution output channels\n        for each input channel.\n        The total number of depthwise convolution output\n        channels will be equal to `filters_in * depth_multiplier`.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    depthwise_initializer: Initializer for the depthwise kernel matrix\n        (see [initializers](../initializers.md)).\n    pointwise_initializer: Initializer for the pointwise kernel matrix\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    depthwise_regularizer: Regularizer function applied to\n        the depthwise kernel matrix\n        (see [regularizer](../regularizers.md)).\n    pointwise_regularizer: Regularizer function applied to\n        the pointwise kernel matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    activity_regularizer: Regularizer function applied to\n        the output of the layer (its \"activation\").\n        (see [regularizer](../regularizers.md)).\n    depthwise_constraint: Constraint function applied to\n        the depthwise kernel matrix\n        (see [constraints](../constraints.md)).\n    pointwise_constraint: Constraint function applied to\n        the pointwise kernel matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n\n# Input shape\n    4D tensor with shape:\n    `(batch, channels, rows, cols)`\n    if `data_format` is `\"channels_first\"`\n    or 4D tensor with shape:\n    `(batch, rows, cols, channels)`\n    if `data_format` is `\"channels_last\"`.\n\n# Output shape\n    4D tensor with shape:\n    `(batch, filters, new_rows, new_cols)`\n    if `data_format` is `\"channels_first\"`\n    or 4D tensor with shape:\n    `(batch, new_rows, new_cols, filters)`\n    if `data_format` is `\"channels_last\"`.\n    `rows` and `cols` values might have changed due to padding.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "SeparableConv2D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "filters",
                    "kernel_size",
                    "strides",
                    "padding",
                    "data_format",
                    "dilation_rate",
                    "depth_multiplier",
                    "activation",
                    "use_bias",
                    "depthwise_initializer",
                    "pointwise_initializer",
                    "bias_initializer",
                    "depthwise_regularizer",
                    "pointwise_regularizer",
                    "bias_regularizer",
                    "activity_regularizer",
                    "depthwise_constraint",
                    "pointwise_constraint",
                    "bias_constraint"
                ],
                "Defaults": [
                    "1",
                    " 'valid'",
                    " 'channels_last'",
                    " 1",
                    " 1",
                    " None",
                    " True",
                    " 'glorot_uniform'",
                    " 'glorot_uniform'",
                    " 'zeros'",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None"
                ],
                "DocStr": "Depthwise separable 1D convolution.\n\nSeparable convolutions consist in first performing\na depthwise spatial convolution\n(which acts on each input channel separately)\nfollowed by a pointwise convolution which mixes together the resulting\noutput channels. The `depth_multiplier` argument controls how many\noutput channels are generated per input channel in the depthwise step.\n\nIntuitively, separable convolutions can be understood as\na way to factorize a convolution kernel into two smaller kernels,\nor as an extreme version of an Inception block.\n\n# Arguments\n    filters: Integer, the dimensionality of the output space\n        (i.e. the number of output filters in the convolution).\n    kernel_size: An integer or tuple/list of single integer,\n        specifying the length of the 1D convolution window.\n    strides: An integer or tuple/list of single integer,\n        specifying the stride length of the convolution.\n        Specifying any stride value != 1 is incompatible with specifying\n        any `dilation_rate` value != 1.\n    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n    data_format: A string,\n        one of `\"channels_last\"` or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs.\n        `\"channels_last\"` corresponds to inputs with shape\n        `(batch, steps, channels)` while `\"channels_first\"`\n        corresponds to inputs with shape\n        `(batch, channels, steps)`.\n    dilation_rate: An integer or tuple/list of a single integer, specifying\n        the dilation rate to use for dilated convolution.\n        Currently, specifying any `dilation_rate` value != 1 is\n        incompatible with specifying any `strides` value != 1.\n    depth_multiplier: The number of depthwise convolution output channels\n        for each input channel.\n        The total number of depthwise convolution output\n        channels will be equal to `filters_in * depth_multiplier`.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    depthwise_initializer: Initializer for the depthwise kernel matrix\n        (see [initializers](../initializers.md)).\n    pointwise_initializer: Initializer for the pointwise kernel matrix\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    depthwise_regularizer: Regularizer function applied to\n        the depthwise kernel matrix\n        (see [regularizer](../regularizers.md)).\n    pointwise_regularizer: Regularizer function applied to\n        the pointwise kernel matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    activity_regularizer: Regularizer function applied to\n        the output of the layer (its \"activation\").\n        (see [regularizer](../regularizers.md)).\n    depthwise_constraint: Constraint function applied to\n        the depthwise kernel matrix\n        (see [constraints](../constraints.md)).\n    pointwise_constraint: Constraint function applied to\n        the pointwise kernel matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n\n# Input shape\n    3D tensor with shape:\n    `(batch, channels, steps)`\n    if `data_format` is `\"channels_first\"`\n    or 3D tensor with shape:\n    `(batch, steps, channels)`\n    if `data_format` is `\"channels_last\"`.\n\n# Output shape\n    3D tensor with shape:\n    `(batch, filters, new_steps)`\n    if `data_format` is `\"channels_first\"`\n    or 3D tensor with shape:\n    `(batch, new_steps, filters)`\n    if `data_format` is `\"channels_last\"`.\n    `new_steps` values might have changed due to padding or strides.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "SeparableConvolution1D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Depthwise separable 2D convolution.\n\nSeparable convolutions consist in first performing\na depthwise spatial convolution\n(which acts on each input channel separately)\nfollowed by a pointwise convolution which mixes together the resulting\noutput channels. The `depth_multiplier` argument controls how many\noutput channels are generated per input channel in the depthwise step.\n\nIntuitively, separable convolutions can be understood as\na way to factorize a convolution kernel into two smaller kernels,\nor as an extreme version of an Inception block.\n\n# Arguments\n    filters: Integer, the dimensionality of the output space\n        (i.e. the number of output filters in the convolution).\n    kernel_size: An integer or tuple/list of 2 integers, specifying the\n        height and width of the 2D convolution window.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n    strides: An integer or tuple/list of 2 integers,\n        specifying the strides of the convolution\n        along the height and width.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n        Specifying any stride value != 1 is incompatible with specifying\n        any `dilation_rate` value != 1.\n    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n    data_format: A string,\n        one of `\"channels_last\"` or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs.\n        `\"channels_last\"` corresponds to inputs with shape\n        `(batch, height, width, channels)` while `\"channels_first\"`\n        corresponds to inputs with shape\n        `(batch, channels, height, width)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n    dilation_rate: An integer or tuple/list of 2 integers, specifying\n        the dilation rate to use for dilated convolution.\n        Currently, specifying any `dilation_rate` value != 1 is\n        incompatible with specifying any `strides` value != 1.\n    depth_multiplier: The number of depthwise convolution output channels\n        for each input channel.\n        The total number of depthwise convolution output\n        channels will be equal to `filters_in * depth_multiplier`.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    depthwise_initializer: Initializer for the depthwise kernel matrix\n        (see [initializers](../initializers.md)).\n    pointwise_initializer: Initializer for the pointwise kernel matrix\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    depthwise_regularizer: Regularizer function applied to\n        the depthwise kernel matrix\n        (see [regularizer](../regularizers.md)).\n    pointwise_regularizer: Regularizer function applied to\n        the pointwise kernel matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    activity_regularizer: Regularizer function applied to\n        the output of the layer (its \"activation\").\n        (see [regularizer](../regularizers.md)).\n    depthwise_constraint: Constraint function applied to\n        the depthwise kernel matrix\n        (see [constraints](../constraints.md)).\n    pointwise_constraint: Constraint function applied to\n        the pointwise kernel matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n\n# Input shape\n    4D tensor with shape:\n    `(batch, channels, rows, cols)`\n    if `data_format` is `\"channels_first\"`\n    or 4D tensor with shape:\n    `(batch, rows, cols, channels)`\n    if `data_format` is `\"channels_last\"`.\n\n# Output shape\n    4D tensor with shape:\n    `(batch, filters, new_rows, new_cols)`\n    if `data_format` is `\"channels_first\"`\n    or 4D tensor with shape:\n    `(batch, new_rows, new_cols, filters)`\n    if `data_format` is `\"channels_last\"`.\n    `rows` and `cols` values might have changed due to padding.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "SeparableConvolution2D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Fully-connected RNN where the output is to be fed back to input.\n\n# Arguments\n    units: Positive integer, dimensionality of the output space.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        Default: hyperbolic tangent (`tanh`).\n        If you pass `None`, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix,\n        used for the linear transformation of the inputs\n        (see [initializers](../initializers.md)).\n    recurrent_initializer: Initializer for the `recurrent_kernel`\n        weights matrix,\n        used for the linear transformation of the recurrent state\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    recurrent_regularizer: Regularizer function applied to\n        the `recurrent_kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    activity_regularizer: Regularizer function applied to\n        the output of the layer (its \"activation\").\n        (see [regularizer](../regularizers.md)).\n    kernel_constraint: Constraint function applied to\n        the `kernel` weights matrix\n        (see [constraints](../constraints.md)).\n    recurrent_constraint: Constraint function applied to\n        the `recurrent_kernel` weights matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n    dropout: Float between 0 and 1.\n        Fraction of the units to drop for\n        the linear transformation of the inputs.\n    recurrent_dropout: Float between 0 and 1.\n        Fraction of the units to drop for\n        the linear transformation of the recurrent state.\n    return_sequences: Boolean. Whether to return the last output\n        in the output sequence, or the full sequence.\n    return_state: Boolean. Whether to return the last state\n        in addition to the output.\n    go_backwards: Boolean (default False).\n        If True, process the input sequence backwards and return the\n        reversed sequence.\n    stateful: Boolean (default False). If True, the last state\n        for each sample at index i in a batch will be used as initial\n        state for the sample of index i in the following batch.\n    unroll: Boolean (default False).\n        If True, the network will be unrolled,\n        else a symbolic loop will be used.\n        Unrolling can speed-up a RNN,\n        although it tends to be more memory-intensive.\n        Unrolling is only suitable for short sequences.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "SimpleRNN",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "units",
                    "activation",
                    "use_bias",
                    "kernel_initializer",
                    "recurrent_initializer",
                    "bias_initializer",
                    "kernel_regularizer",
                    "recurrent_regularizer",
                    "bias_regularizer",
                    "kernel_constraint",
                    "recurrent_constraint",
                    "bias_constraint",
                    "dropout",
                    "recurrent_dropout"
                ],
                "Defaults": [
                    "'tanh'",
                    " True",
                    " 'glorot_uniform'",
                    " 'orthogonal'",
                    " 'zeros'",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None",
                    " None",
                    " 0.0",
                    " 0.0"
                ],
                "DocStr": "Cell class for SimpleRNN.\n\n# Arguments\n    units: Positive integer, dimensionality of the output space.\n    activation: Activation function to use\n        (see [activations](../activations.md)).\n        Default: hyperbolic tangent (`tanh`).\n        If you pass `None`, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix,\n        used for the linear transformation of the inputs\n        (see [initializers](../initializers.md)).\n    recurrent_initializer: Initializer for the `recurrent_kernel`\n        weights matrix,\n        used for the linear transformation of the recurrent state\n        (see [initializers](../initializers.md)).\n    bias_initializer: Initializer for the bias vector\n        (see [initializers](../initializers.md)).\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    recurrent_regularizer: Regularizer function applied to\n        the `recurrent_kernel` weights matrix\n        (see [regularizer](../regularizers.md)).\n    bias_regularizer: Regularizer function applied to the bias vector\n        (see [regularizer](../regularizers.md)).\n    kernel_constraint: Constraint function applied to\n        the `kernel` weights matrix\n        (see [constraints](../constraints.md)).\n    recurrent_constraint: Constraint function applied to\n        the `recurrent_kernel` weights matrix\n        (see [constraints](../constraints.md)).\n    bias_constraint: Constraint function applied to the bias vector\n        (see [constraints](../constraints.md)).\n    dropout: Float between 0 and 1.\n        Fraction of the units to drop for\n        the linear transformation of the inputs.\n    recurrent_dropout: Float between 0 and 1.\n        Fraction of the units to drop for\n        the linear transformation of the recurrent state.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "SimpleRNNCell",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "axis"
                ],
                "Defaults": [
                    "-1"
                ],
                "DocStr": "Softmax activation function.\n\n# Input shape\n    Arbitrary. Use the keyword argument `input_shape`\n    (tuple of integers, does not include the samples axis)\n    when using this layer as the first layer in a model.\n\n# Output shape\n    Same shape as the input.\n\n# Arguments\n    axis: Integer, axis along which the softmax normalization is applied.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Softmax",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Spatial 1D version of Dropout.\n\nThis version performs the same function as Dropout, however it drops\nentire 1D feature maps instead of individual elements. If adjacent frames\nwithin feature maps are strongly correlated (as is normally the case in\nearly convolution layers) then regular dropout will not regularize the\nactivations and will otherwise just result in an effective learning rate\ndecrease. In this case, SpatialDropout1D will help promote independence\nbetween feature maps and should be used instead.\n\n# Arguments\n    rate: float between 0 and 1. Fraction of the input units to drop.\n\n# Input shape\n    3D tensor with shape:\n    `(samples, timesteps, channels)`\n\n# Output shape\n    Same as input\n\n# References\n    - [Efficient Object Localization Using Convolutional Networks]\n      (https://arxiv.org/abs/1411.4280)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "SpatialDropout1D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Spatial 2D version of Dropout.\n\nThis version performs the same function as Dropout, however it drops\nentire 2D feature maps instead of individual elements. If adjacent pixels\nwithin feature maps are strongly correlated (as is normally the case in\nearly convolution layers) then regular dropout will not regularize the\nactivations and will otherwise just result in an effective learning rate\ndecrease. In this case, SpatialDropout2D will help promote independence\nbetween feature maps and should be used instead.\n\n# Arguments\n    rate: float between 0 and 1. Fraction of the input units to drop.\n    data_format: 'channels_first' or 'channels_last'.\n        In 'channels_first' mode, the channels dimension\n        (the depth) is at index 1,\n        in 'channels_last' mode is it at index 3.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n\n# Input shape\n    4D tensor with shape:\n    `(samples, channels, rows, cols)` if data_format='channels_first'\n    or 4D tensor with shape:\n    `(samples, rows, cols, channels)` if data_format='channels_last'.\n\n# Output shape\n    Same as input\n\n# References\n    - [Efficient Object Localization Using Convolutional Networks]\n      (https://arxiv.org/abs/1411.4280)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "SpatialDropout2D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Spatial 3D version of Dropout.\n\nThis version performs the same function as Dropout, however it drops\nentire 3D feature maps instead of individual elements. If adjacent voxels\nwithin feature maps are strongly correlated (as is normally the case in\nearly convolution layers) then regular dropout will not regularize the\nactivations and will otherwise just result in an effective learning rate\ndecrease. In this case, SpatialDropout3D will help promote independence\nbetween feature maps and should be used instead.\n\n# Arguments\n    rate: float between 0 and 1. Fraction of the input units to drop.\n    data_format: 'channels_first' or 'channels_last'.\n        In 'channels_first' mode, the channels dimension (the depth)\n        is at index 1, in 'channels_last' mode is it at index 4.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n\n# Input shape\n    5D tensor with shape:\n    `(samples, channels, dim1, dim2, dim3)` if data_format='channels_first'\n    or 5D tensor with shape:\n    `(samples, dim1, dim2, dim3, channels)` if data_format='channels_last'.\n\n# Output shape\n    Same as input\n\n# References\n    - [Efficient Object Localization Using Convolutional Networks]\n      (https://arxiv.org/abs/1411.4280)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "SpatialDropout3D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "cells"
                ],
                "DocStr": "Wrapper allowing a stack of RNN cells to behave as a single cell.\n\nUsed to implement efficient stacked RNNs.\n\n# Arguments\n    cells: List of RNN cell instances.\n\n# Examples\n\n```python\n    cells = [\n        keras.layers.LSTMCell(output_dim),\n        keras.layers.LSTMCell(output_dim),\n        keras.layers.LSTMCell(output_dim),\n    ]\n\n    inputs = keras.Input((timesteps, input_dim))\n    x = keras.layers.RNN(cells)(inputs)\n```",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config",
                            "custom_objects"
                        ],
                        "Defaults": [
                            "None"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "StackedRNNCells",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self"
                ],
                "DocStr": "Layer that subtracts two inputs.\n\nIt takes as input a list of tensors of size 2,\nboth of the same shape, and returns a single tensor, (inputs[0] - inputs[1]),\nalso of the same shape.\n\n# Examples\n\n```python\n    import keras\n\n    input1 = keras.layers.Input(shape=(16,))\n    x1 = keras.layers.Dense(8, activation='relu')(input1)\n    input2 = keras.layers.Input(shape=(32,))\n    x2 = keras.layers.Dense(8, activation='relu')(input2)\n    # Equivalent to subtracted = keras.layers.subtract([x1, x2])\n    subtracted = keras.layers.Subtract()([x1, x2])\n\n    out = keras.layers.Dense(4)(subtracted)\n    model = keras.models.Model(inputs=[input1, input2], outputs=out)\n```",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Subtract",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "theta"
                ],
                "Defaults": [
                    "1.0"
                ],
                "DocStr": "Thresholded Rectified Linear Unit.\n\nIt follows:\n`f(x) = x for x > theta`,\n`f(x) = 0 otherwise`.\n\n# Input shape\n    Arbitrary. Use the keyword argument `input_shape`\n    (tuple of integers, does not include the samples axis)\n    when using this layer as the first layer in a model.\n\n# Output shape\n    Same shape as the input.\n\n# Arguments\n    theta: float >= 0. Threshold location of activation.\n\n# References\n    - [Zero-Bias Autoencoders and the Benefits of Co-Adapting Features]\n      (https://arxiv.org/abs/1402.3337)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "ThresholdedReLU",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "layer"
                ],
                "DocStr": "This wrapper applies a layer to every temporal slice of an input.\n\nThe input should be at least 3D, and the dimension of index one\nwill be considered to be the temporal dimension.\n\nConsider a batch of 32 samples,\nwhere each sample is a sequence of 10 vectors of 16 dimensions.\nThe batch input shape of the layer is then `(32, 10, 16)`,\nand the `input_shape`, not including the samples dimension, is `(10, 16)`.\n\nYou can then use `TimeDistributed` to apply a `Dense` layer\nto each of the 10 timesteps, independently:\n\n```python\n    # as the first layer in a model\n    model = Sequential()\n    model.add(TimeDistributed(Dense(8), input_shape=(10, 16)))\n    # now model.output_shape == (None, 10, 8)\n```\n\nThe output will then have shape `(32, 10, 8)`.\n\nIn subsequent layers, there is no need for the `input_shape`:\n\n```python\n    model.add(TimeDistributed(Dense(32)))\n    # now model.output_shape == (None, 10, 32)\n```\n\nThe output will then have shape `(32, 10, 32)`.\n\n`TimeDistributed` can be used with arbitrary layers, not just `Dense`,\nfor instance with a `Conv2D` layer:\n\n```python\n    model = Sequential()\n    model.add(TimeDistributed(Conv2D(64, (3, 3)),\n                              input_shape=(10, 299, 299, 3)))\n```\n\n# Arguments\n    layer: a layer instance.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config",
                            "custom_objects"
                        ],
                        "Defaults": [
                            "None"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "TimeDistributed",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Upsampling layer for 1D inputs.\n\nRepeats each temporal step `size` times along the time axis.\n\n# Arguments\n    size: integer. Upsampling factor.\n\n# Input shape\n    3D tensor with shape: `(batch, steps, features)`.\n\n# Output shape\n    3D tensor with shape: `(batch, upsampled_steps, features)`.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "UpSampling1D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Upsampling layer for 2D inputs.\n\nRepeats the rows and columns of the data\nby size[0] and size[1] respectively.\n\n# Arguments\n    size: int, or tuple of 2 integers.\n        The upsampling factors for rows and columns.\n    data_format: A string,\n        one of `\"channels_last\"` or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs.\n        `\"channels_last\"` corresponds to inputs with shape\n        `(batch, height, width, channels)` while `\"channels_first\"`\n        corresponds to inputs with shape\n        `(batch, channels, height, width)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n    interpolation: A string, one of `nearest` or `bilinear`.\n        Note that CNTK does not support yet the `bilinear` upscaling\n        and that with Theano, only `size=(2, 2)` is possible.\n\n# Input shape\n    4D tensor with shape:\n    - If `data_format` is `\"channels_last\"`:\n        `(batch, rows, cols, channels)`\n    - If `data_format` is `\"channels_first\"`:\n        `(batch, channels, rows, cols)`\n\n# Output shape\n    4D tensor with shape:\n    - If `data_format` is `\"channels_last\"`:\n        `(batch, upsampled_rows, upsampled_cols, channels)`\n    - If `data_format` is `\"channels_first\"`:\n        `(batch, channels, upsampled_rows, upsampled_cols)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "UpSampling2D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Upsampling layer for 3D inputs.\n\nRepeats the 1st, 2nd and 3rd dimensions\nof the data by size[0], size[1] and size[2] respectively.\n\n# Arguments\n    size: int, or tuple of 3 integers.\n        The upsampling factors for dim1, dim2 and dim3.\n    data_format: A string,\n        one of `\"channels_last\"` or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs.\n        `\"channels_last\"` corresponds to inputs with shape\n        `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n        while `\"channels_first\"` corresponds to inputs with shape\n        `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n\n# Input shape\n    5D tensor with shape:\n    - If `data_format` is `\"channels_last\"`:\n        `(batch, dim1, dim2, dim3, channels)`\n    - If `data_format` is `\"channels_first\"`:\n        `(batch, channels, dim1, dim2, dim3)`\n\n# Output shape\n    5D tensor with shape:\n    - If `data_format` is `\"channels_last\"`:\n        `(batch, upsampled_dim1, upsampled_dim2, upsampled_dim3, channels)`\n    - If `data_format` is `\"channels_first\"`:\n        `(batch, channels, upsampled_dim1, upsampled_dim2, upsampled_dim3)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "UpSampling3D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "layer"
                ],
                "DocStr": "Abstract wrapper base class.\n\nWrappers take another layer and augment it in various ways.\nDo not use this class as a layer, it is only an abstract base class.\nTwo usable wrappers are the `TimeDistributed` and `Bidirectional` wrappers.\n\n# Arguments\n    layer: The layer to be wrapped.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config",
                            "custom_objects"
                        ],
                        "Defaults": [
                            "None"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Wrapper",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "padding"
                ],
                "Defaults": [
                    "1"
                ],
                "DocStr": "Zero-padding layer for 1D input (e.g. temporal sequence).\n\n# Arguments\n    padding: int, or tuple of int (length 2), or dictionary.\n        - If int:\n        How many zeros to add at the beginning and end of\n        the padding dimension (axis 1).\n        - If tuple of int (length 2):\n        How many zeros to add at the beginning and at the end of\n        the padding dimension (`(left_pad, right_pad)`).\n\n# Input shape\n    3D tensor with shape `(batch, axis_to_pad, features)`\n\n# Output shape\n    3D tensor with shape `(batch, padded_axis, features)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "ZeroPadding1D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Zero-padding layer for 2D input (e.g. picture).\n\nThis layer can add rows and columns of zeros\nat the top, bottom, left and right side of an image tensor.\n\n# Arguments\n    padding: int, or tuple of 2 ints, or tuple of 2 tuples of 2 ints.\n        - If int: the same symmetric padding\n            is applied to height and width.\n        - If tuple of 2 ints:\n            interpreted as two different\n            symmetric padding values for height and width:\n            `(symmetric_height_pad, symmetric_width_pad)`.\n        - If tuple of 2 tuples of 2 ints:\n            interpreted as\n            `((top_pad, bottom_pad), (left_pad, right_pad))`\n    data_format: A string,\n        one of `\"channels_last\"` or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs.\n        `\"channels_last\"` corresponds to inputs with shape\n        `(batch, height, width, channels)` while `\"channels_first\"`\n        corresponds to inputs with shape\n        `(batch, channels, height, width)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n\n# Input shape\n    4D tensor with shape:\n    - If `data_format` is `\"channels_last\"`:\n        `(batch, rows, cols, channels)`\n    - If `data_format` is `\"channels_first\"`:\n        `(batch, channels, rows, cols)`\n\n# Output shape\n    4D tensor with shape:\n    - If `data_format` is `\"channels_last\"`:\n        `(batch, padded_rows, padded_cols, channels)`\n    - If `data_format` is `\"channels_first\"`:\n        `(batch, channels, padded_rows, padded_cols)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "ZeroPadding2D",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Zero-padding layer for 3D data (spatial or spatio-temporal).\n\n# Arguments\n    padding: int, or tuple of 3 ints, or tuple of 3 tuples of 2 ints.\n        - If int: the same symmetric padding\n            is applied to height and width.\n        - If tuple of 3 ints:\n            interpreted as two different\n            symmetric padding values for height and width:\n            `(symmetric_dim1_pad, symmetric_dim2_pad, symmetric_dim3_pad)`.\n        - If tuple of 3 tuples of 2 ints:\n            interpreted as\n            `((left_dim1_pad, right_dim1_pad),\n              (left_dim2_pad, right_dim2_pad),\n              (left_dim3_pad, right_dim3_pad))`\n    data_format: A string,\n        one of `\"channels_last\"` or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs.\n        `\"channels_last\"` corresponds to inputs with shape\n        `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n        while `\"channels_first\"` corresponds to inputs with shape\n        `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n\n# Input shape\n    5D tensor with shape:\n    - If `data_format` is `\"channels_last\"`:\n        `(batch, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad,\n          depth)`\n    - If `data_format` is `\"channels_first\"`:\n        `(batch, depth,\n          first_axis_to_pad, second_axis_to_pad, third_axis_to_pad)`\n\n# Output shape\n    5D tensor with shape:\n    - If `data_format` is `\"channels_last\"`:\n        `(batch, first_padded_axis, second_padded_axis, third_axis_to_pad,\n          depth)`\n    - If `data_format` is `\"channels_first\"`:\n        `(batch, depth,\n          first_padded_axis, second_padded_axis, third_axis_to_pad)`",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "ZeroPadding3D",
                "Type": "Class"
            }
        ],
        "Functions": [
            {
                "Args": [],
                "DocStr": null,
                "Name": "AtrousConv1D"
            },
            {
                "Args": [],
                "DocStr": null,
                "Name": "AtrousConv2D"
            },
            {
                "Args": [],
                "DocStr": null,
                "Name": "AtrousConvolution1D"
            },
            {
                "Args": [],
                "DocStr": null,
                "Name": "AtrousConvolution2D"
            },
            {
                "Args": [
                    "shape",
                    "batch_shape",
                    "name",
                    "dtype",
                    "sparse",
                    "tensor"
                ],
                "Defaults": [
                    "None",
                    " None",
                    " None",
                    " None",
                    " False",
                    " None"
                ],
                "DocStr": "`Input()` is used to instantiate a Keras tensor.\n\nA Keras tensor is a tensor object from the underlying backend\n(Theano, TensorFlow or CNTK), which we augment with certain\nattributes that allow us to build a Keras model\njust by knowing the inputs and outputs of the model.\n\nFor instance, if a, b and c are Keras tensors,\nit becomes possible to do:\n`model = Model(input=[a, b], output=c)`\n\nThe added Keras attributes are:\n    `_keras_shape`: Integer shape tuple propagated\n        via Keras-side shape inference.\n    `_keras_history`: Last layer applied to the tensor.\n        the entire layer graph is retrievable from that layer,\n        recursively.\n\n# Arguments\n    shape: A shape tuple (integer), not including the batch size.\n        For instance, `shape=(32,)` indicates that the expected input\n        will be batches of 32-dimensional vectors.\n    batch_shape: A shape tuple (integer), including the batch size.\n        For instance, `batch_shape=(10, 32)` indicates that\n        the expected input will be batches of 10 32-dimensional vectors.\n        `batch_shape=(None, 32)` indicates batches of an arbitrary number\n        of 32-dimensional vectors.\n    name: An optional name string for the layer.\n        Should be unique in a model (do not reuse the same name twice).\n        It will be autogenerated if it isn't provided.\n    dtype: The data type expected by the input, as a string\n        (`float32`, `float64`, `int32`...)\n    sparse: A boolean specifying whether the placeholder\n        to be created is sparse.\n    tensor: Optional existing tensor to wrap into the `Input` layer.\n        If set, the layer will not create a placeholder tensor.\n\n# Returns\n    A tensor.\n\n# Example\n\n```python\n# this is a logistic regression in Keras\nx = Input(shape=(32,))\ny = Dense(16, activation='softmax')(x)\nmodel = Model(x, y)\n```",
                "Name": "Input"
            },
            {
                "Args": [
                    "inputs"
                ],
                "DocStr": "Functional interface to the `Add` layer.\n\n# Arguments\n    inputs: A list of input tensors (at least 2).\n    **kwargs: Standard layer keyword arguments.\n\n# Returns\n    A tensor, the sum of the inputs.\n\n# Examples\n\n```python\n    import keras\n\n    input1 = keras.layers.Input(shape=(16,))\n    x1 = keras.layers.Dense(8, activation='relu')(input1)\n    input2 = keras.layers.Input(shape=(32,))\n    x2 = keras.layers.Dense(8, activation='relu')(input2)\n    added = keras.layers.add([x1, x2])\n\n    out = keras.layers.Dense(4)(added)\n    model = keras.models.Model(inputs=[input1, input2], outputs=out)\n```",
                "Name": "add"
            },
            {
                "Args": [
                    "inputs"
                ],
                "DocStr": "Functional interface to the `Average` layer.\n\n# Arguments\n    inputs: A list of input tensors (at least 2).\n    **kwargs: Standard layer keyword arguments.\n\n# Returns\n    A tensor, the average of the inputs.",
                "Name": "average"
            },
            {
                "Args": [
                    "inputs",
                    "axis"
                ],
                "Defaults": [
                    "-1"
                ],
                "DocStr": "Functional interface to the `Concatenate` layer.\n\n# Arguments\n    inputs: A list of input tensors (at least 2).\n    axis: Concatenation axis.\n    **kwargs: Standard layer keyword arguments.\n\n# Returns\n    A tensor, the concatenation of the inputs alongside axis `axis`.",
                "Name": "concatenate"
            },
            {
                "Args": [
                    "config",
                    "custom_objects"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": "Instantiate a layer from a config dictionary.\n\n# Arguments\n    config: dict of the form {'class_name': str, 'config': dict}\n    custom_objects: dict mapping class names (or function names)\n        of custom (non-Keras) objects to class/functions\n\n# Returns\n    Layer instance (may be Model, Sequential, Layer...)",
                "Name": "deserialize"
            },
            {
                "Args": [
                    "identifier",
                    "module_objects",
                    "custom_objects",
                    "printable_module_name"
                ],
                "Defaults": [
                    "None",
                    " None",
                    " 'object'"
                ],
                "DocStr": null,
                "Name": "deserialize_keras_object"
            },
            {
                "Args": [
                    "inputs",
                    "axes",
                    "normalize"
                ],
                "Defaults": [
                    "False"
                ],
                "DocStr": "Functional interface to the `Dot` layer.\n\n# Arguments\n    inputs: A list of input tensors (at least 2).\n    axes: Integer or tuple of integers,\n        axis or axes along which to take the dot product.\n    normalize: Whether to L2-normalize samples along the\n        dot product axis before taking the dot product.\n        If set to True, then the output of the dot product\n        is the cosine proximity between the two samples.\n    **kwargs: Standard layer keyword arguments.\n\n# Returns\n    A tensor, the dot product of the samples from the inputs.",
                "Name": "dot"
            },
            {
                "Args": [
                    "func"
                ],
                "DocStr": "Serializes a user defined function.\n\n# Arguments\n    func: the function to serialize.\n\n# Returns\n    A tuple `(code, defaults, closure)`.",
                "Name": "func_dump"
            },
            {
                "Args": [
                    "code",
                    "defaults",
                    "closure",
                    "globs"
                ],
                "Defaults": [
                    "None",
                    " None",
                    " None"
                ],
                "DocStr": "Deserializes a user defined function.\n\n# Arguments\n    code: bytecode of the function.\n    defaults: defaults of the function.\n    closure: closure of the function.\n    globs: dictionary of global objects.\n\n# Returns\n    A function object.",
                "Name": "func_load"
            },
            {
                "Args": [
                    "fn",
                    "name",
                    "accept_all"
                ],
                "Defaults": [
                    "False"
                ],
                "DocStr": "Checks if a callable accepts a given keyword argument.\n\nFor Python 2, checks if there is an argument with the given name.\n\nFor Python 3, checks if there is an argument with the given name, and\nalso whether this argument can be called with a keyword (i.e. if it is\nnot a positional-only argument).\n\n# Arguments\n    fn: Callable to inspect.\n    name: Check if `fn` can be called with `name` as a keyword argument.\n    accept_all: What to return if there is no parameter called `name`\n                but the function accepts a `**kwargs` argument.\n\n# Returns\n    bool, whether `fn` accepts a `name` keyword argument.",
                "Name": "has_arg"
            },
            {
                "Args": [
                    "inputs"
                ],
                "DocStr": "Functional interface to the `Maximum` layer.\n\n# Arguments\n    inputs: A list of input tensors (at least 2).\n    **kwargs: Standard layer keyword arguments.\n\n# Returns\n    A tensor, the element-wise maximum of the inputs.",
                "Name": "maximum"
            },
            {
                "Args": [
                    "inputs"
                ],
                "DocStr": "Functional interface to the `Minimum` layer.\n\n# Arguments\n    inputs: A list of input tensors (at least 2).\n    **kwargs: Standard layer keyword arguments.\n\n# Returns\n    A tensor, the element-wise minimum of the inputs.",
                "Name": "minimum"
            },
            {
                "Args": [
                    "inputs"
                ],
                "DocStr": "Functional interface to the `Multiply` layer.\n\n# Arguments\n    inputs: A list of input tensors (at least 2).\n    **kwargs: Standard layer keyword arguments.\n\n# Returns\n    A tensor, the element-wise product of the inputs.",
                "Name": "multiply"
            },
            {
                "Args": [
                    "typename",
                    "field_names"
                ],
                "DocStr": "Returns a new subclass of tuple with named fields.\n\n>>> Point = namedtuple('Point', ['x', 'y'])\n>>> Point.__doc__                   # docstring for the new class\n'Point(x, y)'\n>>> p = Point(11, y=22)             # instantiate with positional args or keywords\n>>> p[0] + p[1]                     # indexable like a plain tuple\n33\n>>> x, y = p                        # unpack like a regular tuple\n>>> x, y\n(11, 22)\n>>> p.x + p.y                       # fields also accessible by name\n33\n>>> d = p._asdict()                 # convert to a dictionary\n>>> d['x']\n11\n>>> Point(**d)                      # convert from a dictionary\nPoint(x=11, y=22)\n>>> p._replace(x=100)               # _replace() is like str.replace() but targets named fields\nPoint(x=100, y=22)",
                "Name": "namedtuple"
            },
            {
                "Args": [
                    "object_list"
                ],
                "DocStr": null,
                "Name": "object_list_uid"
            },
            {
                "Args": [
                    "layer"
                ],
                "DocStr": "Serialize a layer.\n\n# Arguments\n    layer: a Layer object.\n\n# Returns\n    dictionary with config.",
                "Name": "serialize"
            },
            {
                "Args": [
                    "inputs"
                ],
                "DocStr": "Functional interface to the `Subtract` layer.\n\n# Arguments\n    inputs: A list of input tensors (exactly 2).\n    **kwargs: Standard layer keyword arguments.\n\n# Returns\n    A tensor, the difference of the inputs.\n\n# Examples\n\n```python\n    import keras\n\n    input1 = keras.layers.Input(shape=(16,))\n    x1 = keras.layers.Dense(8, activation='relu')(input1)\n    input2 = keras.layers.Input(shape=(32,))\n    x2 = keras.layers.Dense(8, activation='relu')(input2)\n    subtracted = keras.layers.subtract([x1, x2])\n\n    out = keras.layers.Dense(4)(subtracted)\n    model = keras.models.Model(inputs=[input1, input2], outputs=out)\n```",
                "Name": "subtract"
            },
            {
                "Args": [
                    "x",
                    "allow_tuple"
                ],
                "Defaults": [
                    "False"
                ],
                "DocStr": "Normalizes a list/tensor into a list.\n\nIf a tensor is passed, we return\na list of size 1 containing the tensor.\n\n# Arguments\n    x: target object to be normalized.\n    allow_tuple: If False and x is a tuple,\n        it will be converted into a list\n        with a single element (the tuple).\n        Else converts the tuple to a list.\n\n# Returns\n    A list.",
                "Name": "to_list"
            },
            {
                "Args": [
                    "shape",
                    "target_format",
                    "spatial_axes"
                ],
                "DocStr": "Converts a tuple or a list to the correct `data_format`.\n\nIt does so by switching the positions of its elements.\n\n# Arguments\n    shape: Tuple or list, often representing shape,\n        corresponding to `'channels_last'`.\n    target_format: A string, either `'channels_first'` or `'channels_last'`.\n    spatial_axes: A tuple of integers.\n        Correspond to the indexes of the spatial axes.\n        For example, if you pass a shape\n        representing (batch_size, timesteps, rows, cols, channels),\n        then `spatial_axes=(2, 3)`.\n\n# Returns\n    A tuple or list, with the elements permuted according\n    to `target_format`.\n\n# Example\n```python\n    >>> from keras.utils.generic_utils import transpose_shape\n    >>> transpose_shape((16, 128, 128, 32),'channels_first', spatial_axes=(1, 2))\n    (16, 32, 128, 128)\n    >>> transpose_shape((16, 128, 128, 32), 'channels_last', spatial_axes=(1, 2))\n    (16, 128, 128, 32)\n    >>> transpose_shape((128, 128, 32), 'channels_first', spatial_axes=(0, 1))\n    (32, 128, 128)\n```\n\n# Raises\n    ValueError: if `value` or the global `data_format` invalid.",
                "Name": "transpose_shape"
            }
        ],
        "Name": "layers",
        "Type": "Module"
    },
    {
        "Classes": [],
        "Functions": [],
        "Name": "legacy",
        "Type": "Module"
    },
    {
        "Classes": [],
        "Functions": [
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "KLD"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "MAE"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "MAPE"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "MSE"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "MSLE"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "binary_crossentropy"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "categorical_crossentropy"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "categorical_hinge"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "cosine"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "cosine_proximity"
            },
            {
                "Args": [
                    "name",
                    "custom_objects"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": null,
                "Name": "deserialize"
            },
            {
                "Args": [
                    "identifier",
                    "module_objects",
                    "custom_objects",
                    "printable_module_name"
                ],
                "Defaults": [
                    "None",
                    " None",
                    " 'object'"
                ],
                "DocStr": null,
                "Name": "deserialize_keras_object"
            },
            {
                "Args": [
                    "identifier"
                ],
                "DocStr": "Get the `identifier` loss function.\n\n# Arguments\n    identifier: None or str, name of the function.\n\n# Returns\n    The loss function or None if `identifier` is None.\n\n# Raises\n    ValueError if unknown identifier.",
                "Name": "get"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "hinge"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "kld"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "kullback_leibler_divergence"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": "Logarithm of the hyperbolic cosine of the prediction error.\n\n`log(cosh(x))` is approximately equal to `(x ** 2) / 2` for small `x` and\nto `abs(x) - log(2)` for large `x`. This means that 'logcosh' works mostly\nlike the mean squared error, but will not be so strongly affected by the\noccasional wildly incorrect prediction.\n\n# Arguments\n    y_true: tensor of true targets.\n    y_pred: tensor of predicted targets.\n\n# Returns\n    Tensor with one scalar loss entry per sample.",
                "Name": "logcosh"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "mae"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "mape"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "mean_absolute_error"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "mean_absolute_percentage_error"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "mean_squared_error"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "mean_squared_logarithmic_error"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "mse"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "msle"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "poisson"
            },
            {
                "Args": [
                    "loss"
                ],
                "DocStr": null,
                "Name": "serialize"
            },
            {
                "Args": [
                    "instance"
                ],
                "DocStr": null,
                "Name": "serialize_keras_object"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "sparse_categorical_crossentropy"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "squared_hinge"
            }
        ],
        "Name": "losses",
        "Type": "Module"
    },
    {
        "Classes": [],
        "Functions": [
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "MAE"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "MAPE"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "MSE"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "MSLE"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "binary_accuracy"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "binary_crossentropy"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "categorical_accuracy"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "categorical_crossentropy"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "cosine"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "cosine_proximity"
            },
            {
                "Args": [
                    "config",
                    "custom_objects"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": null,
                "Name": "deserialize"
            },
            {
                "Args": [
                    "identifier",
                    "module_objects",
                    "custom_objects",
                    "printable_module_name"
                ],
                "Defaults": [
                    "None",
                    " None",
                    " 'object'"
                ],
                "DocStr": null,
                "Name": "deserialize_keras_object"
            },
            {
                "Args": [
                    "identifier"
                ],
                "DocStr": null,
                "Name": "get"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "hinge"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "kullback_leibler_divergence"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": "Logarithm of the hyperbolic cosine of the prediction error.\n\n`log(cosh(x))` is approximately equal to `(x ** 2) / 2` for small `x` and\nto `abs(x) - log(2)` for large `x`. This means that 'logcosh' works mostly\nlike the mean squared error, but will not be so strongly affected by the\noccasional wildly incorrect prediction.\n\n# Arguments\n    y_true: tensor of true targets.\n    y_pred: tensor of predicted targets.\n\n# Returns\n    Tensor with one scalar loss entry per sample.",
                "Name": "logcosh"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "mae"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "mape"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "mean_absolute_error"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "mean_absolute_percentage_error"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "mean_squared_error"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "mean_squared_logarithmic_error"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "mse"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "msle"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "poisson"
            },
            {
                "Args": [
                    "metric"
                ],
                "DocStr": null,
                "Name": "serialize"
            },
            {
                "Args": [
                    "instance"
                ],
                "DocStr": null,
                "Name": "serialize_keras_object"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "sparse_categorical_accuracy"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "sparse_categorical_crossentropy"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred",
                    "k"
                ],
                "Defaults": [
                    "5"
                ],
                "DocStr": null,
                "Name": "sparse_top_k_categorical_accuracy"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred"
                ],
                "DocStr": null,
                "Name": "squared_hinge"
            },
            {
                "Args": [
                    "y_true",
                    "y_pred",
                    "k"
                ],
                "Defaults": [
                    "5"
                ],
                "DocStr": null,
                "Name": "top_k_categorical_accuracy"
            }
        ],
        "Name": "metrics",
        "Type": "Module"
    },
    {
        "Classes": [
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Layer to be used as an entry point into a model.\n\nIt can either wrap an existing tensor (pass an `input_tensor` argument)\nor create its a placeholder tensor (pass arguments `input_shape`\nor `batch_input_shape` as well as `dtype`).\n\n# Arguments\n    input_shape: Shape tuple, not including the batch axis.\n    batch_size: Optional input batch size (integer or None).\n    batch_input_shape: Shape tuple, including the batch axis.\n    dtype: Datatype of the input.\n    input_tensor: Optional tensor to use as layer input\n        instead of creating a placeholder.\n    sparse: Boolean, whether the placeholder created\n        is meant to be sparse.\n    name: Name of the layer (string).",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": "Creates a layer from its config.\n\nThis method is the reverse of `get_config`,\ncapable of instantiating the same layer from the config\ndictionary. It does not handle layer connectivity\n(handled by Network), nor weights (handled by `set_weights`).\n\n# Arguments\n    config: A Python dictionary, typically the\n        output of get_config.\n\n# Returns\n    A layer instance.",
                        "Name": "from_config"
                    }
                ],
                "Name": "InputLayer",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "The `Model` class adds training & evaluation routines to a `Network`.\n    ",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config",
                            "custom_objects"
                        ],
                        "Defaults": [
                            "None"
                        ],
                        "DocStr": "Instantiates a Model from its config (output of `get_config()`).\n\n# Arguments\n    config: Model config dictionary.\n    custom_objects: Optional dictionary mapping names\n        (strings) to custom classes or functions to be\n        considered during deserialization.\n\n# Returns\n    A model instance.\n\n# Raises\n    ValueError: In case of improperly formatted config dict.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Model",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "layers",
                    "name"
                ],
                "Defaults": [
                    "None",
                    " None"
                ],
                "DocStr": "Linear stack of layers.\n\n# Arguments\n    layers: list of layers to add to the model.\n\n# Example\n\n```python\n# Optionally, the first layer can receive an `input_shape` argument:\nmodel = Sequential()\nmodel.add(Dense(32, input_shape=(500,)))\n\n# Afterwards, we do automatic shape inference:\nmodel.add(Dense(32))\n\n# This is identical to the following:\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=500))\n\n# And to the following:\nmodel = Sequential()\nmodel.add(Dense(32, batch_input_shape=(None, 500)))\n\n# Note that you can also omit the `input_shape` argument:\n# In that case the model gets built the first time you call `fit` (or other\n# training and evaluation methods).\nmodel = Sequential()\nmodel.add(Dense(32))\nmodel.add(Dense(32))\nmodel.compile(optimizer=optimizer, loss=loss)\n\n# This builds the model for the first time:\nmodel.fit(x, y, batch_size=32, epochs=10)\n\n# Note that when using this delayed-build pattern\n# (no input shape specified),\n# the model doesn't have any weights until the first call\n# to a training/evaluation method (since it isn't yet built):\nmodel = Sequential()\nmodel.add(Dense(32))\nmodel.add(Dense(32))\nmodel.weights  # returns []\n\n# Whereas if you specify the input shape, the model gets built continuously\n# as you are adding layers:\nmodel = Sequential()\nmodel.add(Dense(32, input_shape=(500,)))\nmodel.add(Dense(32))\nmodel.weights  # returns list of length 4\n\n# When using the delayed-build pattern (no input shape specified), you can\n# choose to manually build your model by calling\n# `build(batch_input_shape)`:\nmodel = Sequential()\nmodel.add(Dense(32))\nmodel.add(Dense(32))\nmodel.build((None, 500))\nmodel.weights  # returns list of length 4\n```",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config",
                            "custom_objects"
                        ],
                        "Defaults": [
                            "None"
                        ],
                        "DocStr": "Instantiates a Model from its config (output of `get_config()`).\n\n# Arguments\n    config: Model config dictionary.\n    custom_objects: Optional dictionary mapping names\n        (strings) to custom classes or functions to be\n        considered during deserialization.\n\n# Returns\n    A model instance.\n\n# Raises\n    ValueError: In case of improperly formatted config dict.",
                        "Name": "from_config"
                    }
                ],
                "Name": "Sequential",
                "Type": "Class"
            }
        ],
        "Functions": [
            {
                "Args": [
                    "shape",
                    "batch_shape",
                    "name",
                    "dtype",
                    "sparse",
                    "tensor"
                ],
                "Defaults": [
                    "None",
                    " None",
                    " None",
                    " None",
                    " False",
                    " None"
                ],
                "DocStr": "`Input()` is used to instantiate a Keras tensor.\n\nA Keras tensor is a tensor object from the underlying backend\n(Theano, TensorFlow or CNTK), which we augment with certain\nattributes that allow us to build a Keras model\njust by knowing the inputs and outputs of the model.\n\nFor instance, if a, b and c are Keras tensors,\nit becomes possible to do:\n`model = Model(input=[a, b], output=c)`\n\nThe added Keras attributes are:\n    `_keras_shape`: Integer shape tuple propagated\n        via Keras-side shape inference.\n    `_keras_history`: Last layer applied to the tensor.\n        the entire layer graph is retrievable from that layer,\n        recursively.\n\n# Arguments\n    shape: A shape tuple (integer), not including the batch size.\n        For instance, `shape=(32,)` indicates that the expected input\n        will be batches of 32-dimensional vectors.\n    batch_shape: A shape tuple (integer), including the batch size.\n        For instance, `batch_shape=(10, 32)` indicates that\n        the expected input will be batches of 10 32-dimensional vectors.\n        `batch_shape=(None, 32)` indicates batches of an arbitrary number\n        of 32-dimensional vectors.\n    name: An optional name string for the layer.\n        Should be unique in a model (do not reuse the same name twice).\n        It will be autogenerated if it isn't provided.\n    dtype: The data type expected by the input, as a string\n        (`float32`, `float64`, `int32`...)\n    sparse: A boolean specifying whether the placeholder\n        to be created is sparse.\n    tensor: Optional existing tensor to wrap into the `Input` layer.\n        If set, the layer will not create a placeholder tensor.\n\n# Returns\n    A tensor.\n\n# Example\n\n```python\n# this is a logistic regression in Keras\nx = Input(shape=(32,))\ny = Dense(16, activation='softmax')(x)\nmodel = Model(x, y)\n```",
                "Name": "Input"
            },
            {
                "Args": [
                    "model",
                    "input_tensors"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": "Clone a functional `Model` instance.\n\nModel cloning is similar to calling a model on new inputs,\nexcept that it creates new layers (and thus new weights) instead\nof sharing the weights of the existing layers.\n\n# Arguments\n    model: Instance of `Model`.\n    input_tensors: optional list of input tensors\n        to build the model upon. If not provided,\n        placeholders will be created.\n\n# Returns\n    An instance of `Model` reproducing the behavior\n    of the original model, on top of new inputs tensors,\n    using newly instantiated weights.\n\n# Raises\n    ValueError: in case of invalid `model` argument value.",
                "Name": "_clone_functional_model"
            },
            {
                "Args": [
                    "model",
                    "input_tensors"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": "Clone a `Sequential` model instance.\n\nModel cloning is similar to calling a model on new inputs,\nexcept that it creates new layers (and thus new weights) instead\nof sharing the weights of the existing layers.\n\n# Arguments\n    model: Instance of `Sequential`.\n    input_tensors: optional list of input tensors\n        to build the model upon. If not provided,\n        placeholders will be created.\n\n# Returns\n    An instance of `Sequential` reproducing the behavior\n    of the original model, on top of new inputs tensors,\n    using newly instantiated weights.\n\n# Raises\n    ValueError: in case of invalid `model` argument value.",
                "Name": "_clone_sequential_model"
            },
            {
                "Args": [
                    "model",
                    "input_tensors"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": "Clone any `Model` instance.\n\nModel cloning is similar to calling a model on new inputs,\nexcept that it creates new layers (and thus new weights) instead\nof sharing the weights of the existing layers.\n\n# Arguments\n    model: Instance of `Model`\n        (could be a functional model or a Sequential model).\n    input_tensors: optional list of input tensors\n        to build the model upon. If not provided,\n        placeholders will be created.\n\n# Returns\n    An instance of `Model` reproducing the behavior\n    of the original model, on top of new inputs tensors,\n    using newly instantiated weights.\n\n# Raises\n    ValueError: in case of invalid `model` argument value.",
                "Name": "clone_model"
            },
            {
                "Args": [
                    "fn",
                    "name",
                    "accept_all"
                ],
                "Defaults": [
                    "False"
                ],
                "DocStr": "Checks if a callable accepts a given keyword argument.\n\nFor Python 2, checks if there is an argument with the given name.\n\nFor Python 3, checks if there is an argument with the given name, and\nalso whether this argument can be called with a keyword (i.e. if it is\nnot a positional-only argument).\n\n# Arguments\n    fn: Callable to inspect.\n    name: Check if `fn` can be called with `name` as a keyword argument.\n    accept_all: What to return if there is no parameter called `name`\n                but the function accepts a `**kwargs` argument.\n\n# Returns\n    bool, whether `fn` accepts a `name` keyword argument.",
                "Name": "has_arg"
            },
            {
                "Args": [
                    "filepath",
                    "custom_objects",
                    "compile"
                ],
                "Defaults": [
                    "None",
                    " True"
                ],
                "DocStr": "Loads a model saved via `save_model`.\n\n# Arguments\n    filepath: one of the following:\n        - string, path to the saved model, or\n        - h5py.File or h5py.Group object from which to load the model\n    custom_objects: Optional dictionary mapping names\n        (strings) to custom classes or functions to be\n        considered during deserialization.\n    compile: Boolean, whether to compile the model\n        after loading.\n\n# Returns\n    A Keras model instance. If an optimizer was found\n    as part of the saved model, the model is already\n    compiled. Otherwise, the model is uncompiled and\n    a warning will be displayed. When `compile` is set\n    to False, the compilation is omitted without any\n    warning.\n\n# Raises\n    ImportError: if h5py is not available.\n    ValueError: In case of an invalid savefile.",
                "Name": "load_model"
            },
            {
                "Args": [
                    "config",
                    "custom_objects"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": "Instantiates a Keras model from its config.\n\n# Arguments\n    config: Configuration dictionary.\n    custom_objects: Optional dictionary mapping names\n        (strings) to custom classes or functions to be\n        considered during deserialization.\n\n# Returns\n    A Keras model instance (uncompiled).\n\n# Raises\n    TypeError: if `config` is not a dictionary.",
                "Name": "model_from_config"
            },
            {
                "Args": [
                    "json_string",
                    "custom_objects"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": "Parses a JSON model configuration file and returns a model instance.\n\n# Arguments\n    json_string: JSON string encoding a model configuration.\n    custom_objects: Optional dictionary mapping names\n        (strings) to custom classes or functions to be\n        considered during deserialization.\n\n# Returns\n    A Keras model instance (uncompiled).",
                "Name": "model_from_json"
            },
            {
                "Args": [
                    "yaml_string",
                    "custom_objects"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": "Parses a yaml model configuration file and returns a model instance.\n\n# Arguments\n    yaml_string: YAML string encoding a model configuration.\n    custom_objects: Optional dictionary mapping names\n        (strings) to custom classes or functions to be\n        considered during deserialization.\n\n# Returns\n    A Keras model instance (uncompiled).",
                "Name": "model_from_yaml"
            },
            {
                "Args": [
                    "model",
                    "filepath",
                    "overwrite",
                    "include_optimizer"
                ],
                "Defaults": [
                    "True",
                    " True"
                ],
                "DocStr": "Save a model to a HDF5 file.\n\nNote: Please also see\n[How can I install HDF5 or h5py to save my models in Keras?](\n    /getting-started/faq/\n    #how-can-i-install-HDF5-or-h5py-to-save-my-models-in-Keras)\nin the FAQ for instructions on how to install `h5py`.\n\nThe saved model contains:\n    - the model's configuration (topology)\n    - the model's weights\n    - the model's optimizer's state (if any)\n\nThus the saved model can be reinstantiated in\nthe exact same state, without any of the code\nused for model definition or training.\n\n# Arguments\n    model: Keras model instance to be saved.\n    filepath: one of the following:\n        - string, path where to save the model, or\n        - h5py.File or h5py.Group object where to save the model\n    overwrite: Whether we should overwrite any existing\n        model at the target location, or instead\n        ask the user with a manual prompt.\n    include_optimizer: If True, save optimizer's state together.\n\n# Raises\n    ImportError: if h5py is not available.",
                "Name": "save_model"
            },
            {
                "Args": [
                    "x",
                    "allow_tuple"
                ],
                "Defaults": [
                    "False"
                ],
                "DocStr": "Normalizes a list/tensor into a list.\n\nIf a tensor is passed, we return\na list of size 1 containing the tensor.\n\n# Arguments\n    x: target object to be normalized.\n    allow_tuple: If False and x is a tuple,\n        it will be converted into a list\n        with a single element (the tuple).\n        Else converts the tuple to a list.\n\n# Returns\n    A list.",
                "Name": "to_list"
            }
        ],
        "Name": "models",
        "Type": "Module"
    },
    {
        "Classes": [
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "lr",
                    "rho",
                    "epsilon",
                    "decay"
                ],
                "Defaults": [
                    "1.0",
                    " 0.95",
                    " None",
                    " 0.0"
                ],
                "DocStr": "Adadelta optimizer.\n\nAdadelta is a more robust extension of Adagrad\nthat adapts learning rates based on a moving window of gradient updates,\ninstead of accumulating all past gradients. This way, Adadelta continues\nlearning even when many updates have been done. Compared to Adagrad, in the\noriginal version of Adadelta you don't have to set an initial learning\nrate. In this version, initial learning rate and decay factor can\nbe set, as in most other Keras optimizers.\n\nIt is recommended to leave the parameters of this optimizer\nat their default values.\n\n# Arguments\n    lr: float >= 0. Initial learning rate, defaults to 1.\n        It is recommended to leave it at the default value.\n    rho: float >= 0. Adadelta decay factor, corresponding to fraction of\n        gradient to keep at each time step.\n    epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n    decay: float >= 0. Initial learning rate decay.\n\n# References\n    - [Adadelta - an adaptive learning rate method]\n      (https://arxiv.org/abs/1212.5701)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "Adadelta",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "lr",
                    "epsilon",
                    "decay"
                ],
                "Defaults": [
                    "0.01",
                    " None",
                    " 0.0"
                ],
                "DocStr": "Adagrad optimizer.\n\nAdagrad is an optimizer with parameter-specific learning rates,\nwhich are adapted relative to how frequently a parameter gets\nupdated during training. The more updates a parameter receives,\nthe smaller the updates.\n\nIt is recommended to leave the parameters of this optimizer\nat their default values.\n\n# Arguments\n    lr: float >= 0. Initial learning rate.\n    epsilon: float >= 0. If `None`, defaults to `K.epsilon()`.\n    decay: float >= 0. Learning rate decay over each update.\n\n# References\n    - [Adaptive Subgradient Methods for Online Learning and Stochastic\n       Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "Adagrad",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "lr",
                    "beta_1",
                    "beta_2",
                    "epsilon",
                    "decay",
                    "amsgrad"
                ],
                "Defaults": [
                    "0.001",
                    " 0.9",
                    " 0.999",
                    " None",
                    " 0.0",
                    " False"
                ],
                "DocStr": "Adam optimizer.\n\nDefault parameters follow those provided in the original paper.\n\n# Arguments\n    lr: float >= 0. Learning rate.\n    beta_1: float, 0 < beta < 1. Generally close to 1.\n    beta_2: float, 0 < beta < 1. Generally close to 1.\n    epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n    decay: float >= 0. Learning rate decay over each update.\n    amsgrad: boolean. Whether to apply the AMSGrad variant of this\n        algorithm from the paper \"On the Convergence of Adam and\n        Beyond\".\n\n# References\n    - [Adam - A Method for Stochastic Optimization]\n      (https://arxiv.org/abs/1412.6980v8)\n    - [On the Convergence of Adam and Beyond]\n      (https://openreview.net/forum?id=ryQu7f-RZ)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "Adam",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "lr",
                    "beta_1",
                    "beta_2",
                    "epsilon",
                    "decay"
                ],
                "Defaults": [
                    "0.002",
                    " 0.9",
                    " 0.999",
                    " None",
                    " 0.0"
                ],
                "DocStr": "Adamax optimizer from Adam paper's Section 7.\n\nIt is a variant of Adam based on the infinity norm.\nDefault parameters follow those provided in the paper.\n\n# Arguments\n    lr: float >= 0. Learning rate.\n    beta_1/beta_2: floats, 0 < beta < 1. Generally close to 1.\n    epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n    decay: float >= 0. Learning rate decay over each update.\n\n# References\n    - [Adam - A Method for Stochastic Optimization]\n      (https://arxiv.org/abs/1412.6980v8)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "Adamax",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "lr",
                    "beta_1",
                    "beta_2",
                    "epsilon",
                    "schedule_decay"
                ],
                "Defaults": [
                    "0.002",
                    " 0.9",
                    " 0.999",
                    " None",
                    " 0.004"
                ],
                "DocStr": "Nesterov Adam optimizer.\n\nMuch like Adam is essentially RMSprop with momentum,\nNadam is Adam RMSprop with Nesterov momentum.\n\nDefault parameters follow those provided in the paper.\nIt is recommended to leave the parameters of this optimizer\nat their default values.\n\n# Arguments\n    lr: float >= 0. Learning rate.\n    beta_1/beta_2: floats, 0 < beta < 1. Generally close to 1.\n    epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n\n# References\n    - [Nadam report](http://cs229.stanford.edu/proj2015/054_report.pdf)\n    - [On the importance of initialization and momentum in deep learning]\n      (http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "Nadam",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self"
                ],
                "DocStr": "Abstract optimizer base class.\n\nNote: this is the parent class of all optimizers, not an actual optimizer\nthat can be used for training models.\n\nAll Keras optimizers support the following keyword arguments:\n\n    clipnorm: float >= 0. Gradients will be clipped\n        when their L2 norm exceeds this value.\n    clipvalue: float >= 0. Gradients will be clipped\n        when their absolute value exceeds this value.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "Optimizer",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "lr",
                    "rho",
                    "epsilon",
                    "decay"
                ],
                "Defaults": [
                    "0.001",
                    " 0.9",
                    " None",
                    " 0.0"
                ],
                "DocStr": "RMSProp optimizer.\n\nIt is recommended to leave the parameters of this optimizer\nat their default values\n(except the learning rate, which can be freely tuned).\n\nThis optimizer is usually a good choice for recurrent\nneural networks.\n\n# Arguments\n    lr: float >= 0. Learning rate.\n    rho: float >= 0.\n    epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n    decay: float >= 0. Learning rate decay over each update.\n\n# References\n    - [rmsprop: Divide the gradient by a running average of its recent magnitude]\n      (http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "RMSprop",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "lr",
                    "momentum",
                    "decay",
                    "nesterov"
                ],
                "Defaults": [
                    "0.01",
                    " 0.0",
                    " 0.0",
                    " False"
                ],
                "DocStr": "Stochastic gradient descent optimizer.\n\nIncludes support for momentum,\nlearning rate decay, and Nesterov momentum.\n\n# Arguments\n    lr: float >= 0. Learning rate.\n    momentum: float >= 0. Parameter that accelerates SGD\n        in the relevant direction and dampens oscillations.\n    decay: float >= 0. Learning rate decay over each update.\n    nesterov: boolean. Whether to apply Nesterov momentum.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "SGD",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "optimizer"
                ],
                "DocStr": "Wrapper class for native TensorFlow optimizers.\n    ",
                "Functions": [],
                "Name": "TFOptimizer",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "lr",
                    "rho",
                    "epsilon",
                    "decay"
                ],
                "Defaults": [
                    "1.0",
                    " 0.95",
                    " None",
                    " 0.0"
                ],
                "DocStr": "Adadelta optimizer.\n\nAdadelta is a more robust extension of Adagrad\nthat adapts learning rates based on a moving window of gradient updates,\ninstead of accumulating all past gradients. This way, Adadelta continues\nlearning even when many updates have been done. Compared to Adagrad, in the\noriginal version of Adadelta you don't have to set an initial learning\nrate. In this version, initial learning rate and decay factor can\nbe set, as in most other Keras optimizers.\n\nIt is recommended to leave the parameters of this optimizer\nat their default values.\n\n# Arguments\n    lr: float >= 0. Initial learning rate, defaults to 1.\n        It is recommended to leave it at the default value.\n    rho: float >= 0. Adadelta decay factor, corresponding to fraction of\n        gradient to keep at each time step.\n    epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n    decay: float >= 0. Initial learning rate decay.\n\n# References\n    - [Adadelta - an adaptive learning rate method]\n      (https://arxiv.org/abs/1212.5701)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "adadelta",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "lr",
                    "epsilon",
                    "decay"
                ],
                "Defaults": [
                    "0.01",
                    " None",
                    " 0.0"
                ],
                "DocStr": "Adagrad optimizer.\n\nAdagrad is an optimizer with parameter-specific learning rates,\nwhich are adapted relative to how frequently a parameter gets\nupdated during training. The more updates a parameter receives,\nthe smaller the updates.\n\nIt is recommended to leave the parameters of this optimizer\nat their default values.\n\n# Arguments\n    lr: float >= 0. Initial learning rate.\n    epsilon: float >= 0. If `None`, defaults to `K.epsilon()`.\n    decay: float >= 0. Learning rate decay over each update.\n\n# References\n    - [Adaptive Subgradient Methods for Online Learning and Stochastic\n       Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "adagrad",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "lr",
                    "beta_1",
                    "beta_2",
                    "epsilon",
                    "decay",
                    "amsgrad"
                ],
                "Defaults": [
                    "0.001",
                    " 0.9",
                    " 0.999",
                    " None",
                    " 0.0",
                    " False"
                ],
                "DocStr": "Adam optimizer.\n\nDefault parameters follow those provided in the original paper.\n\n# Arguments\n    lr: float >= 0. Learning rate.\n    beta_1: float, 0 < beta < 1. Generally close to 1.\n    beta_2: float, 0 < beta < 1. Generally close to 1.\n    epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n    decay: float >= 0. Learning rate decay over each update.\n    amsgrad: boolean. Whether to apply the AMSGrad variant of this\n        algorithm from the paper \"On the Convergence of Adam and\n        Beyond\".\n\n# References\n    - [Adam - A Method for Stochastic Optimization]\n      (https://arxiv.org/abs/1412.6980v8)\n    - [On the Convergence of Adam and Beyond]\n      (https://openreview.net/forum?id=ryQu7f-RZ)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "adam",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "lr",
                    "beta_1",
                    "beta_2",
                    "epsilon",
                    "decay"
                ],
                "Defaults": [
                    "0.002",
                    " 0.9",
                    " 0.999",
                    " None",
                    " 0.0"
                ],
                "DocStr": "Adamax optimizer from Adam paper's Section 7.\n\nIt is a variant of Adam based on the infinity norm.\nDefault parameters follow those provided in the paper.\n\n# Arguments\n    lr: float >= 0. Learning rate.\n    beta_1/beta_2: floats, 0 < beta < 1. Generally close to 1.\n    epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n    decay: float >= 0. Learning rate decay over each update.\n\n# References\n    - [Adam - A Method for Stochastic Optimization]\n      (https://arxiv.org/abs/1412.6980v8)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "adamax",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "lr",
                    "beta_1",
                    "beta_2",
                    "epsilon",
                    "schedule_decay"
                ],
                "Defaults": [
                    "0.002",
                    " 0.9",
                    " 0.999",
                    " None",
                    " 0.004"
                ],
                "DocStr": "Nesterov Adam optimizer.\n\nMuch like Adam is essentially RMSprop with momentum,\nNadam is Adam RMSprop with Nesterov momentum.\n\nDefault parameters follow those provided in the paper.\nIt is recommended to leave the parameters of this optimizer\nat their default values.\n\n# Arguments\n    lr: float >= 0. Learning rate.\n    beta_1/beta_2: floats, 0 < beta < 1. Generally close to 1.\n    epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n\n# References\n    - [Nadam report](http://cs229.stanford.edu/proj2015/054_report.pdf)\n    - [On the importance of initialization and momentum in deep learning]\n      (http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "nadam",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "lr",
                    "rho",
                    "epsilon",
                    "decay"
                ],
                "Defaults": [
                    "0.001",
                    " 0.9",
                    " None",
                    " 0.0"
                ],
                "DocStr": "RMSProp optimizer.\n\nIt is recommended to leave the parameters of this optimizer\nat their default values\n(except the learning rate, which can be freely tuned).\n\nThis optimizer is usually a good choice for recurrent\nneural networks.\n\n# Arguments\n    lr: float >= 0. Learning rate.\n    rho: float >= 0.\n    epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n    decay: float >= 0. Learning rate decay over each update.\n\n# References\n    - [rmsprop: Divide the gradient by a running average of its recent magnitude]\n      (http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "rmsprop",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "lr",
                    "momentum",
                    "decay",
                    "nesterov"
                ],
                "Defaults": [
                    "0.01",
                    " 0.0",
                    " 0.0",
                    " False"
                ],
                "DocStr": "Stochastic gradient descent optimizer.\n\nIncludes support for momentum,\nlearning rate decay, and Nesterov momentum.\n\n# Arguments\n    lr: float >= 0. Learning rate.\n    momentum: float >= 0. Parameter that accelerates SGD\n        in the relevant direction and dampens oscillations.\n    decay: float >= 0. Learning rate decay over each update.\n    nesterov: boolean. Whether to apply Nesterov momentum.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "sgd",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "DocStr": "zip(iter1 [,iter2 [...]]) --> zip object\n\nReturn a zip object whose .__next__() method returns a tuple where\nthe i-th element comes from the i-th iterable argument.  The .__next__()\nmethod continues until the shortest iterable in the argument sequence\nis exhausted and then it raises StopIteration.",
                "Functions": [],
                "Name": "zip",
                "Type": "Class"
            }
        ],
        "Functions": [
            {
                "Args": [
                    "g",
                    "c",
                    "n"
                ],
                "DocStr": "Clip the gradient `g` if the L2 norm `n` exceeds `c`.\n\n# Arguments\n    g: Tensor, the gradient tensor\n    c: float >= 0. Gradients will be clipped\n        when their L2 norm exceeds this value.\n    n: Tensor, actual norm of `g`.\n\n# Returns\n    Tensor, the gradient clipped if required.",
                "Name": "clip_norm"
            },
            {
                "Args": [
                    "config",
                    "custom_objects"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": "Inverse of the `serialize` function.\n\n# Arguments\n    config: Optimizer configuration dictionary.\n    custom_objects: Optional dictionary mapping\n        names (strings) to custom objects\n        (classes and functions)\n        to be considered during deserialization.\n\n# Returns\n    A Keras Optimizer instance.",
                "Name": "deserialize"
            },
            {
                "Args": [
                    "identifier",
                    "module_objects",
                    "custom_objects",
                    "printable_module_name"
                ],
                "Defaults": [
                    "None",
                    " None",
                    " 'object'"
                ],
                "DocStr": null,
                "Name": "deserialize_keras_object"
            },
            {
                "Args": [
                    "identifier"
                ],
                "DocStr": "Retrieves a Keras Optimizer instance.\n\n# Arguments\n    identifier: Optimizer identifier, one of\n        - String: name of an optimizer\n        - Dictionary: configuration dictionary.\n        - Keras Optimizer instance (it will be returned unchanged).\n        - TensorFlow Optimizer instance\n            (it will be wrapped as a Keras Optimizer).\n\n# Returns\n    A Keras Optimizer instance.\n\n# Raises\n    ValueError: If `identifier` cannot be interpreted.",
                "Name": "get"
            },
            {
                "Args": [
                    "optimizer"
                ],
                "DocStr": null,
                "Name": "serialize"
            },
            {
                "Args": [
                    "instance"
                ],
                "DocStr": null,
                "Name": "serialize_keras_object"
            }
        ],
        "Name": "optimizers",
        "Type": "Module"
    },
    {
        "Classes": [],
        "Functions": [],
        "Name": "preprocessing",
        "Type": "Module"
    },
    {
        "Classes": [
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "l1",
                    "l2"
                ],
                "Defaults": [
                    "0.0",
                    " 0.0"
                ],
                "DocStr": "Regularizer for L1 and L2 regularization.\n\n# Arguments\n    l1: Float; L1 regularization factor.\n    l2: Float; L2 regularization factor.",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "L1L2",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Regularizer base class.\n    ",
                "Functions": [
                    {
                        "Args": [
                            "cls",
                            "config"
                        ],
                        "DocStr": null,
                        "Name": "from_config"
                    }
                ],
                "Name": "Regularizer",
                "Type": "Class"
            }
        ],
        "Functions": [
            {
                "Args": [
                    "config",
                    "custom_objects"
                ],
                "Defaults": [
                    "None"
                ],
                "DocStr": null,
                "Name": "deserialize"
            },
            {
                "Args": [
                    "identifier",
                    "module_objects",
                    "custom_objects",
                    "printable_module_name"
                ],
                "Defaults": [
                    "None",
                    " None",
                    " 'object'"
                ],
                "DocStr": null,
                "Name": "deserialize_keras_object"
            },
            {
                "Args": [
                    "identifier"
                ],
                "DocStr": null,
                "Name": "get"
            },
            {
                "Args": [
                    "l"
                ],
                "Defaults": [
                    "0.01"
                ],
                "DocStr": null,
                "Name": "l1"
            },
            {
                "Args": [
                    "l1",
                    "l2"
                ],
                "Defaults": [
                    "0.01",
                    " 0.01"
                ],
                "DocStr": null,
                "Name": "l1_l2"
            },
            {
                "Args": [
                    "l"
                ],
                "Defaults": [
                    "0.01"
                ],
                "DocStr": null,
                "Name": "l2"
            },
            {
                "Args": [
                    "regularizer"
                ],
                "DocStr": null,
                "Name": "serialize"
            },
            {
                "Args": [
                    "instance"
                ],
                "DocStr": null,
                "Name": "serialize_keras_object"
            }
        ],
        "Name": "regularizers",
        "Type": "Module"
    },
    {
        "Classes": [
            {
                "Abstract": false,
                "Args": [
                    "self"
                ],
                "DocStr": "Provides a scope that changes to `_GLOBAL_CUSTOM_OBJECTS` cannot escape.\n\nCode within a `with` statement will be able to access custom objects\nby name. Changes to global custom objects persist\nwithin the enclosing `with` statement. At end of the `with` statement,\nglobal custom objects are reverted to state\nat beginning of the `with` statement.\n\n# Example\n\nConsider a custom object `MyObject` (e.g. a class):\n\n```python\n    with CustomObjectScope({'MyObject':MyObject}):\n        layer = Dense(..., kernel_regularizer='MyObject')\n        # save, load, etc. will recognize custom object by name\n```",
                "Functions": [],
                "Name": "CustomObjectScope",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "sequence",
                    "use_multiprocessing",
                    "wait_time",
                    "random_seed"
                ],
                "Defaults": [
                    "False",
                    " None",
                    " None"
                ],
                "DocStr": "Builds a queue out of a data generator.\n\nThe provided generator can be finite in which case the class will throw\na `StopIteration` exception.\n\nUsed in `fit_generator`, `evaluate_generator`, `predict_generator`.\n\n# Arguments\n    generator: a generator function which yields data\n    use_multiprocessing: use multiprocessing if True, otherwise threading\n    wait_time: time to sleep in-between calls to `put()`\n    random_seed: Initial seed for workers,\n        will be incremented by one for each worker.",
                "Functions": [],
                "Name": "GeneratorEnqueuer",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "datapath",
                    "dataset",
                    "start",
                    "end",
                    "normalizer"
                ],
                "Defaults": [
                    "0",
                    " None",
                    " None"
                ],
                "DocStr": "Representation of HDF5 dataset to be used instead of a Numpy array.\n\n# Example\n\n```python\n    x_data = HDF5Matrix('input/file.hdf5', 'data')\n    model.predict(x_data)\n```\n\nProviding `start` and `end` allows use of a slice of the dataset.\n\nOptionally, a normalizer function (or lambda) can be given. This will\nbe called on every slice of data retrieved.\n\n# Arguments\n    datapath: string, path to a HDF5 file\n    dataset: string, name of the HDF5 dataset in the file specified\n        in datapath\n    start: int, start of desired slice of the specified dataset\n    end: int, end of desired slice of the specified dataset\n    normalizer: function to be called on data when retrieved\n\n# Returns\n    An array-like HDF5 dataset.",
                "Functions": [],
                "Name": "HDF5Matrix",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "sequence",
                    "use_multiprocessing",
                    "shuffle"
                ],
                "Defaults": [
                    "False",
                    " False"
                ],
                "DocStr": "Builds a Enqueuer from a Sequence.\n\nUsed in `fit_generator`, `evaluate_generator`, `predict_generator`.\n\n# Arguments\n    sequence: A `keras.utils.data_utils.Sequence` object.\n    use_multiprocessing: use multiprocessing if True, otherwise threading\n    shuffle: whether to shuffle the data at the beginning of each epoch",
                "Functions": [],
                "Name": "OrderedEnqueuer",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "target",
                    "width",
                    "verbose",
                    "interval",
                    "stateful_metrics"
                ],
                "Defaults": [
                    "30",
                    " 1",
                    " 0.05",
                    " None"
                ],
                "DocStr": "Displays a progress bar.\n\n# Arguments\n    target: Total number of steps expected, None if unknown.\n    width: Progress bar width on screen.\n    verbose: Verbosity mode, 0 (silent), 1 (verbose), 2 (semi-verbose)\n    stateful_metrics: Iterable of string names of metrics that\n        should *not* be averaged over time. Metrics in this list\n        will be displayed as-is. All others will be averaged\n        by the progbar before display.\n    interval: Minimum visual progress update interval (in seconds).",
                "Functions": [],
                "Name": "Progbar",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [],
                "DocStr": "Base object for fitting to a sequence of data, such as a dataset.\n\nEvery `Sequence` must implement the `__getitem__` and the `__len__` methods.\nIf you want to modify your dataset between epochs you may implement\n`on_epoch_end`. The method `__getitem__` should return a complete batch.\n\n# Notes\n\n`Sequence` are a safer way to do multiprocessing. This structure guarantees\nthat the network will only train once on each sample per epoch which is not\nthe case with generators.\n\n# Examples\n\n```python\n    from skimage.io import imread\n    from skimage.transform import resize\n    import numpy as np\n\n    # Here, `x_set` is list of path to the images\n    # and `y_set` are the associated classes.\n\n    class CIFAR10Sequence(Sequence):\n\n        def __init__(self, x_set, y_set, batch_size):\n            self.x, self.y = x_set, y_set\n            self.batch_size = batch_size\n\n        def __len__(self):\n            return int(np.ceil(len(self.x) / float(self.batch_size)))\n\n        def __getitem__(self, idx):\n            batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n            batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n\n            return np.array([\n                resize(imread(file_name), (200, 200))\n                   for file_name in batch_x]), np.array(batch_y)\n```",
                "Functions": [],
                "Name": "Sequence",
                "Type": "Class"
            },
            {
                "Abstract": false,
                "Args": [
                    "self",
                    "path",
                    "mode"
                ],
                "Defaults": [
                    "'a'"
                ],
                "DocStr": "A dict-like wrapper around h5py groups (or dicts).\n\nThis allows us to have a single serialization logic\nfor both pickling and saving to disk.\n\nNote: This is not intended to be a generic wrapper.\nThere are lot of edge cases which have been hardcoded,\nand makes sense only in the context of model serialization/\ndeserialization.",
                "Functions": [],
                "Name": "h5dict",
                "Type": "Class"
            }
        ],
        "Functions": [
            {
                "Args": [
                    "model"
                ],
                "DocStr": "Converts all convolution kernels in a model from Theano to TensorFlow.\n\nAlso works from TensorFlow to Theano.\n\n# Arguments\n    model: target model for the conversion.",
                "Name": "convert_all_kernels_in_model"
            },
            {
                "Args": [],
                "DocStr": "Provides a scope that changes to `_GLOBAL_CUSTOM_OBJECTS` cannot escape.\n\nConvenience wrapper for `CustomObjectScope`.\nCode within a `with` statement will be able to access custom objects\nby name. Changes to global custom objects persist\nwithin the enclosing `with` statement. At end of the `with` statement,\nglobal custom objects are reverted to state\nat beginning of the `with` statement.\n\n# Example\n\nConsider a custom object `MyObject`\n\n```python\n    with custom_object_scope({'MyObject':MyObject}):\n        layer = Dense(..., kernel_regularizer='MyObject')\n        # save, load, etc. will recognize custom object by name\n```\n\n# Arguments\n    *args: Variable length list of dictionaries of name,\n        class pairs to add to custom objects.\n\n# Returns\n    Object of type `CustomObjectScope`.",
                "Name": "custom_object_scope"
            },
            {
                "Args": [
                    "identifier",
                    "module_objects",
                    "custom_objects",
                    "printable_module_name"
                ],
                "Defaults": [
                    "None",
                    " None",
                    " 'object'"
                ],
                "DocStr": null,
                "Name": "deserialize_keras_object"
            },
            {
                "Args": [],
                "DocStr": "Retrieves a live reference to the global dictionary of custom objects.\n\nUpdating and clearing custom objects using `custom_object_scope`\nis preferred, but `get_custom_objects` can\nbe used to directly access `_GLOBAL_CUSTOM_OBJECTS`.\n\n# Example\n\n```python\n    get_custom_objects().clear()\n    get_custom_objects()['MyObject'] = MyObject\n```\n\n# Returns\n    Global dictionary of names to classes (`_GLOBAL_CUSTOM_OBJECTS`).",
                "Name": "get_custom_objects"
            },
            {
                "Args": [
                    "fname",
                    "origin",
                    "untar",
                    "md5_hash",
                    "file_hash",
                    "cache_subdir",
                    "hash_algorithm",
                    "extract",
                    "archive_format",
                    "cache_dir"
                ],
                "Defaults": [
                    "False",
                    " None",
                    " None",
                    " 'datasets'",
                    " 'auto'",
                    " False",
                    " 'auto'",
                    " None"
                ],
                "DocStr": "Downloads a file from a URL if it not already in the cache.\n\nBy default the file at the url `origin` is downloaded to the\ncache_dir `~/.keras`, placed in the cache_subdir `datasets`,\nand given the filename `fname`. The final location of a file\n`example.txt` would therefore be `~/.keras/datasets/example.txt`.\n\nFiles in tar, tar.gz, tar.bz, and zip formats can also be extracted.\nPassing a hash will verify the file after download. The command line\nprograms `shasum` and `sha256sum` can compute the hash.\n\n# Arguments\n    fname: Name of the file. If an absolute path `/path/to/file.txt` is\n        specified the file will be saved at that location.\n    origin: Original URL of the file.\n    untar: Deprecated in favor of 'extract'.\n        boolean, whether the file should be decompressed\n    md5_hash: Deprecated in favor of 'file_hash'.\n        md5 hash of the file for verification\n    file_hash: The expected hash string of the file after download.\n        The sha256 and md5 hash algorithms are both supported.\n    cache_subdir: Subdirectory under the Keras cache dir where the file is\n        saved. If an absolute path `/path/to/folder` is\n        specified the file will be saved at that location.\n    hash_algorithm: Select the hash algorithm to verify the file.\n        options are 'md5', 'sha256', and 'auto'.\n        The default 'auto' detects the hash algorithm in use.\n    extract: True tries extracting the file as an Archive, like tar or zip.\n    archive_format: Archive format to try for extracting the file.\n        Options are 'auto', 'tar', 'zip', and None.\n        'tar' includes tar, tar.gz, and tar.bz files.\n        The default 'auto' is ['tar', 'zip'].\n        None or an empty list will return no matches found.\n    cache_dir: Location to store cached files, when None it\n        defaults to the [Keras Directory](/faq/#where-is-the-keras-configuration-filed-stored).\n\n# Returns\n    Path to the downloaded file",
                "Name": "get_file"
            },
            {
                "Args": [
                    "tensor",
                    "layer",
                    "node_index"
                ],
                "Defaults": [
                    "None",
                    " None"
                ],
                "DocStr": "Returns the list of input tensors necessary to compute `tensor`.\n\nOutput will always be a list of tensors\n(potentially with 1 element).\n\n# Arguments\n    tensor: The tensor to start from.\n    layer: Origin layer of the tensor. Will be\n        determined via tensor._keras_history if not provided.\n    node_index: Origin node index of the tensor.\n\n# Returns\n    List of input tensors.",
                "Name": "get_source_inputs"
            },
            {
                "Args": [
                    "model",
                    "gpus",
                    "cpu_merge",
                    "cpu_relocation"
                ],
                "Defaults": [
                    "None",
                    " True",
                    " False"
                ],
                "DocStr": "Replicates a model on different GPUs.\n\nSpecifically, this function implements single-machine\nmulti-GPU data parallelism. It works in the following way:\n\n- Divide the model's input(s) into multiple sub-batches.\n- Apply a model copy on each sub-batch. Every model copy\n    is executed on a dedicated GPU.\n- Concatenate the results (on CPU) into one big batch.\n\nE.g. if your `batch_size` is 64 and you use `gpus=2`,\nthen we will divide the input into 2 sub-batches of 32 samples,\nprocess each sub-batch on one GPU, then return the full\nbatch of 64 processed samples.\n\nThis induces quasi-linear speedup on up to 8 GPUs.\n\nThis function is only available with the TensorFlow backend\nfor the time being.\n\n# Arguments\n    model: A Keras model instance. To avoid OOM errors,\n        this model could have been built on CPU, for instance\n        (see usage example below).\n    gpus: Integer >= 2 or list of integers, number of GPUs or\n        list of GPU IDs on which to create model replicas.\n    cpu_merge: A boolean value to identify whether to force\n        merging model weights under the scope of the CPU or not.\n    cpu_relocation: A boolean value to identify whether to\n        create the model's weights under the scope of the CPU.\n        If the model is not defined under any preceding device\n        scope, you can still rescue it by activating this option.\n\n# Returns\n    A Keras `Model` instance which can be used just like the initial\n    `model` argument, but which distributes its workload on multiple GPUs.\n\n# Example 1 - Training models with weights merge on CPU\n\n```python\n    import tensorflow as tf\n    from keras.applications import Xception\n    from keras.utils import multi_gpu_model\n    import numpy as np\n\n    num_samples = 1000\n    height = 224\n    width = 224\n    num_classes = 1000\n\n    # Instantiate the base model (or \"template\" model).\n    # We recommend doing this with under a CPU device scope,\n    # so that the model's weights are hosted on CPU memory.\n    # Otherwise they may end up hosted on a GPU, which would\n    # complicate weight sharing.\n    with tf.device('/cpu:0'):\n        model = Xception(weights=None,\n                         input_shape=(height, width, 3),\n                         classes=num_classes)\n\n    # Replicates the model on 8 GPUs.\n    # This assumes that your machine has 8 available GPUs.\n    parallel_model = multi_gpu_model(model, gpus=8)\n    parallel_model.compile(loss='categorical_crossentropy',\n                           optimizer='rmsprop')\n\n    # Generate dummy data.\n    x = np.random.random((num_samples, height, width, 3))\n    y = np.random.random((num_samples, num_classes))\n\n    # This `fit` call will be distributed on 8 GPUs.\n    # Since the batch size is 256, each GPU will process 32 samples.\n    parallel_model.fit(x, y, epochs=20, batch_size=256)\n\n    # Save model via the template model (which shares the same weights):\n    model.save('my_model.h5')\n```\n\n# Example 2 - Training models with weights merge on CPU using cpu_relocation\n\n```python\n     ..\n     # Not needed to change the device scope for model definition:\n     model = Xception(weights=None, ..)\n\n     try:\n         model = multi_gpu_model(model, cpu_relocation=True)\n         print(\"Training using multiple GPUs..\")\n     except:\n         print(\"Training using single GPU or CPU..\")\n\n     model.compile(..)\n     ..\n```\n\n# Example 3 - Training models with weights merge on GPU (recommended for NV-link)\n\n```python\n     ..\n     # Not needed to change the device scope for model definition:\n     model = Xception(weights=None, ..)\n\n     try:\n         model = multi_gpu_model(model, cpu_merge=False)\n         print(\"Training using multiple GPUs..\")\n     except:\n         print(\"Training using single GPU or CPU..\")\n\n     model.compile(..)\n     ..\n```\n\n# On model saving\n\nTo save the multi-gpu model, use `.save(fname)` or `.save_weights(fname)`\nwith the template model (the argument you passed to `multi_gpu_model`),\nrather than the model returned by `multi_gpu_model`.",
                "Name": "multi_gpu_model"
            },
            {
                "Args": [
                    "x",
                    "axis",
                    "order"
                ],
                "Defaults": [
                    "-1",
                    " 2"
                ],
                "DocStr": "Normalizes a Numpy array.\n\n# Arguments\n    x: Numpy array to normalize.\n    axis: axis along which to normalize.\n    order: Normalization order (e.g. 2 for L2 norm).\n\n# Returns\n    A normalized copy of the array.",
                "Name": "normalize"
            },
            {
                "Args": [
                    "model",
                    "to_file",
                    "show_shapes",
                    "show_layer_names",
                    "rankdir"
                ],
                "Defaults": [
                    "'model.png'",
                    " False",
                    " True",
                    " 'TB'"
                ],
                "DocStr": "Converts a Keras model to dot format and save to a file.\n\n# Arguments\n    model: A Keras model instance\n    to_file: File name of the plot image.\n    show_shapes: whether to display shape information.\n    show_layer_names: whether to display layer names.\n    rankdir: `rankdir` argument passed to PyDot,\n        a string specifying the format of the plot:\n        'TB' creates a vertical plot;\n        'LR' creates a horizontal plot.",
                "Name": "plot_model"
            },
            {
                "Args": [
                    "model",
                    "line_length",
                    "positions",
                    "print_fn"
                ],
                "Defaults": [
                    "None",
                    " None",
                    " None"
                ],
                "DocStr": "Prints a summary of a model.\n\n# Arguments\n    model: Keras model instance.\n    line_length: Total length of printed lines\n        (e.g. set this to adapt the display to different\n        terminal window sizes).\n    positions: Relative or absolute positions of log elements in each line.\n        If not provided, defaults to `[.33, .55, .67, 1.]`.\n    print_fn: Print function to use.\n        It will be called on each line of the summary.\n        You can set it to a custom function\n        in order to capture the string summary.\n        It defaults to `print` (prints to stdout).",
                "Name": "print_summary"
            },
            {
                "Args": [
                    "instance"
                ],
                "DocStr": null,
                "Name": "serialize_keras_object"
            },
            {
                "Args": [
                    "y",
                    "num_classes",
                    "dtype"
                ],
                "Defaults": [
                    "None",
                    " 'float32'"
                ],
                "DocStr": "Converts a class vector (integers) to binary class matrix.\n\nE.g. for use with categorical_crossentropy.\n\n# Arguments\n    y: class vector to be converted into a matrix\n        (integers from 0 to num_classes).\n    num_classes: total number of classes.\n    dtype: The data type expected by the input, as a string\n        (`float32`, `float64`, `int32`...)\n\n# Returns\n    A binary matrix representation of the input. The classes axis\n    is placed last.",
                "Name": "to_categorical"
            }
        ],
        "Name": "utils",
        "Type": "Module"
    },
    {
        "Classes": [],
        "Functions": [],
        "Name": "wrappers",
        "Type": "Module"
    }
]